{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9248debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# NFL BIG DATA BOWL 2026 - å®Œæ•´è§£å†³æ–¹æ¡ˆ\n",
    "# ä½¿ç”¨æ—¶é—´åºåˆ—ç‰¹å¾é¢„æµ‹ä¼ çƒæˆ˜æœ¯ä¸­çƒå‘˜ç§»åŠ¨\n",
    "# ================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98da3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- æ ‡å‡†åº“ ---\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- ç¬¬ä¸‰æ–¹åº“ ---\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- æ·±åº¦å­¦ä¹ ï¼ˆPyTorchï¼‰---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    LinearLR,\n",
    "    OneCycleLR,\n",
    "    SequentialLR,\n",
    ")\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼ˆå¯é€‰ï¼‰\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f0e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    å…¨å±€é…ç½®ç±»\n",
    "    ç”¨äºæ§åˆ¶æ¨¡å‹è®­ç»ƒã€æ•°æ®è·¯å¾„ã€è®¾å¤‡å‚æ•°ç­‰å…¨å±€è®¾ç½®\n",
    "    \"\"\"\n",
    "\n",
    "    # ğŸ“ æ•°æ®è·¯å¾„è®¾ç½®\n",
    "    DATA_DIR            = Path(r\"D:\\æ•°æ®\\Kaggle\\2026 å¹´ NFL å¤§æ•°æ®ç¢— - é¢„æµ‹\\DATA_DIR000\")\n",
    "    NN_PRETRAIN_DIR     = Path(r\"D:\\æ•°æ®\\Kaggle\\2026 å¹´ NFL å¤§æ•°æ®ç¢— - é¢„æµ‹\\DATA_DIR000\\bigru-public\")\n",
    "\n",
    "    # âš™ï¸ é€šç”¨å‚æ•°\n",
    "    SEED                = 42                               # éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°\n",
    "    N_FOLDS             = 5                                # äº¤å‰éªŒè¯æŠ˜æ•°\n",
    "    USE_PLAYERS_INTERACTIONS = True                        # æ˜¯å¦ä½¿ç”¨çƒå‘˜äº¤äº’ç‰¹å¾\n",
    "\n",
    "    # ğŸŸï¸ çƒåœºå‚æ•°ï¼ˆå•ä½ï¼šç ï¼‰\n",
    "    FIELD_X_MIN         = 0.0\n",
    "    FIELD_X_MAX         = 120.0\n",
    "    FIELD_Y_MIN         = 0.0\n",
    "    FIELD_Y_MAX         = 53.3\n",
    "\n",
    "    # ğŸ§  æ¨¡å‹ç›¸å…³å‚æ•°\n",
    "    WINDOW_SIZE         = 12                               # æ—¶åºçª—å£é•¿åº¦\n",
    "    DEVICE              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \"\"\"è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒç»“æœå¯å¤ç°ã€‚\"\"\"\n",
    "    import random, os\n",
    "    random.seed(SEED)                               # Python å†…ç½®éšæœºæ•°æ¨¡å—\n",
    "    np.random.seed(SEED)                            # NumPy éšæœºæ•°ç”Ÿæˆå™¨\n",
    "    torch.manual_seed(SEED)                         # PyTorch CPU éšæœºæ•°\n",
    "    torch.cuda.manual_seed_all(SEED)                # PyTorch GPU éšæœºæ•°ï¼ˆå…¨éƒ¨ GPUï¼‰\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)        # æ§åˆ¶å“ˆå¸ŒéšæœºåŒ–ï¼ˆå­—å…¸ç­‰ç»“æ„ï¼‰\n",
    "    torch.backends.cudnn.deterministic = True       # ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼Œä¿è¯ç»“æœä¸€è‡´\n",
    "    torch.backends.cudnn.benchmark = False          # å…³é—­è‡ªåŠ¨ä¼˜åŒ–ï¼Œé¿å…ç»“æœä¸ç¨³å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ Config ä¸­å®šä¹‰çš„éšæœºç§å­\n",
    "def set_global_seeds(seed: int = 42):\n",
    "    \"\"\"è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒç»“æœå¯å¤ç°ã€‚\"\"\"\n",
    "    import random, os\n",
    "    random.seed(seed)                               # Python å†…ç½®éšæœºæ•°æ¨¡å—\n",
    "    np.random.seed(seed)                            # NumPy éšæœºæ•°ç”Ÿæˆå™¨\n",
    "    torch.manual_seed(seed)                         # PyTorch CPU éšæœºæ•°\n",
    "    torch.cuda.manual_seed_all(seed)                # PyTorch GPU éšæœºæ•°ï¼ˆå…¨éƒ¨ GPUï¼‰\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)        # æ§åˆ¶å“ˆå¸ŒéšæœºåŒ–ï¼ˆå­—å…¸ç­‰ç»“æ„ï¼‰\n",
    "    torch.backends.cudnn.deterministic = True       # ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼Œä¿è¯ç»“æœä¸€è‡´\n",
    "    torch.backends.cudnn.benchmark = False          # å…³é—­è‡ªåŠ¨ä¼˜åŒ–ï¼Œé¿å…ç»“æœä¸ç¨³å®š\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8797b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "def load_data(debug_fraction: float = 1.0):\n",
    "    \"\"\"åŠ è½½è®­ç»ƒé›†ä¸æµ‹è¯•é›†æ•°æ®ï¼Œå¯æŒ‰æ¯”ä¾‹åŠ è½½éƒ¨åˆ†æ•°æ®ç”¨äºè°ƒè¯•ã€‚\"\"\"\n",
    "    print(\"å¼€å§‹åŠ è½½æ•°æ®...\")\n",
    "\n",
    "    # æ„é€ è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå…± 18 å‘¨ï¼‰\n",
    "    train_input_files  = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\"  for w in range(1, 19)]\n",
    "    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "\n",
    "    # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„æ–‡ä»¶\n",
    "    train_input_files  = [f for f in train_input_files  if f.exists()]\n",
    "    train_output_files = [f for f in train_output_files if f.exists()]\n",
    "    print(f\"æ£€æµ‹åˆ° {len(train_input_files)} å‘¨çš„æœ‰æ•ˆè®­ç»ƒæ•°æ®ã€‚\")\n",
    "\n",
    "    # è¯»å–å¹¶åˆå¹¶è®­ç»ƒæ•°æ®ï¼ŒåŒæ—¶æ·»åŠ  week åˆ—åŒºåˆ†å‘¨æ¬¡\n",
    "    train_input = pd.concat(\n",
    "        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_input_files, start=1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    train_output = pd.concat(\n",
    "        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_output_files, start=1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # è¯»å–æµ‹è¯•é›†æ•°æ®\n",
    "    test_input    = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n",
    "    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n",
    "    print(f\"å·²åŠ è½½ {len(train_input):,} æ¡è¾“å…¥è®°å½•ï¼Œ{len(train_output):,} æ¡è¾“å‡ºè®°å½•ã€‚\")\n",
    "\n",
    "    # è‹¥å¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œä»…ä½¿ç”¨éƒ¨åˆ†æ¯”èµ›æ•°æ®\n",
    "    if debug_fraction < 1.0:\n",
    "        unique_game_ids  = train_input[\"game_id\"].unique()  # æ‰€æœ‰æ¯”èµ› ID\n",
    "        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n",
    "        train_input  = train_input[train_input[\"game_id\"].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        train_output = train_output[train_output[\"game_id\"].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        print(f\"è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨ {len(train_input):,} æ¡è¾“å…¥è®°å½•ï¼Œå…± {len(sampled_game_ids)} åœºæ¯”èµ›ã€‚\")\n",
    "\n",
    "    return train_input, train_output, test_input, test_template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90308916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼šç”¨äºåœ¨è¯„åˆ†æ—¶å‘å‚èµ›è€…æç¤ºé”™è¯¯ä¿¡æ¯ã€‚\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®— NFL ç«èµ›çš„é¢„æµ‹å¾—åˆ†ï¼ˆRMSEï¼‰ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        solution:   å®˜æ–¹çœŸå®æ•°æ®ï¼ˆDataFrameï¼‰\n",
    "        submission: é€‰æ‰‹æäº¤çš„é¢„æµ‹ç»“æœï¼ˆDataFrameï¼‰\n",
    "        row_id_column_name: å”¯ä¸€æ ‡è¯†åˆ—åï¼ˆé€šå¸¸ä¸º 'id'ï¼‰\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        float ç±»å‹çš„ RMSEï¼ˆRoot Mean Squared Errorï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼‰\n",
    "\n",
    "    è¦æ±‚ï¼š\n",
    "        solution å’Œ submission å¿…é¡»éƒ½åŒ…å«ï¼š\n",
    "            - 'id'ï¼šæ¯æ¡æ ·æœ¬å”¯ä¸€æ ‡è¯†ï¼ˆç”± game_id, play_id, nfl_id, frame_id ç»„æˆï¼‰\n",
    "            - 'x' å’Œ 'y'ï¼šçƒå‘˜åœ¨åœºä¸Šçš„ä½ç½®åæ ‡\n",
    "\n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> row_id_column_name = 'id'\n",
    "        >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n",
    "        >>> submission = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n",
    "        >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "        0.0913\n",
    "    \"\"\"\n",
    "\n",
    "    TARGET = ['x', 'y']  # éœ€è¦è®¡ç®—è¯¯å·®çš„ç›®æ ‡åˆ—\n",
    "\n",
    "    # æ£€æŸ¥å”¯ä¸€æ ‡è¯†åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    if row_id_column_name not in solution.columns:\n",
    "        raise ParticipantVisibleError(f\"Solution æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: '{row_id_column_name}'\")\n",
    "    if row_id_column_name not in submission.columns:\n",
    "        raise ParticipantVisibleError(f\"Submission æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: '{row_id_column_name}'\")\n",
    "\n",
    "    # æ£€æŸ¥é¢„æµ‹ä¸çœŸå®æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡åˆ— 'x'ã€'y'\n",
    "    missing_in_solution = set(TARGET) - set(solution.columns)\n",
    "    missing_in_submission = set(TARGET) - set(submission.columns)\n",
    "\n",
    "    if missing_in_solution:\n",
    "        raise ParticipantVisibleError(f\"Solution æ–‡ä»¶ç¼ºå°‘åˆ—: {missing_in_solution}\")\n",
    "    if missing_in_submission:\n",
    "        raise ParticipantVisibleError(f\"Submission æ–‡ä»¶ç¼ºå°‘åˆ—: {missing_in_submission}\")\n",
    "\n",
    "    # åªä¿ç•™ idã€xã€y åˆ—ï¼ˆé˜²æ­¢é¢å¤–æ— å…³åˆ—å¹²æ‰°ï¼‰\n",
    "    submission = submission[['id'] + TARGET]\n",
    "\n",
    "    # æŒ‰ id åˆå¹¶çœŸå®å€¼ä¸é¢„æµ‹å€¼\n",
    "    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n",
    "\n",
    "    # æ£€æŸ¥é¢„æµ‹ç»“æœä¸­æ˜¯å¦å­˜åœ¨ NaN\n",
    "    nanx_in_pred = merged_df['x_pred'].isna().sum()\n",
    "    nany_in_pred = merged_df['y_pred'].isna().sum()\n",
    "    if nanx_in_pred > 0:\n",
    "        print(f\"è­¦å‘Šï¼šé¢„æµ‹ç»“æœä¸­ x_pred å­˜åœ¨ {nanx_in_pred} ä¸ª NaN å€¼ã€‚\")\n",
    "    if nany_in_pred > 0:\n",
    "        print(f\"è­¦å‘Šï¼šé¢„æµ‹ç»“æœä¸­ y_pred å­˜åœ¨ {nany_in_pred} ä¸ª NaN å€¼ã€‚\")\n",
    "\n",
    "    # æ£€æŸ¥çœŸå®å€¼ä¸­å¯¹åº” NaN é¢„æµ‹çš„æ ·æœ¬æ˜¯å¦ä¹Ÿæœ‰ç¼ºå¤±\n",
    "    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n",
    "    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n",
    "    if nanx_in_true > 0:\n",
    "        print(f\"è­¦å‘Šï¼šçœŸå®å€¼ä¸­ x_true å­˜åœ¨ {nanx_in_true} ä¸ªä¸ NaN é¢„æµ‹å¯¹åº”çš„ç¼ºå¤±ã€‚\")\n",
    "    if nany_in_true > 0:\n",
    "        print(f\"è­¦å‘Šï¼šçœŸå®å€¼ä¸­ y_true å­˜åœ¨ {nany_in_true} ä¸ªä¸ NaN é¢„æµ‹å¯¹åº”çš„ç¼ºå¤±ã€‚\")\n",
    "\n",
    "    # è®¡ç®— RMSEï¼ˆå¯¹ x ä¸ y åˆ†åˆ«è®¡ç®— MSE åå–å¹³å‡å†å¼€æ–¹ï¼‰\n",
    "    rmse = np.sqrt(\n",
    "        0.5 * (\n",
    "            mean_squared_error(merged_df['x_true'], merged_df['x_pred']) +\n",
    "            mean_squared_error(merged_df['y_true'], merged_df['y_pred'])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return float(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeec1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_interactions_for_play_frames(df_play_frames: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¯ä¸ª (game_id, play_id, frame_id, nfl_id) çš„çƒå‘˜äº¤äº’ç‰¹å¾ã€‚\n",
    "\n",
    "    è¾“å‡ºç‰¹å¾åŒ…æ‹¬ï¼š\n",
    "      - è·ç¦»ç»Ÿè®¡ï¼ˆè¿›æ”»æ–¹/é˜²å®ˆæ–¹çš„å¹³å‡ã€æœ€å°ã€æœ€å¤§è·ç¦»ï¼‰\n",
    "      - ç›¸å¯¹é€Ÿåº¦ç»Ÿè®¡ï¼ˆå¹³å‡ã€æœ€å°ã€æœ€å¤§ï¼‰\n",
    "      - è§’åº¦ç»Ÿè®¡ï¼ˆè¿›æ”»æ–¹/é˜²å®ˆæ–¹çš„å¹³å‡ã€æœ€å°ã€æœ€å¤§è§’åº¦ï¼Œå¹³å‡è§’åº¦ä¸ºåœ†å‡å€¼ï¼‰\n",
    "      - æœ€è¿‘å¯¹æ‰‹çš„è·ç¦»ã€è§’åº¦ã€ç›¸å¯¹é€Ÿåº¦\n",
    "\n",
    "    æ³¨æ„ï¼š\n",
    "      - è‹¥å­˜åœ¨åˆ— 'player_to_predict'ï¼Œåˆ™åªè®¡ç®—è¯¥åˆ—ä¸º True çš„çƒå‘˜ï¼›\n",
    "      - å¦åˆ™ï¼Œå¯¹æ‰€æœ‰çƒå‘˜è®¡ç®—ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    out_rows = []  # å­˜å‚¨æ¯ä¸€å¸§çš„è®¡ç®—ç»“æœ\n",
    "\n",
    "    # éå†æ¯ä¸ª (æ¯”èµ›, æˆ˜æœ¯, å¸§)\n",
    "    for (g, p, f), grp in df_play_frames.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "        n = len(grp)\n",
    "        if n == 0:\n",
    "            continue  # è·³è¿‡ç©ºå¸§\n",
    "\n",
    "        # æå–åŸºæœ¬ä¿¡æ¯\n",
    "        nfl_ids = grp['nfl_id'].to_numpy()\n",
    "        x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "        y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "        vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "        vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "        is_off = grp['is_offense'].to_numpy().astype(bool)\n",
    "\n",
    "        # è‹¥å­˜åœ¨ player_to_predictï¼Œåˆ™ä»…è®¡ç®—æ ‡è®°ä¸º True çš„çƒå‘˜\n",
    "        compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n",
    "\n",
    "        # è®¡ç®—ä¸¤ä¸¤çƒå‘˜é—´çš„å‡ ä½•å…³ç³»ï¼ˆnÃ—nçŸ©é˜µï¼‰\n",
    "        dx = x[None, :] - x[:, None]\n",
    "        dy = y[None, :] - y[:, None]\n",
    "        dist = np.sqrt(dx * dx + dy * dy)                # è·ç¦»çŸ©é˜µ\n",
    "        angle_mat = np.arctan2(-dy, -dx)                 # ä»çƒå‘˜ i æŒ‡å‘ j çš„è§’åº¦\n",
    "        dvx = vx[:, None] - vx[None, :]\n",
    "        dvy = vy[:, None] - vy[None, :]\n",
    "        rel_speed = np.sqrt(dvx * dvx + dvy * dvy)       # ç›¸å¯¹é€Ÿåº¦çŸ©é˜µ\n",
    "\n",
    "        # æ©ç çŸ©é˜µï¼šè¿›æ”»æ–¹ã€ é˜²å®ˆæ–¹ã€ å¯¹æ‰‹æ–¹\n",
    "        opp_mask = (is_off[:, None] != is_off[None, :])  # True è¡¨ç¤ºå¯¹æ‰‹å…³ç³»\n",
    "        np.fill_diagonal(opp_mask, False)                # è‡ªèº«ç½® False\n",
    "\n",
    "        mask_off = np.broadcast_to(is_off[None, :], (n, n)).copy()   # è¿›æ”»æ–¹çŸ©é˜µ\n",
    "        mask_def = np.broadcast_to(~is_off[None, :], (n, n)).copy()  # é˜²å®ˆæ–¹çŸ©é˜µ\n",
    "        np.fill_diagonal(mask_off, False)\n",
    "        np.fill_diagonal(mask_def, False)\n",
    "\n",
    "        # æœ€è¿‘å¯¹æ‰‹ï¼ˆåŸºäºæœ€å°è·ç¦»ï¼‰\n",
    "        dist_opp = np.where(opp_mask, dist, np.nan)\n",
    "        nearest_dist = np.nanmin(dist_opp, axis=1)\n",
    "        nearest_idx = np.nanargmin(dist_opp, axis=1)\n",
    "        all_nan = ~np.isfinite(nearest_dist)\n",
    "        nearest_idx_safe = nearest_idx.copy()\n",
    "        nearest_idx_safe[all_nan] = 0  # é˜²æ­¢ nanargmin æŠ¥é”™\n",
    "        nearest_angle = np.take_along_axis(angle_mat, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "        nearest_rel   = np.take_along_axis(rel_speed, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "        nearest_angle[all_nan] = np.nan\n",
    "        nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "        # è®¡ç®—è·ç¦»çš„ç»Ÿè®¡ç‰¹å¾\n",
    "        d_off = np.where(mask_off, dist, np.nan)\n",
    "        d_def = np.where(mask_def, dist, np.nan)\n",
    "        d_mean_o = np.nanmean(d_off, axis=1); d_min_o = np.nanmin(d_off, axis=1); d_max_o = np.nanmax(d_off, axis=1)\n",
    "        d_mean_d = np.nanmean(d_def, axis=1); d_min_d = np.nanmin(d_def, axis=1); d_max_d = np.nanmax(d_def, axis=1)\n",
    "\n",
    "        # è®¡ç®—ç›¸å¯¹é€Ÿåº¦çš„ç»Ÿè®¡ç‰¹å¾\n",
    "        v_off = np.where(mask_off, rel_speed, np.nan)\n",
    "        v_def = np.where(mask_def, rel_speed, np.nan)\n",
    "        v_mean_o = np.nanmean(v_off, axis=1); v_min_o = np.nanmin(v_off, axis=1); v_max_o = np.nanmax(v_off, axis=1)\n",
    "        v_mean_d = np.nanmean(v_def, axis=1); v_min_d = np.nanmin(v_def, axis=1); v_max_d = np.nanmax(v_def, axis=1)\n",
    "\n",
    "        # è§’åº¦çš„å¹³å‡å€¼ï¼ˆåœ†å‡å€¼ï¼‰ä¸æœ€å°/æœ€å¤§è§’\n",
    "        sinA = np.sin(angle_mat); cosA = np.cos(angle_mat)\n",
    "        cnt_off = mask_off.sum(axis=1).astype(np.float32)\n",
    "        cnt_def = mask_def.sum(axis=1).astype(np.float32)\n",
    "        denom_off = np.where(cnt_off > 0, cnt_off, np.nan)\n",
    "        denom_def = np.where(cnt_def > 0, cnt_def, np.nan)\n",
    "        sin_sum_off = (sinA * mask_off).sum(axis=1)\n",
    "        cos_sum_off = (cosA * mask_off).sum(axis=1)\n",
    "        sin_sum_def = (sinA * mask_def).sum(axis=1)\n",
    "        cos_sum_def = (cosA * mask_def).sum(axis=1)\n",
    "        a_mean_o = np.arctan2(sin_sum_off / denom_off, cos_sum_off / denom_off)\n",
    "        a_mean_d = np.arctan2(sin_sum_def / denom_def, cos_sum_def / denom_def)\n",
    "        a_off = np.where(mask_off, angle_mat, np.nan)\n",
    "        a_def = np.where(mask_def, angle_mat, np.nan)\n",
    "        a_min_o = np.nanmin(a_off, axis=1); a_max_o = np.nanmax(a_off, axis=1)\n",
    "        a_min_d = np.nanmin(a_def, axis=1); a_max_d = np.nanmax(a_def, axis=1)\n",
    "\n",
    "        # ç”Ÿæˆè¾“å‡ºç»“æœï¼Œä»…å¯¹ compute_mask ä¸º True çš„çƒå‘˜è¾“å‡º\n",
    "        for idx, nid in enumerate(nfl_ids):\n",
    "            if not compute_mask[idx]:\n",
    "                continue\n",
    "            out_rows.append({\n",
    "                'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': int(nid),\n",
    "\n",
    "                # è¿›æ”»æ–¹è·ç¦»å’Œé€Ÿåº¦ç‰¹å¾\n",
    "                'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                'distance_to_player_min_offense':  d_min_o[idx],\n",
    "                'distance_to_player_max_offense':  d_max_o[idx],\n",
    "                'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                'relative_velocity_magnitude_min_offense':  v_min_o[idx],\n",
    "                'relative_velocity_magnitude_max_offense':  v_max_o[idx],\n",
    "                'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                'angle_to_player_min_offense':  a_min_o[idx],\n",
    "                'angle_to_player_max_offense':  a_max_o[idx],\n",
    "\n",
    "                # é˜²å®ˆæ–¹è·ç¦»å’Œé€Ÿåº¦ç‰¹å¾\n",
    "                'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                'distance_to_player_min_defense':  d_min_d[idx],\n",
    "                'distance_to_player_max_defense':  d_max_d[idx],\n",
    "                'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                'relative_velocity_magnitude_min_defense':  v_min_d[idx],\n",
    "                'relative_velocity_magnitude_max_defense':  v_max_d[idx],\n",
    "                'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                'angle_to_player_min_defense':  a_min_d[idx],\n",
    "                'angle_to_player_max_defense':  a_max_d[idx],\n",
    "\n",
    "                # æœ€è¿‘å¯¹æ‰‹çš„ç‰¹å¾\n",
    "                'nearest_opponent_dist': float(nearest_dist[idx]) if np.isfinite(nearest_dist[idx]) else np.nan,\n",
    "                'nearest_opponent_angle': float(nearest_angle[idx]) if np.isfinite(nearest_angle[idx]) else np.nan,\n",
    "                'nearest_opponent_rel_speed': float(nearest_rel[idx]) if np.isfinite(nearest_rel[idx]) else np.nan,\n",
    "            })\n",
    "\n",
    "    # è¿”å›åŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®è¡¨\n",
    "    return pd.DataFrame(\n",
    "        out_rows,\n",
    "        columns=[\n",
    "            'game_id', 'play_id', 'frame_id', 'nfl_id',\n",
    "\n",
    "            # è¿›æ”»æ–¹è·ç¦»ç‰¹å¾\n",
    "            'distance_to_player_mean_offense',\n",
    "            'distance_to_player_min_offense',\n",
    "            'distance_to_player_max_offense',\n",
    "\n",
    "            # è¿›æ”»æ–¹ç›¸å¯¹é€Ÿåº¦ç‰¹å¾\n",
    "            'relative_velocity_magnitude_mean_offense',\n",
    "            'relative_velocity_magnitude_min_offense',\n",
    "            'relative_velocity_magnitude_max_offense',\n",
    "\n",
    "            # è¿›æ”»æ–¹è§’åº¦ç‰¹å¾\n",
    "            'angle_to_player_mean_offense',\n",
    "            'angle_to_player_min_offense',\n",
    "            'angle_to_player_max_offense',\n",
    "\n",
    "            # é˜²å®ˆæ–¹è·ç¦»ç‰¹å¾\n",
    "            'distance_to_player_mean_defense',\n",
    "            'distance_to_player_min_defense',\n",
    "            'distance_to_player_max_defense',\n",
    "\n",
    "            # é˜²å®ˆæ–¹ç›¸å¯¹é€Ÿåº¦ç‰¹å¾\n",
    "            'relative_velocity_magnitude_mean_defense',\n",
    "            'relative_velocity_magnitude_min_defense',\n",
    "            'relative_velocity_magnitude_max_defense',\n",
    "\n",
    "            # é˜²å®ˆæ–¹è§’åº¦ç‰¹å¾\n",
    "            'angle_to_player_mean_defense',\n",
    "            'angle_to_player_min_defense',\n",
    "            'angle_to_player_max_defense',\n",
    "\n",
    "            # æœ€è¿‘å¯¹æ‰‹ç‰¹å¾\n",
    "            'nearest_opponent_dist',\n",
    "            'nearest_opponent_angle',\n",
    "            'nearest_opponent_rel_speed'\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dc5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def height_to_feet(height_str: str) -> Optional[float]:\n",
    "    \"\"\"å°†èº«é«˜ä» 'è‹±å°º-è‹±å¯¸' æ ¼å¼ï¼ˆå¦‚ '6-2'ï¼‰è½¬æ¢ä¸ºä»¥è‹±å°ºä¸ºå•ä½çš„å°æ•°ã€‚\"\"\"\n",
    "    try:\n",
    "        ft, inches = map(int, height_str.split('-'))\n",
    "        return ft + inches / 12\n",
    "    except Exception:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f38eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"æ·»åŠ  30~40 ä¸ªé«˜çº§ç‰¹å¾ï¼Œä»¥æå‡æ¨¡å‹é¢„æµ‹æ€§èƒ½ã€‚\"\"\"\n",
    "    print(\"æ­£åœ¨æ·»åŠ é«˜çº§ç‰¹å¾...\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols =             ['game_id', 'play_id', 'nfl_id']  # åˆ†ç»„é”®ï¼ˆæ¯åçƒå‘˜çš„æ—¶é—´åºåˆ—ï¼‰\n",
    "\n",
    "    # 1. è·ç¦»å˜åŒ–ç‰¹å¾ï¼ˆDistance Rate Featuresï¼‰\n",
    "    if 'distance_to_ball' in df.columns:\n",
    "        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)  # è·ç¦»å˜åŒ–\n",
    "        df['distance_to_ball_accel']  = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)  # è·ç¦»åŠ é€Ÿåº¦\n",
    "        df['time_to_intercept']       = (df['distance_to_ball'] /\n",
    "                                        (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)  # ä¼°ç®—åˆ°çƒæ—¶é—´\n",
    "\n",
    "    # 2. æ–¹å‘å¯¹é½ç‰¹å¾ï¼ˆTarget Alignment Featuresï¼‰\n",
    "    if 'ball_direction_x' in df.columns:\n",
    "        # é€Ÿåº¦åœ¨çƒæ–¹å‘ä¸Šçš„æŠ•å½±ï¼ˆå¯¹é½ç¨‹åº¦ï¼‰\n",
    "        df['velocity_alignment'] = (\n",
    "            df['velocity_x'] * df['ball_direction_x'] +\n",
    "            df['velocity_y'] * df['ball_direction_y']\n",
    "        )\n",
    "        # å‚ç›´äºçƒæ–¹å‘çš„é€Ÿåº¦åˆ†é‡\n",
    "        df['velocity_perpendicular'] = (\n",
    "            df['velocity_x'] * (-df['ball_direction_y']) +\n",
    "            df['velocity_y'] * df['ball_direction_x']\n",
    "        )\n",
    "        # åŠ é€Ÿåº¦åœ¨çƒæ–¹å‘ä¸Šçš„åˆ†é‡\n",
    "        if 'acceleration_x' in df.columns:\n",
    "            df['accel_alignment'] = (\n",
    "                df['acceleration_x'] * df['ball_direction_x'] +\n",
    "                df['acceleration_y'] * df['ball_direction_y']\n",
    "            )\n",
    "\n",
    "    # 3. å¤šçª—å£æ»šåŠ¨ç‰¹å¾ï¼ˆMulti-Window Rollingï¼‰\n",
    "    for window in [3, 5, 10]:\n",
    "        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in df.columns:\n",
    "                # æ»šåŠ¨å¹³å‡å€¼ï¼ˆå¹³æ»‘è¶‹åŠ¿ï¼‰\n",
    "                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
    "                )\n",
    "                # æ»šåŠ¨æ ‡å‡†å·®ï¼ˆçŸ­æœŸæ³¢åŠ¨æ€§ï¼‰\n",
    "                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).std()\n",
    "                ).fillna(0)\n",
    "\n",
    "    # 4. æ»åç‰¹å¾ï¼ˆExtended Lag Featuresï¼‰\n",
    "    for lag in [4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n",
    "\n",
    "    # 5. é€Ÿåº¦å˜åŒ–ç‰¹å¾ï¼ˆVelocity Change Featuresï¼‰\n",
    "    if 'velocity_x' in df.columns:\n",
    "        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)  # Xæ–¹å‘é€Ÿåº¦å˜åŒ–\n",
    "        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)  # Yæ–¹å‘é€Ÿåº¦å˜åŒ–\n",
    "        df['speed_change']      = df.groupby(gcols)['s'].diff().fillna(0)           # é€Ÿåº¦æ¨¡å˜åŒ–\n",
    "        df['direction_change']  = df.groupby(gcols)['dir'].diff().fillna(0)         # æ–¹å‘å˜åŒ–ï¼ˆè§’åº¦ï¼‰\n",
    "        # ä¿®æ­£è§’åº¦è·³å˜ï¼ˆå¦‚ -179Â° â†’ 181Â°ï¼‰\n",
    "        df['direction_change']  = df['direction_change'].apply(\n",
    "            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n",
    "        )\n",
    "\n",
    "    # 6. åœºåœ°ä½ç½®ç‰¹å¾ï¼ˆField Position Featuresï¼‰\n",
    "    df['dist_from_left']      = df['y']                         # è·ç¦»å·¦è¾¹çº¿\n",
    "    df['dist_from_right']     = 53.3 - df['y']                  # è·ç¦»å³è¾¹çº¿\n",
    "    df['dist_from_sideline']  = np.minimum(df['dist_from_left'], df['dist_from_right'])  # è·ç¦»æœ€è¿‘è¾¹çº¿\n",
    "    df['dist_from_endzone']   = np.minimum(df['x'], 120 - df['x'])  # è·ç¦»ç«¯åŒºï¼ˆå‰åæ–¹å‘ï¼‰\n",
    "\n",
    "    # 7. è§’è‰²ç‰¹å¾ï¼ˆRole-Specific Featuresï¼‰\n",
    "    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n",
    "        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']              # æ¥çƒæ‰‹è¿åŠ¨æ–¹å‘åŒ¹é…åº¦\n",
    "        df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))  # åç¦»çƒæ–¹å‘ç¨‹åº¦\n",
    "    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n",
    "        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']               # é˜²å®ˆçƒå‘˜é€¼è¿‘é€Ÿåº¦\n",
    "\n",
    "    # 8. æ—¶é—´ç‰¹å¾ï¼ˆTime Featuresï¼‰\n",
    "    df['frames_elapsed']  = df.groupby(gcols).cumcount()  # å½“å‰å¸§åºå·\n",
    "    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n",
    "        lambda x: x / (x.max() + 1)                       # å½’ä¸€åŒ–æ—¶é—´ï¼ˆ0~1ï¼‰\n",
    "    )\n",
    "\n",
    "    print(f\"ç‰¹å¾å¢å¼ºåæ€»åˆ—æ•°: {len(df.columns)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b02322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    é«˜æ–¯å™ªå£°å±‚ï¼ˆGaussian Noise Layerï¼‰\n",
    "\n",
    "    ä½œç”¨ï¼š\n",
    "        åœ¨è®­ç»ƒé˜¶æ®µå‘è¾“å…¥å¼ é‡æ·»åŠ éšæœºé«˜æ–¯å™ªå£°ï¼Œç”¨äºæ•°æ®å¢å¼ºæˆ–æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "        åœ¨æ¨ç†ï¼ˆè¯„ä¼°ï¼‰é˜¶æ®µåˆ™ä¸æ·»åŠ å™ªå£°ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        stddev (float): é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®ï¼Œæ§åˆ¶å™ªå£°å¼ºåº¦ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stddev: float):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ï¼š\n",
    "            è‹¥å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œåˆ™å‘è¾“å…¥æ·»åŠ å™ªå£°ï¼›\n",
    "            è‹¥å¤„äºè¯„ä¼°æ¨¡å¼ï¼ˆmodel.eval()ï¼‰ï¼Œåˆ™ç›´æ¥è¿”å›åŸè¾“å…¥ã€‚\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.stddev   # ç”Ÿæˆä¸è¾“å…¥åŒå½¢çŠ¶çš„é«˜æ–¯å™ªå£°\n",
    "            return x + noise                            # è¾“å…¥åŠ å™ª\n",
    "        return x                                        # æµ‹è¯•æ—¶ä¸åŠ å™ª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f3b528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SeqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    æ—¶åºé¢„æµ‹æ¨¡å‹ï¼ˆSequence Modelï¼‰\n",
    "    ---------------------------------------------------------\n",
    "    æ¨¡å‹ç»“æ„ï¼š\n",
    "        [è¾“å…¥åºåˆ—ç‰¹å¾] â†’ [åŒå‘GRU] â†’ [æ³¨æ„åŠ›æ± åŒ–] â†’ [å…¨è¿æ¥é¢„æµ‹å¤´]\n",
    "                       â†’ [ç´¯ç§¯è¾“å‡ºï¼ˆé¢„æµ‹è½¨è¿¹/ä½ç§»ï¼‰]\n",
    "\n",
    "    å‚æ•°è¯´æ˜ï¼š\n",
    "        input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆæ¯å¸§çš„ç‰¹å¾æ•°ï¼‰\n",
    "        horizon (int):   é¢„æµ‹æ—¶é—´æ­¥é•¿ï¼ˆè¾“å‡ºé•¿åº¦ï¼Œä¾‹å¦‚æœªæ¥Nå¸§ï¼‰\n",
    "\n",
    "    æ¨¡å—è¯´æ˜ï¼š\n",
    "        - GRUå±‚ï¼šæå–æ—¶åºåŠ¨æ€ç‰¹å¾ï¼ˆåŒå‘ï¼‰\n",
    "        - LayerNormï¼šè§„èŒƒåŒ–éšè—çŠ¶æ€ï¼Œæå‡æ•°å€¼ç¨³å®šæ€§\n",
    "        - Multi-Head Attentionï¼šåŸºäºå…¨å±€ä¸Šä¸‹æ–‡çš„åŠ æƒæ±‡èšï¼ˆæ± åŒ–å±‚ï¼‰\n",
    "        - Linear Headï¼šå¤šå±‚æ„ŸçŸ¥å™¨è¾“å‡ºé¢„æµ‹å€¼\n",
    "        - torch.cumsumï¼šå¯¹é¢„æµ‹ç»“æœè¿›è¡Œç´¯ç§¯æ±‚å’Œï¼ˆä¾‹å¦‚é¢„æµ‹ä½ç§»åºåˆ—ï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, horizon: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # â‘  åŒå‘ GRU å±‚ï¼šè¾“å…¥â†’éšè—â†’è¾“å‡º\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim,              # è¾“å…¥ç»´åº¦\n",
    "            128,                    # éšè—å±‚ç»´åº¦\n",
    "            num_layers=2,           # å †å å±‚æ•°\n",
    "            batch_first=True,       # è¾“å…¥æ ¼å¼ (B, T, D)\n",
    "            dropout=0.1,            # å±‚é—´ dropout\n",
    "            bidirectional=True      # åŒå‘ GRU\n",
    "        )\n",
    "\n",
    "        # â‘¡ å±‚å½’ä¸€åŒ– LayerNormï¼šç¨³å®šåŒå‘æ‹¼æ¥åçš„è¾“å‡º\n",
    "        self.pool_ln = nn.LayerNorm(256)  # 128Ã—2 = 256\n",
    "\n",
    "        # â‘¢ æ³¨æ„åŠ›æ± åŒ– Multi-Head Attention\n",
    "        self.pool_attn = nn.MultiheadAttention(\n",
    "            embed_dim=256,          # è¾“å…¥ç»´åº¦ä¸ LayerNorm è¾“å‡ºç›¸åŒ\n",
    "            num_heads=4,            # å¤šå¤´æ³¨æ„åŠ›æ•°\n",
    "            batch_first=True        # æ”¯æŒ (B, T, D) æ ¼å¼\n",
    "        )\n",
    "\n",
    "        # æ³¨æ„åŠ›æŸ¥è¯¢å‘é‡ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 256))\n",
    "\n",
    "        # â‘£ è¾“å‡ºé¢„æµ‹å¤´ï¼ˆMLPï¼‰\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è¿‡ç¨‹\n",
    "        å‚æ•°ï¼š\n",
    "            x: è¾“å…¥å¼ é‡ [B, T, input_dim]\n",
    "        è¿”å›ï¼š\n",
    "            y: è¾“å‡ºå¼ é‡ [B, horizon]\n",
    "        \"\"\"\n",
    "        # â‘  æå–æ—¶é—´ç‰¹å¾\n",
    "        h, _ = self.gru(x)                # h: [B, T, 256]\n",
    "        B = h.size(0)\n",
    "\n",
    "        # â‘¡ å¤åˆ¶æŸ¥è¯¢å‘é‡ï¼ˆæ¯ä¸ªbatchä¸€ä»½ï¼‰\n",
    "        q = self.pool_query.expand(B, -1, -1)  # [B, 1, 256]\n",
    "\n",
    "        # â‘¢ æ³¨æ„åŠ›æ± åŒ–ï¼šè®©æŸ¥è¯¢qå…³æ³¨æ•´æ®µåºåˆ—h\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))  # ctx: [B, 1, 256]\n",
    "\n",
    "        # â‘£ é€šè¿‡é¢„æµ‹å¤´æ˜ å°„è¾“å‡ºï¼ˆé€æ­¥é¢„æµ‹ï¼‰\n",
    "        out = self.head(ctx.squeeze(1))   # [B, horizon]\n",
    "\n",
    "        # â‘¤ å¯¹é¢„æµ‹ç»“æœè¿›è¡Œç´¯ç§¯æ±‚å’Œï¼Œç”Ÿæˆå¹³æ»‘è½¨è¿¹\n",
    "        return torch.cumsum(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf8ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# æ¨¡å‹æ„å»ºã€ä¿å­˜ä¸åŠ è½½å·¥å…·å‡½æ•°\n",
    "# =========================================================\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "\n",
    "def build_axis_model_from_config(cfg):\n",
    "    \"\"\"\n",
    "    æ ¹æ®é…ç½®å­—å…¸å®ä¾‹åŒ– SeqModel æ¨¡å‹ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        cfg (dict): åŒ…å«æ¨¡å‹è¶…å‚æ•°çš„é…ç½®ï¼Œä¾‹å¦‚ï¼š\n",
    "            {\n",
    "                'input_dim': 128,\n",
    "                'horizon': 20\n",
    "            }\n",
    "\n",
    "    è¿”å›:\n",
    "        model (SeqModel): æ„å»ºå¥½çš„åºåˆ—æ¨¡å‹å®ä¾‹ã€‚\n",
    "    \"\"\"\n",
    "    input_dim = cfg['input_dim']\n",
    "    horizon = cfg['horizon']\n",
    "    print(f\"ğŸ“¦ ä»é…ç½®æ„å»º SeqModel (input_dim={input_dim}, horizon={horizon})\")\n",
    "    return SeqModel(input_dim=input_dim, horizon=horizon)\n",
    "\n",
    "\n",
    "def _model_tag_from_instance(model):\n",
    "    \"\"\"\n",
    "    æ ¹æ®æ¨¡å‹å®ä¾‹è¿”å›å…¶ç±»å‹æ ‡ç­¾ï¼ˆtagï¼‰ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        model (nn.Module): æ¨¡å‹å®ä¾‹ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "        str: æ¨¡å‹ç±»å‹æ ‡ç­¾ï¼Œä¾‹å¦‚ 'seq'ã€‚\n",
    "    \"\"\"\n",
    "    if isinstance(model, SeqModel):\n",
    "        return 'seq'\n",
    "    return model.__class__.__name__.lower()\n",
    "\n",
    "\n",
    "def create_model_save_config(model, input_dim, horizon):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆä¸€ä¸ªæœ€å°åŒ–çš„æ¨¡å‹é…ç½®ï¼Œç”¨äºæ¨¡å‹é‡å»ºã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        model (nn.Module): æ¨¡å‹å®ä¾‹ï¼ˆç›®å‰ä»…æ”¯æŒ SeqModelï¼‰\n",
    "        input_dim (int): è¾“å…¥ç»´åº¦\n",
    "        horizon (int): é¢„æµ‹æ­¥é•¿\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: å¯ä¿å­˜åˆ° checkpoint çš„é…ç½®å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§© åˆ›å»ºæ¨¡å‹é…ç½®: input_dim={input_dim}, horizon={horizon}\")\n",
    "    return {\n",
    "        'model': 'seq',\n",
    "        'input_dim': int(input_dim),\n",
    "        'horizon': int(horizon),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_axis_checkpoint(model, cfg, fold_dir, axis_name='x'):\n",
    "    \"\"\"\n",
    "    ä¿å­˜ SeqModel çš„æ¨¡å‹æƒé‡ä¸é…ç½®åˆ°æŒ‡å®šè·¯å¾„ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        model (SeqModel): æ¨¡å‹å®ä¾‹\n",
    "        cfg (dict): æ¨¡å‹é…ç½®\n",
    "        fold_dir (str | Path): ä¿å­˜ç›®å½•\n",
    "        axis_name (str): åæ ‡è½´åç§° ('x' æˆ– 'y')\n",
    "\n",
    "    ä¿å­˜å†…å®¹:\n",
    "        fold_dir/axis_x.pt\n",
    "        fold_dir/axis_y.pt\n",
    "    \"\"\"\n",
    "    cfg = dict(cfg or {})\n",
    "    cfg['model'] = 'seq'\n",
    "    path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    torch.save({'state_dict': model.state_dict(), 'config': cfg}, str(path))\n",
    "    print(f\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {path}\")\n",
    "\n",
    "\n",
    "def load_axis_checkpoint(fold_dir, axis_name='x', device=None):\n",
    "    \"\"\"\n",
    "    ä»æŒ‡å®šè·¯å¾„åŠ è½½å•ä¸ª SeqModel æ£€æŸ¥ç‚¹ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        fold_dir (str | Path): æ¨¡å‹æ–‡ä»¶æ‰€åœ¨ç›®å½•\n",
    "        axis_name (str): æ¨¡å‹è½´æ ‡ç­¾ ('x' æˆ– 'y')\n",
    "        device (torch.device | None): åŠ è½½ç›®æ ‡è®¾å¤‡\n",
    "\n",
    "    è¿”å›:\n",
    "        model (SeqModel): å·²åŠ è½½å‚æ•°çš„æ¨¡å‹å®ä¾‹\n",
    "        cfg (dict): æ¨¡å‹é…ç½®\n",
    "    \"\"\"\n",
    "    device = device or Config.DEVICE\n",
    "    ckpt_path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    print(f\"ğŸ“‚ æ­£åœ¨åŠ è½½æ¨¡å‹ [{axis_name}] æ¥è‡ª {ckpt_path}\")\n",
    "\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=device)\n",
    "    cfg = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "\n",
    "    try:\n",
    "        # å°è¯•ä¸¥æ ¼åŒ¹é…åŠ è½½\n",
    "        model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        model.eval()\n",
    "        print(f\"âœ… [{axis_name}] æ¨¡å‹åŠ è½½æˆåŠŸ (strict=True)\")\n",
    "        return model, cfg\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ [{axis_name}] ä¸¥æ ¼åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨ non-strict æ¨¡å¼...\")\n",
    "\n",
    "    # å®½æ¾åŒ¹é…åŠ è½½ï¼ˆé€‚ç”¨äºå‚æ•°åæˆ–ç»“æ„ç•¥æœ‰å·®å¼‚çš„æƒ…å†µï¼‰\n",
    "    model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"âš ï¸ [{axis_name}] éä¸¥æ ¼åŠ è½½: ç¼ºå¤±å‚æ•°={len(missing)}, æ„å¤–å‚æ•°={len(unexpected)}\")\n",
    "    model.eval()\n",
    "    print(f\"âœ… [{axis_name}] æ¨¡å‹åŠ è½½æˆåŠŸ (strict=False)\")\n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "def load_folds_xy(num_folds, models_dir=None, device=None):\n",
    "    \"\"\"\n",
    "    åŠ è½½æ‰€æœ‰æŠ˜æ¬¡ (folds) çš„ X/Y æ–¹å‘æ¨¡å‹åŠå…¶å¯¹åº”ç‰¹å¾æ ‡å‡†åŒ–å™¨ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        num_folds (int): äº¤å‰éªŒè¯æŠ˜æ•°\n",
    "        models_dir (str | Path | None): æ¨¡å‹æ–‡ä»¶ç›®å½•\n",
    "        device (torch.device | None): åŠ è½½ç›®æ ‡è®¾å¤‡\n",
    "\n",
    "    è¿”å›:\n",
    "        (models_x, models_y, scalers, cfgs): å››ä¸ªåˆ—è¡¨ï¼Œåˆ†åˆ«ä¸ºï¼š\n",
    "            - models_x: æ‰€æœ‰ x æ–¹å‘æ¨¡å‹\n",
    "            - models_y: æ‰€æœ‰ y æ–¹å‘æ¨¡å‹\n",
    "            - scalers:  æ¯æŠ˜çš„ç‰¹å¾æ ‡å‡†åŒ–å™¨ï¼ˆjoblib å¯¹è±¡ï¼‰\n",
    "            - cfgs:     æ¯æŠ˜çš„é…ç½®å­—å…¸\n",
    "    \"\"\"\n",
    "    device = device or Config.DEVICE\n",
    "    base = Path(models_dir) if models_dir else Path('.')\n",
    "\n",
    "    models_x, models_y, scalers, cfgs = [], [], [], []\n",
    "\n",
    "    print(f\"ğŸ“ å¼€å§‹åŠ è½½ {num_folds} æŠ˜æ¨¡å‹ (è·¯å¾„: {base})\")\n",
    "\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        fold_dir = base / f'fold_{fold}'\n",
    "        try:\n",
    "            # åˆ†åˆ«åŠ è½½ X/Y æ¨¡å‹ä¸ scaler\n",
    "            mx, cfgx = load_axis_checkpoint(fold_dir, 'x', device=device)\n",
    "            my, cfgy = load_axis_checkpoint(fold_dir, 'y', device=device)\n",
    "            scaler = joblib.load(str(fold_dir / 'lstm_feature_scaler_fold.joblib'))\n",
    "\n",
    "            models_x.append(mx)\n",
    "            models_y.append(my)\n",
    "            scalers.append(scaler)\n",
    "            cfgs.append(cfgx)\n",
    "            print(f\"âœ… Fold {fold} åŠ è½½å®Œæˆ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Fold {fold} åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    print(f\"ğŸ“¦ å…±åŠ è½½ {len(models_x)} ä¸ª fold æ¨¡å‹ï¼ˆX/Y æ–¹å‘ï¼‰\")\n",
    "    return models_x, models_y, scalers, cfgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3da5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_predictions_xy(\n",
    "    models_x, models_y, scalers, X_test_unscaled, test_seq_ids, test_template, batch_size=1024\n",
    "):\n",
    "    \"\"\"\n",
    "    åŸºäºå¤šä¸ªæŠ˜ï¼ˆfoldï¼‰çš„ X/Y è½´æ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹ï¼ˆEnsembleï¼‰ï¼Œè¾“å‡ºæœ€ç»ˆåæ ‡é¢„æµ‹ç»“æœã€‚\n",
    "\n",
    "    å‚æ•°è¯´æ˜ï¼š\n",
    "        models_x, models_y : list\n",
    "            ä¸åŒæŠ˜çš„ SeqModel æ¨¡å‹ï¼ˆx è½´ä¸ y è½´é¢„æµ‹æ¨¡å‹ï¼‰ã€‚\n",
    "        scalers : list | None\n",
    "            å¯¹åº”æ¯ä¸ª fold çš„ StandardScaler å¯¹è±¡ï¼ˆæˆ– Noneï¼‰ã€‚\n",
    "        X_test_unscaled : list[np.ndarray]\n",
    "            æœªç¼©æ”¾çš„æµ‹è¯•åºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º (T, F)ã€‚\n",
    "        test_seq_ids : list[dict]\n",
    "            æ¯ä¸ªåºåˆ—å¯¹åº”çš„å…ƒæ•°æ®ï¼ŒåŒ…å«ï¼š\n",
    "            {'game_id', 'play_id', 'nfl_id', 'frame_id'ï¼ˆæœ€åä¸€å¸§ï¼‰}\n",
    "        test_template : pd.DataFrame\n",
    "            å®˜æ–¹æäº¤æ ¼å¼çš„æ¨¡æ¿ DataFrameï¼ŒåŒ…å«æ‰€æœ‰ (game_id, play_id, nfl_id, frame_id)ã€‚\n",
    "        batch_size : int\n",
    "            æ‰¹é‡å¤§å°ï¼Œé»˜è®¤ 1024ã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        submission : pd.DataFrame\n",
    "            æœ€ç»ˆé¢„æµ‹ç»“æœï¼ŒåŒ…å«åˆ— ['id', 'x', 'y']ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================\n",
    "    # Step 0. æ¨¡å‹ä¸æ•°æ®æ£€æŸ¥\n",
    "    # ============================\n",
    "    if len(models_x) == 0 or len(models_x) != len(models_y):\n",
    "        print(f\"âš ï¸ æ¨¡å‹æ•°é‡é”™è¯¯ï¼šlen(models_x)={len(models_x)}, len(models_y)={len(models_y)}\")\n",
    "        print(\"âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æˆ–æŠ˜æ•°ä¸åŒ¹é…ã€‚\")\n",
    "        return None\n",
    "\n",
    "    if scalers is not None and len(scalers) != len(models_x):\n",
    "        raise ValueError(\"âŒ scalers çš„æ•°é‡å¿…é¡»ä¸æ¨¡å‹æŠ˜æ•°ç›¸åŒã€‚\")\n",
    "\n",
    "    print(f\"ğŸ§© å…±æ£€æµ‹åˆ° {len(models_x)} ä¸ªæŠ˜ (fold) æ¨¡å‹ï¼Œå°†è¿›è¡Œé›†æˆé¢„æµ‹ã€‚\")\n",
    "\n",
    "    # ============================\n",
    "    # Step 1. æ•°æ®é¢„å¤„ç†\n",
    "    # ============================\n",
    "    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n",
    "    N = len(X_test_unscaled)\n",
    "    print(f\"ğŸ“¦ æµ‹è¯•åºåˆ—æ•°é‡: {N}\")\n",
    "\n",
    "    # è·å–æ¯ä¸ªåºåˆ—æœ€åä¸€å¸§çš„åŸå§‹åæ ‡ (x, y)\n",
    "    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n",
    "    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n",
    "\n",
    "    per_fold_dx, per_fold_dy = [], []\n",
    "\n",
    "    # ============================\n",
    "    # Step 2. å„æŠ˜æ¨¡å‹é¢„æµ‹\n",
    "    # ============================\n",
    "    for i in range(len(models_x)):\n",
    "        print(f\"\\nğŸš€ ç¬¬ {i + 1}/{len(models_x)} æŠ˜æ¨¡å‹é¢„æµ‹ä¸­...\")\n",
    "        model_x = models_x[i]\n",
    "        model_y = models_y[i]\n",
    "        scaler = scalers[i] if scalers is not None else None\n",
    "\n",
    "        # å¦‚æœå­˜åœ¨æ ‡å‡†åŒ–å™¨ï¼Œåˆ™é€åºåˆ—è¿›è¡Œç¼©æ”¾\n",
    "        if scaler is not None:\n",
    "            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n",
    "        else:\n",
    "            scaled = X_test_unscaled\n",
    "\n",
    "        # æ‹¼æ¥ä¸º (N, T, F)\n",
    "        X = np.stack(scaled.astype(np.float32))\n",
    "        device = next(model_x.parameters()).device\n",
    "\n",
    "        # æ„å»º DataLoader\n",
    "        ds = TensorDataset(torch.from_numpy(X))\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        dx_list, dy_list = [], []\n",
    "        model_x.eval()\n",
    "        model_y.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in dl:\n",
    "                batch = batch.to(device)\n",
    "                dx = model_x(batch)   # (B, H)\n",
    "                dy = model_y(batch)   # (B, H)\n",
    "                dx_list.append(dx.cpu().numpy())\n",
    "                dy_list.append(dy.cpu().numpy())\n",
    "\n",
    "        # æ‹¼æ¥æ‰€æœ‰ batch è¾“å‡º\n",
    "        dx_cum = np.vstack(dx_list)  # (N, H)\n",
    "        dy_cum = np.vstack(dy_list)  # (N, H)\n",
    "\n",
    "        per_fold_dx.append(dx_cum)\n",
    "        per_fold_dy.append(dy_cum)\n",
    "        print(f\"âœ… ç¬¬ {i + 1} æŠ˜é¢„æµ‹å®Œæˆ (å½¢çŠ¶: dx={dx_cum.shape}, dy={dy_cum.shape})\")\n",
    "\n",
    "    # ============================\n",
    "    # Step 3. æ¨¡å‹é›†æˆ (å–å‡å€¼)\n",
    "    # ============================\n",
    "    print(\"\\nğŸ”— å¼€å§‹æ¨¡å‹é›†æˆ (å¹³å‡èåˆå„æŠ˜é¢„æµ‹)...\")\n",
    "    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)  # (N, H)\n",
    "    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)  # (N, H)\n",
    "    print(f\"âœ… é›†æˆå®Œæˆ (ens_dx={ens_dx.shape}, ens_dy={ens_dy.shape})\")\n",
    "\n",
    "    # ============================\n",
    "    # Step 4. æ„å»ºæäº¤ç»“æœ\n",
    "    # ============================\n",
    "    test_meta = pd.DataFrame(test_seq_ids)\n",
    "    out_rows = []\n",
    "    H = ens_dx.shape[1]\n",
    "\n",
    "    print(\"\\nğŸ§® æ­£åœ¨ç”Ÿæˆæœ€ç»ˆåæ ‡é¢„æµ‹ç»“æœ...\")\n",
    "\n",
    "    for i, seq_info in test_meta.iterrows():\n",
    "        game_id = int(seq_info['game_id'])\n",
    "        play_id = int(seq_info['play_id'])\n",
    "        nfl_id = int(seq_info['nfl_id'])\n",
    "\n",
    "        # æ‰¾å‡ºè¯¥çƒå‘˜çš„é¢„æµ‹å¸§åºåˆ—\n",
    "        frame_ids = (\n",
    "            test_template[\n",
    "                (test_template['game_id'] == game_id) &\n",
    "                (test_template['play_id'] == play_id) &\n",
    "                (test_template['nfl_id'] == nfl_id)\n",
    "            ]['frame_id'].sort_values()\n",
    "        )\n",
    "\n",
    "        # ç”Ÿæˆæ¯å¸§é¢„æµ‹åæ ‡\n",
    "        for t, frame_id in enumerate(frame_ids):\n",
    "            tt = t if t < H else H - 1\n",
    "            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            out_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n",
    "                'x': px,\n",
    "                'y': py\n",
    "            })\n",
    "\n",
    "    submission = pd.DataFrame(out_rows)\n",
    "    print(f\"\\nâœ… å…±ç”Ÿæˆ {len(submission)} æ¡é¢„æµ‹ç»“æœã€‚\")\n",
    "\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcea68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_val_predictions(\n",
    "    models, scalers, X_val_unscaled, val_ids,\n",
    "    y_val_dx_fold, y_val_dy_fold, val_data,\n",
    "    exclude_fold=None\n",
    "):\n",
    "    \"\"\"\n",
    "    åœ¨éªŒè¯é›†ä¸Šç”Ÿæˆé›†æˆé¢„æµ‹ï¼ˆensembleï¼‰ï¼Œå¹¶å‡†å¤‡çœŸå®å€¼ä¸é¢„æµ‹ç»“æœä»¥ä¾›è¯„åˆ†ã€‚\n",
    "\n",
    "    åŠŸèƒ½ï¼š\n",
    "        - å¯¹æ¯ä¸ªéªŒè¯åºåˆ—ï¼Œä½¿ç”¨å…¶ä»–æŠ˜ï¼ˆfoldï¼‰çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼›\n",
    "        - é˜²æ­¢æ•°æ®æ³„éœ²ï¼ˆæ’é™¤å½“å‰foldå¯¹åº”çš„æ¨¡å‹ï¼‰ï¼›\n",
    "        - è¿”å›çœŸå®è½¨è¿¹ä¸é¢„æµ‹è½¨è¿¹çš„ DataFrameã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        models : list[nn.Module]\n",
    "            è®­ç»ƒå¥½çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé€šå¸¸ä¸ºå¤šä¸ªæŠ˜çš„æ¨¡å‹ï¼‰ã€‚\n",
    "        scalers : list[StandardScaler]\n",
    "            ä¸æ¨¡å‹å¯¹åº”çš„ç‰¹å¾æ ‡å‡†åŒ–å™¨ã€‚\n",
    "        X_val_unscaled : list[np.ndarray]\n",
    "            æœªç¼©æ”¾çš„éªŒè¯åºåˆ—ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (T, F)ã€‚\n",
    "        val_ids : list[dict]\n",
    "            æ¯æ¡åºåˆ—çš„å…ƒä¿¡æ¯ï¼ŒåŒ…å« ['game_id', 'play_id', 'nfl_id']ã€‚\n",
    "        y_val_dx_fold, y_val_dy_fold : list[np.ndarray]\n",
    "            æ¯æ¡éªŒè¯æ ·æœ¬çš„çœŸå®ä½ç§»ï¼ˆÎ”x, Î”yï¼‰ã€‚\n",
    "        val_data : pd.DataFrame\n",
    "            åŒ…å«éªŒè¯é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„ x_lastã€y_lastã€‚\n",
    "        exclude_fold : int | None\n",
    "            éœ€è¦æ’é™¤çš„æŠ˜å·ï¼ˆ0-basedï¼‰ï¼Œç”¨äºé¿å…æ¨¡å‹æ³„éœ²ã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        tuple(pd.DataFrame, pd.DataFrame)\n",
    "            (ensemble_pred_df, ensemble_true_df)\n",
    "            å…¶ä¸­ï¼š\n",
    "            - ensemble_pred_dfï¼šé¢„æµ‹ç»“æœ DataFrameï¼ˆid, x, yï¼‰\n",
    "            - ensemble_true_dfï¼šçœŸå®ç»“æœ DataFrameï¼ˆid, x, yï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    pred_rows = []\n",
    "    true_rows = []\n",
    "\n",
    "    print(\"ğŸ” å¼€å§‹ç”ŸæˆéªŒè¯é›†é›†æˆé¢„æµ‹...\")\n",
    "    if exclude_fold is not None:\n",
    "        print(f\"âš™ï¸  å½“å‰æ’é™¤ç¬¬ {exclude_fold + 1} æŠ˜çš„æ¨¡å‹ï¼Œä»¥é˜²æ­¢æ•°æ®æ³„éœ²ã€‚\")\n",
    "\n",
    "    for i, seq_info in enumerate(val_ids):\n",
    "        game_id = seq_info[\"game_id\"]\n",
    "        play_id = seq_info[\"play_id\"]\n",
    "        nfl_id = seq_info[\"nfl_id\"]\n",
    "\n",
    "        x_last = val_data.iloc[i][\"x_last\"]\n",
    "        y_last = val_data.iloc[i][\"y_last\"]\n",
    "\n",
    "        # ----------------------------\n",
    "        # çœŸå®è½¨è¿¹ (Ground Truth)\n",
    "        # ----------------------------\n",
    "        dx_true = y_val_dx_fold[i]\n",
    "        dy_true = y_val_dy_fold[i]\n",
    "        for t in range(len(dx_true)):\n",
    "            frame_rel = t + 1\n",
    "            true_x = x_last + dx_true[t]\n",
    "            true_y = y_last + dy_true[t]\n",
    "            true_rows.append({\n",
    "                \"id\": f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n",
    "                \"x\": true_x,\n",
    "                \"y\": true_y\n",
    "            })\n",
    "\n",
    "        # ----------------------------\n",
    "        # æ¨¡å‹é¢„æµ‹ï¼ˆæ’é™¤å½“å‰ foldï¼‰\n",
    "        # ----------------------------\n",
    "        per_model_dx = []\n",
    "        per_model_dy = []\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            if exclude_fold is not None and j == exclude_fold:\n",
    "                continue  # æ’é™¤å½“å‰éªŒè¯ fold å¯¹åº”æ¨¡å‹\n",
    "\n",
    "            scaler = scalers[j]\n",
    "            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n",
    "            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(scaled_seq).cpu().numpy()[0]  # (H, 2)\n",
    "            per_model_dx.append(output[:, 0])\n",
    "            per_model_dy.append(output[:, 1])\n",
    "\n",
    "        # ----------------------------\n",
    "        # é›†æˆï¼ˆå–å¹³å‡ï¼‰\n",
    "        # ----------------------------\n",
    "        if per_model_dx:  # è‡³å°‘ä¸€ä¸ªæ¨¡å‹å¯ç”¨\n",
    "            ens_dx = np.mean(per_model_dx, axis=0)\n",
    "            ens_dy = np.mean(per_model_dy, axis=0)\n",
    "        else:\n",
    "            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œé™¤éåªæœ‰ä¸€ä¸ª fold\n",
    "            ens_dx = np.zeros(len(dx_true))\n",
    "            ens_dy = np.zeros(len(dy_true))\n",
    "            print(f\"âš ï¸ æ ·æœ¬ {i} æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œä½¿ç”¨é›¶é¢„æµ‹ã€‚\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # ç”Ÿæˆé¢„æµ‹è½¨è¿¹\n",
    "        # ----------------------------\n",
    "        for t in range(len(dx_true)):\n",
    "            pred_x = x_last + ens_dx[t]\n",
    "            pred_y = y_last + ens_dy[t]\n",
    "            pred_rows.append({\n",
    "                \"id\": f\"{game_id}_{play_id}_{nfl_id}_{t + 1}\",\n",
    "                \"x\": np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n",
    "                \"y\": np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            })\n",
    "\n",
    "    print(f\"âœ… é›†æˆé¢„æµ‹å®Œæˆï¼Œå…±ç”Ÿæˆ {len(pred_rows)} æ¡é¢„æµ‹æ ·æœ¬ã€‚\")\n",
    "\n",
    "    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8c7e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åŠ è½½æ•°æ®...\n",
      "æ£€æµ‹åˆ° 18 å‘¨çš„æœ‰æ•ˆè®­ç»ƒæ•°æ®ã€‚\n",
      "å·²åŠ è½½ 4,880,579 æ¡è¾“å…¥è®°å½•ï¼Œ562,936 æ¡è¾“å‡ºè®°å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "set_global_seeds(Config.SEED)\n",
    "# print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n",
    "# models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n",
    "\n",
    "\n",
    "train_input, train_output, test_input, test_template = load_data(debug_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9cb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ç”Ÿæˆæ—¶åºæ ·æœ¬ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰\n",
    "# test_sequences, test_seq_ids, feature_cols = prepare_sequences(\n",
    "#     test_input, test_template=test_template, is_training=False, window_size=Config.WINDOW_SIZE\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… å·²å‡†å¤‡å¥½ {len(test_sequences)} ä¸ªæµ‹è¯•åºåˆ—ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« {len(feature_cols)} ä¸ªç‰¹å¾ã€‚\")\n",
    "# print(f\"ğŸ“ ç¤ºä¾‹åºåˆ—å½¢çŠ¶ï¼š{test_sequences[0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a7244",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ•°æ®è‹¥å¸§æ•°ä¸è¶³ ä¸å¡«å……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de5f46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(input_df, output_df=None, test_template=None,\n",
    "                      is_training=True, window_size=8,\n",
    "                      use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n",
    "    \"\"\"\n",
    "    æ„å»ºåŒ…å«æ‰€æœ‰é«˜çº§ç‰¹å¾çš„æ—¶åºæ ·æœ¬åºåˆ—ï¼ˆæ”¯æŒè®­ç»ƒä¸æµ‹è¯•ï¼‰ã€‚\n",
    "    input_dfï¼šè¾“å…¥ç‰¹å¾æ•°æ®\n",
    "    output_dfï¼šè®­ç»ƒæ ‡ç­¾ï¼ˆä»…åœ¨ is_training=True æ—¶ä½¿ç”¨ï¼‰\n",
    "    test_templateï¼šæµ‹è¯•æ¨¡æ¿ï¼ˆä»…åœ¨é¢„æµ‹æ—¶ä½¿ç”¨ï¼‰\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"ğŸš€ å¼€å§‹æ„å»ºæ—¶åºæ ·æœ¬ï¼ˆåŒ…å«é«˜çº§ç‰¹å¾ï¼‰\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"çª—å£å¤§å°ï¼ˆwindow_sizeï¼‰: {window_size}\")\n",
    "\n",
    "    input_df = input_df.copy()\n",
    "\n",
    "    # Step 1ï¼šåŸºç¡€ç‰¹å¾æ„å»º\n",
    "    print(\"æ­¥éª¤ 1/4 â–¶ æ·»åŠ åŸºç¡€ç‰¹å¾...\")\n",
    "\n",
    "    # å°†çƒå‘˜èº«é«˜ä»â€œè‹±å°º-è‹±å¯¸â€æ ¼å¼è½¬æ¢ä¸ºè‹±å°ºå°æ•°\n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "\n",
    "    # è®¡ç®—é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„ xã€y åˆ†é‡\n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "\n",
    "    # è§’åº¦ç‰¹å¾ï¼šå°†æœå‘è§’(o)ä¸è¿åŠ¨æ–¹å‘(dir)ç¼–ç ä¸ºæ­£å¼¦/ä½™å¼¦å½¢å¼\n",
    "    input_df['o_sin']  = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['o_cos']  = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "\n",
    "    # è§’è‰²ç‰¹å¾ï¼šè¿›æ”»/é˜²å®ˆ/ä¼ çƒ/æ¥çƒ/é˜²å®ˆè¦†ç›–\n",
    "    input_df['is_offense']  = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense']  = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer']   = (input_df['player_role'] == 'Passer').astype(int)\n",
    "\n",
    "    # ç‰©ç†ç‰¹å¾ï¼šåŠ¨é‡ä¸åŠ¨èƒ½\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462  # ç£…â†’åƒå…‹\n",
    "    input_df['momentum_x']     = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y']     = input_df['velocity_y'] * mass_kg\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "\n",
    "    # çƒä¸çƒå‘˜ä¹‹é—´çš„ç©ºé—´ç‰¹å¾\n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball']   = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball']      = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x']   = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y']   = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed']      = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "\n",
    "    # æ—¶é—´æ’åºï¼ˆç¡®ä¿å¸§åºä¸€è‡´ï¼‰\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "\n",
    "    # æ·»åŠ æ»åç‰¹å¾ï¼ˆå†å² 1~3 å¸§ï¼‰\n",
    "    for lag in [1, 2, 3]:\n",
    "        input_df[f'x_lag{lag}']          = input_df.groupby(gcols)['x'].shift(lag)\n",
    "        input_df[f'y_lag{lag}']          = input_df.groupby(gcols)['y'].shift(lag)\n",
    "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "\n",
    "    # EMAï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰å¹³æ»‘é€Ÿåº¦å˜åŒ–\n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # Step 2ï¼šé«˜çº§ç‰¹å¾\n",
    "    print(\"æ­¥éª¤ 2/4 â–¶ æ·»åŠ é«˜çº§ç‰¹å¾...\")\n",
    "    input_df = add_advanced_features(input_df)\n",
    "\n",
    "    # Step 3ï¼šçƒå‘˜äº¤äº’ç‰¹å¾\n",
    "    print(\"æ­¥éª¤ 3/4 â–¶ æ·»åŠ çƒå‘˜äº¤äº’ç‰¹å¾...\")\n",
    "    if use_players_interactions:\n",
    "        print(\"âœ… å·²å¯ç”¨çƒå‘˜äº¤äº’ç‰¹å¾è®¡ç®—ï¼ˆuse_players_interactions=Trueï¼‰\")\n",
    "\n",
    "        agg_rows = []  # ç”¨äºä¿å­˜æ¯ä¸€å¸§çš„äº¤äº’ç»Ÿè®¡ç»“æœ\n",
    "\n",
    "        # æŒ‰æ¯”èµ› (game_id)ã€å›åˆ (play_id)ã€å¸§ (frame_id) åˆ†ç»„\n",
    "        # æ¯ä¸€ç»„åŒ…å«åŒä¸€å¸§ä¸­æ‰€æœ‰çƒå‘˜çš„ç©ºé—´çŠ¶æ€\n",
    "        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "            n = len(grp)\n",
    "            nfl_ids = grp['nfl_id'].to_numpy()\n",
    "\n",
    "            # å¦‚æœå­˜åœ¨ player_to_predictï¼Œåˆ™åªå¯¹éœ€è¦é¢„æµ‹çš„çƒå‘˜è®¡ç®—äº¤äº’ç‰¹å¾\n",
    "            compute_mask = (\n",
    "                grp['player_to_predict'].to_numpy().astype(bool)\n",
    "                if 'player_to_predict' in grp.columns\n",
    "                else np.ones(n, dtype=bool)\n",
    "            )\n",
    "\n",
    "            # è‹¥è¯¥å¸§çƒå‘˜ä¸è¶³ 2 äººï¼Œåˆ™æ„é€ ç©ºè®°å½•ï¼ˆNaNï¼‰\n",
    "            if n < 2:\n",
    "                for nid in nfl_ids[compute_mask]:\n",
    "                    agg_rows.append({\n",
    "                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                        'distance_to_player_mean_offense': np.nan,\n",
    "                        'distance_to_player_min_offense': np.nan,\n",
    "                        'distance_to_player_max_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                        'angle_to_player_mean_offense': np.nan,\n",
    "                        'angle_to_player_min_offense': np.nan,\n",
    "                        'angle_to_player_max_offense': np.nan,\n",
    "                        'distance_to_player_mean_defense': np.nan,\n",
    "                        'distance_to_player_min_defense': np.nan,\n",
    "                        'distance_to_player_max_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                        'angle_to_player_mean_defense': np.nan,\n",
    "                        'angle_to_player_min_defense': np.nan,\n",
    "                        'angle_to_player_max_defense': np.nan,\n",
    "                        'nearest_opponent_dist': np.nan,\n",
    "                        'nearest_opponent_angle': np.nan,\n",
    "                        'nearest_opponent_rel_speed': np.nan,\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # è·å–çƒå‘˜ä½ç½®ä¸é€Ÿåº¦ä¿¡æ¯\n",
    "            x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "            y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "            is_offense = grp['is_offense'].to_numpy()\n",
    "            is_defense = grp['is_defense'].to_numpy()\n",
    "\n",
    "            # --- è®¡ç®—ä¸¤ä¸¤é—´çš„å‡ ä½•å…³ç³»çŸ©é˜µ ---\n",
    "            dx = x[None, :] - x[:, None]      # Xæ–¹å‘å·®å€¼çŸ©é˜µ\n",
    "            dy = y[None, :] - y[:, None]      # Yæ–¹å‘å·®å€¼çŸ©é˜µ\n",
    "            dist = np.sqrt(dx ** 2 + dy ** 2) # æ¬§æ°è·ç¦»çŸ©é˜µ (nÃ—n)\n",
    "            angle_mat = np.arctan2(-dy, -dx)  # ä»çƒå‘˜ i æŒ‡å‘ j çš„è§’åº¦\n",
    "\n",
    "            # --- ç›¸å¯¹é€Ÿåº¦çŸ©é˜µ ---\n",
    "            dvx = vx[:, None] - vx[None, :]\n",
    "            dvy = vy[:, None] - vy[None, :]\n",
    "            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "\n",
    "            # --- å„ç±»æ©ç  ---\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            np.fill_diagonal(offense_mask, False)  # è‡ªèº«ä¸å‚ä¸è®¡ç®—\n",
    "\n",
    "            defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "            np.fill_diagonal(defense_mask, False)\n",
    "\n",
    "            opp_mask = (is_offense[:, None] != is_offense[None, :])  # å¯¹æ‰‹é˜µè¥\n",
    "            np.fill_diagonal(opp_mask, False)\n",
    "\n",
    "            # --- å°†è‡ªèº«çš„å€¼ç½®ä¸º NaNï¼Œé¿å…å¹²æ‰°ç»Ÿè®¡ ---\n",
    "            dist_diag_nan  = dist.copy();      np.fill_diagonal(dist_diag_nan,  np.nan)\n",
    "            rel_diag_nan   = rel_speed.copy(); np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "            angle_diag_nan = angle_mat.copy(); np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "\n",
    "            # --- å®šä¹‰ç»Ÿè®¡å‡½æ•°ï¼šè®¡ç®—å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ ---\n",
    "            def masked_stats(mat, mask):\n",
    "                masked = np.where(mask, mat, np.nan)\n",
    "                cnt  = mask.sum(axis=1)\n",
    "                mean = np.nanmean(masked, axis=1)\n",
    "                amin = np.nanmin(masked, axis=1)\n",
    "                amax = np.nanmax(masked, axis=1)\n",
    "                zero = cnt == 0  # è‹¥æ— æœ‰æ•ˆæ•°æ®åˆ™ç½® NaN\n",
    "                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "                return mean, amin, amax\n",
    "\n",
    "            # --- è®¡ç®—è¿›æ”»æ–¹ä¹‹é—´çš„è·ç¦»ã€ç›¸å¯¹é€Ÿåº¦ã€è§’åº¦ç»Ÿè®¡ ---\n",
    "            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "\n",
    "            # --- è®¡ç®—é˜²å®ˆæ–¹ä¹‹é—´çš„ç»Ÿè®¡ ---\n",
    "            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "\n",
    "            # --- è®¡ç®—æœ€è¿‘å¯¹æ‰‹è·ç¦»/è§’åº¦/ç›¸å¯¹é€Ÿåº¦ ---\n",
    "            masked_dist_opp = np.where(opp_mask, dist_diag_nan, np.nan)\n",
    "            nearest_dist = np.nanmin(masked_dist_opp, axis=1)\n",
    "            nearest_idx  = np.nanargmin(masked_dist_opp, axis=1)\n",
    "            all_nan = ~np.isfinite(nearest_dist)\n",
    "            nearest_idx_safe = nearest_idx.copy()\n",
    "            nearest_idx_safe[all_nan] = 0\n",
    "\n",
    "            nearest_angle = np.take_along_axis(angle_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_rel   = np.take_along_axis(rel_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_angle[all_nan] = np.nan\n",
    "            nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "            # --- æ±‡æ€»æ¯ä½çƒå‘˜çš„äº¤äº’ç‰¹å¾ ---\n",
    "            for idx, nid in enumerate(nfl_ids):\n",
    "                if not compute_mask[idx]:\n",
    "                    continue\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                    'distance_to_player_min_offense':  d_min_o[idx],\n",
    "                    'distance_to_player_max_offense':  d_max_o[idx],\n",
    "                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                    'relative_velocity_magnitude_min_offense':  v_min_o[idx],\n",
    "                    'relative_velocity_magnitude_max_offense':  v_max_o[idx],\n",
    "                    'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                    'angle_to_player_min_offense':  a_min_o[idx],\n",
    "                    'angle_to_player_max_offense':  a_max_o[idx],\n",
    "\n",
    "                    'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                    'distance_to_player_min_defense':  d_min_d[idx],\n",
    "                    'distance_to_player_max_defense':  d_max_d[idx],\n",
    "                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                    'relative_velocity_magnitude_min_defense':  v_min_d[idx],\n",
    "                    'relative_velocity_magnitude_max_defense':  v_max_d[idx],\n",
    "                    'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                    'angle_to_player_min_defense':  a_min_d[idx],\n",
    "                    'angle_to_player_max_defense':  a_max_d[idx],\n",
    "\n",
    "                    'nearest_opponent_dist':      nearest_dist[idx],\n",
    "                    'nearest_opponent_angle':     nearest_angle[idx],\n",
    "                    'nearest_opponent_rel_speed': nearest_rel[idx],\n",
    "                })\n",
    "\n",
    "        # åˆå¹¶äº¤äº’ç‰¹å¾å›ä¸»è¡¨\n",
    "        interaction_agg = pd.DataFrame(agg_rows)\n",
    "        input_df = input_df.merge(\n",
    "            interaction_agg,\n",
    "            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        print(\"âœ… çƒå‘˜äº¤äº’ç‰¹å¾æ·»åŠ å®Œæˆã€‚\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš ï¸ è·³è¿‡çƒå‘˜äº¤äº’ç‰¹å¾è®¡ç®—ï¼ˆuse_players_interactions=Falseï¼‰ã€‚\")\n",
    "\n",
    "\n",
    "    # Step 4ï¼šæ„å»ºè¾“å…¥åºåˆ—\n",
    "    print(\"æ­¥éª¤ 4/4 â–¶ æ„å»ºè¾“å…¥åºåˆ—æ ·æœ¬...\")\n",
    "\n",
    "\n",
    "    feature_cols = [\n",
    "        # â€”â€” åŸºç¡€æ ¸å¿ƒç‰¹å¾ï¼ˆCore, 6ï¼‰â€”â€”\n",
    "        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n",
    "\n",
    "        # â€”â€” è§’åº¦ç¼–ç ç‰¹å¾ï¼ˆAngles encoded, 4ï¼‰â€”â€”\n",
    "        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n",
    "\n",
    "        # â€”â€” çƒå‘˜é™æ€ç‰¹å¾ï¼ˆPlayer, 2ï¼‰â€”â€”\n",
    "        'player_height_feet', 'player_weight',\n",
    "\n",
    "        # â€”â€” åŠ¨æ€è¿åŠ¨ç‰¹å¾ï¼ˆMotion, 7ï¼‰â€”â€”\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "\n",
    "        # â€”â€” è§’è‰²èº«ä»½ç‰¹å¾ï¼ˆRoles, 5ï¼‰â€”â€”\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "\n",
    "        # â€”â€” çƒä¸çƒå‘˜ç©ºé—´ç‰¹å¾ï¼ˆBall relation, 5ï¼‰â€”â€”\n",
    "        'distance_to_ball', 'angle_to_ball',\n",
    "        'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "\n",
    "        # â€”â€” åŸå§‹æ—¶åºæ»åç‰¹å¾ï¼ˆOriginal temporal lags, 15ï¼‰â€”â€”\n",
    "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "\n",
    "        # â€”â€” è·ç¦»å˜åŒ–é€Ÿç‡ç‰¹å¾ï¼ˆDistance rate, 3ï¼‰â€”â€”\n",
    "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "\n",
    "        # â€”â€” ç›®æ ‡æ–¹å‘å¯¹é½ç‰¹å¾ï¼ˆTarget alignment, 3ï¼‰â€”â€”\n",
    "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "\n",
    "        # â€”â€” å¤šçª—å£æ»šåŠ¨ç‰¹å¾ï¼ˆMulti-window rolling, 24ï¼‰â€”â€”\n",
    "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "\n",
    "        # â€”â€” æ‰©å±•æ—¶åºæ»åç‰¹å¾ï¼ˆExtended lags, 8ï¼‰â€”â€”\n",
    "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "\n",
    "        # â€”â€” é€Ÿåº¦å˜åŒ–ç‰¹å¾ï¼ˆVelocity change, 4ï¼‰â€”â€”\n",
    "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "\n",
    "        # â€”â€” åœºåœ°ä½ç½®ç‰¹å¾ï¼ˆField position, 2ï¼‰â€”â€”\n",
    "        'dist_from_sideline', 'dist_from_endzone',\n",
    "\n",
    "        # â€”â€” è§’è‰²ç›¸å…³ç‰¹å¾ï¼ˆRole-specific, 3ï¼‰â€”â€”\n",
    "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "\n",
    "        # â€”â€” æ—¶é—´è¿›ç¨‹ç‰¹å¾ï¼ˆTime, 2ï¼‰â€”â€”\n",
    "        'frames_elapsed', 'normalized_time',\n",
    "\n",
    "        # â€”â€” çƒå‘˜äº¤äº’ç‰¹å¾ï¼ˆPlayer interactions, 21ï¼‰â€”â€”\n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n",
    "        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n",
    "    ]\n",
    "\n",
    "    # ä¿ç•™å½“å‰æ•°æ®é›†ä¸­ç¡®å®å­˜åœ¨çš„ç‰¹å¾åˆ—ï¼ˆé¿å… KeyErrorï¼‰\n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "\n",
    "    # è¾“å‡ºç‰¹å¾æ•°é‡ä¿¡æ¯\n",
    "    print(f\"âœ… ä½¿ç”¨çš„ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)} ä¸ª\")\n",
    "\n",
    "\n",
    "    # CREATE SEQUENCES\n",
    "\n",
    "    # è®¾ç½®ç´¢å¼•å¹¶æŒ‰çƒå‘˜åˆ†ç»„\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "\n",
    "    # é€‰æ‹©ç›®æ ‡æ•°æ®æºï¼ˆè®­ç»ƒæˆ–æµ‹è¯•ï¼‰\n",
    "    target_rows = output_df if is_training else test_template\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "\n",
    "    # å­˜å‚¨å®¹å™¨\n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "\n",
    "    # éå†æ¯ä¸ªçƒå‘˜çš„æ—¶é—´åºåˆ—\n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"â³ æ­£åœ¨åˆ›å»ºåºåˆ—\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "\n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # æå–æ—¶é—´çª—å£ï¼ˆæœ€å window_size å¸§ï¼‰\n",
    "        input_window = group_df.tail(window_size)\n",
    "\n",
    "        # è‹¥å¸§æ•°ä¸è¶³åˆ™è¿›è¡Œå¡«å……\n",
    "        if len(input_window) < window_size:\n",
    "            if is_training:\n",
    "                print(f\"Skipping sequence with insufficient history for {key}\")\n",
    "                continue\n",
    "            print(f\"âš ï¸ åºåˆ—ä¸è¶³ {window_size} å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š{key}\")\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df  = pd.concat([input_window.iloc[0:1]] * pad_len, ignore_index=True)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "\n",
    "        # ç¼ºå¤±å€¼å‰å‘/åå‘å¡«å……\n",
    "        input_window = input_window.ffill().bfill().fillna(0.0)\n",
    "\n",
    "        seq = input_window[feature_cols].values\n",
    "        if np.isnan(seq).any():\n",
    "            print(f\"âš ï¸ åœ¨ {key} çš„åºåˆ—ä¸­å‘ç° NaNï¼Œå·²ç”¨ 0.0 æ›¿æ¢ã€‚\")\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "\n",
    "        sequences.append(seq)\n",
    "\n",
    "        # è‹¥ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œåˆ™è®¡ç®—é¢„æµ‹ç›®æ ‡ï¼ˆÎ”x, Î”yï¼‰\n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id'] == row['game_id']) &\n",
    "                (output_df['play_id'] == row['play_id']) &\n",
    "                (output_df['nfl_id']  == row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "\n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "\n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "\n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "\n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "\n",
    "    print(f\"\\nâœ… å…±ç”Ÿæˆ {len(sequences)} ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« {len(feature_cols)} ä¸ªç‰¹å¾ã€‚\")\n",
    "\n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n",
    "    return sequences, sequence_ids, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bb9c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ å¼€å§‹æ„å»ºæ—¶åºæ ·æœ¬ï¼ˆåŒ…å«é«˜çº§ç‰¹å¾ï¼‰\n",
      "================================================================================\n",
      "çª—å£å¤§å°ï¼ˆwindow_sizeï¼‰: 12\n",
      "æ­¥éª¤ 1/4 â–¶ æ·»åŠ åŸºç¡€ç‰¹å¾...\n",
      "æ­¥éª¤ 2/4 â–¶ æ·»åŠ é«˜çº§ç‰¹å¾...\n",
      "æ­£åœ¨æ·»åŠ é«˜çº§ç‰¹å¾...\n",
      "ç‰¹å¾å¢å¼ºåæ€»åˆ—æ•°: 112\n",
      "æ­¥éª¤ 3/4 â–¶ æ·»åŠ çƒå‘˜äº¤äº’ç‰¹å¾...\n",
      "âœ… å·²å¯ç”¨çƒå‘˜äº¤äº’ç‰¹å¾è®¡ç®—ï¼ˆuse_players_interactions=Trueï¼‰\n",
      "âœ… çƒå‘˜äº¤äº’ç‰¹å¾æ·»åŠ å®Œæˆã€‚\n",
      "æ­¥éª¤ 4/4 â–¶ æ„å»ºè¾“å…¥åºåˆ—æ ·æœ¬...\n",
      "âœ… ä½¿ç”¨çš„ç‰¹å¾åˆ—æ•°é‡: 114 ä¸ª\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9cbc86ed24bbc85859be3e78a8707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "â³ æ­£åœ¨åˆ›å»ºåºåˆ—:   0%|          | 0/46045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sequence with insufficient history for (2023091004, 1594, 52453)\n",
      "Skipping sequence with insufficient history for (2023091004, 1594, 52430)\n",
      "Skipping sequence with insufficient history for (2023091004, 4122, 41233)\n",
      "Skipping sequence with insufficient history for (2023091008, 1292, 48516)\n",
      "Skipping sequence with insufficient history for (2023091008, 1292, 55921)\n",
      "Skipping sequence with insufficient history for (2023091008, 1292, 54597)\n",
      "Skipping sequence with insufficient history for (2023091400, 318, 52430)\n",
      "Skipping sequence with insufficient history for (2023091700, 1728, 46087)\n",
      "Skipping sequence with insufficient history for (2023091700, 1728, 54473)\n",
      "Skipping sequence with insufficient history for (2023091709, 493, 53531)\n",
      "Skipping sequence with insufficient history for (2023091709, 493, 53601)\n",
      "Skipping sequence with insufficient history for (2023091709, 493, 56042)\n",
      "Skipping sequence with insufficient history for (2023091712, 81, 43327)\n",
      "Skipping sequence with insufficient history for (2023091712, 81, 42357)\n",
      "Skipping sequence with insufficient history for (2023091712, 2480, 43299)\n",
      "Skipping sequence with insufficient history for (2023091712, 2480, 42357)\n",
      "Skipping sequence with insufficient history for (2023092406, 3048, 44819)\n",
      "Skipping sequence with insufficient history for (2023092410, 1352, 46150)\n",
      "Skipping sequence with insufficient history for (2023092410, 2619, 53565)\n",
      "Skipping sequence with insufficient history for (2023092410, 2619, 52425)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 53476)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 54679)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 55919)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 41282)\n",
      "Skipping sequence with insufficient history for (2023100807, 4264, 43986)\n",
      "Skipping sequence with insufficient history for (2023100807, 4264, 55005)\n",
      "Skipping sequence with insufficient history for (2023100810, 2151, 54583)\n",
      "Skipping sequence with insufficient history for (2023100810, 2151, 54519)\n",
      "Skipping sequence with insufficient history for (2023100900, 2328, 42381)\n",
      "Skipping sequence with insufficient history for (2023100900, 2328, 47807)\n",
      "Skipping sequence with insufficient history for (2023100900, 2962, 45038)\n",
      "Skipping sequence with insufficient history for (2023100900, 2962, 54597)\n",
      "Skipping sequence with insufficient history for (2023101509, 1300, 42361)\n",
      "Skipping sequence with insufficient history for (2023101509, 1300, 53549)\n",
      "Skipping sequence with insufficient history for (2023101509, 3424, 46902)\n",
      "Skipping sequence with insufficient history for (2023101509, 3424, 45185)\n",
      "Skipping sequence with insufficient history for (2023101511, 3940, 43351)\n",
      "Skipping sequence with insufficient history for (2023101511, 3940, 54475)\n",
      "Skipping sequence with insufficient history for (2023102205, 3383, 47877)\n",
      "Skipping sequence with insufficient history for (2023102205, 3383, 54473)\n",
      "Skipping sequence with insufficient history for (2023102300, 353, 55887)\n",
      "Skipping sequence with insufficient history for (2023102908, 236, 47859)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 39984)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 43351)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 43353)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 54548)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 54808)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 55931)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 47859)\n",
      "Skipping sequence with insufficient history for (2023102909, 3040, 54533)\n",
      "Skipping sequence with insufficient history for (2023102909, 3040, 47847)\n",
      "Skipping sequence with insufficient history for (2023102913, 950, 53476)\n",
      "Skipping sequence with insufficient history for (2023102913, 950, 46093)\n",
      "Skipping sequence with insufficient history for (2023110509, 1738, 44830)\n",
      "Skipping sequence with insufficient history for (2023110509, 1738, 41282)\n",
      "Skipping sequence with insufficient history for (2023110600, 412, 54475)\n",
      "Skipping sequence with insufficient history for (2023110600, 2558, 54475)\n",
      "Skipping sequence with insufficient history for (2023111204, 341, 47816)\n",
      "Skipping sequence with insufficient history for (2023111204, 341, 56527)\n",
      "Skipping sequence with insufficient history for (2023111204, 341, 43336)\n",
      "Skipping sequence with insufficient history for (2023111210, 1200, 43373)\n",
      "Skipping sequence with insufficient history for (2023111210, 1200, 55884)\n",
      "Skipping sequence with insufficient history for (2023111211, 679, 52547)\n",
      "Skipping sequence with insufficient history for (2023111211, 679, 47974)\n",
      "Skipping sequence with insufficient history for (2023111211, 679, 54475)\n",
      "Skipping sequence with insufficient history for (2023111211, 2937, 55969)\n",
      "Skipping sequence with insufficient history for (2023111211, 2937, 54475)\n",
      "Skipping sequence with insufficient history for (2023111211, 3659, 55969)\n",
      "Skipping sequence with insufficient history for (2023111211, 3659, 54475)\n",
      "Skipping sequence with insufficient history for (2023111903, 1604, 45571)\n",
      "Skipping sequence with insufficient history for (2023111903, 1604, 56220)\n",
      "Skipping sequence with insufficient history for (2023111906, 587, 43327)\n",
      "Skipping sequence with insufficient history for (2023111906, 587, 41282)\n",
      "Skipping sequence with insufficient history for (2023112604, 1355, 55888)\n",
      "Skipping sequence with insufficient history for (2023112604, 1355, 56052)\n",
      "Skipping sequence with insufficient history for (2023112604, 1588, 55888)\n",
      "Skipping sequence with insufficient history for (2023112604, 1588, 42357)\n",
      "Skipping sequence with insufficient history for (2023120400, 351, 44872)\n",
      "Skipping sequence with insufficient history for (2023120400, 351, 46095)\n",
      "Skipping sequence with insufficient history for (2023121007, 2198, 54583)\n",
      "Skipping sequence with insufficient history for (2023121007, 2198, 41282)\n",
      "Skipping sequence with insufficient history for (2023121705, 174, 56042)\n",
      "Skipping sequence with insufficient history for (2023123102, 3589, 56117)\n",
      "Skipping sequence with insufficient history for (2023123102, 3589, 52495)\n",
      "Skipping sequence with insufficient history for (2023123102, 3589, 42489)\n",
      "Skipping sequence with insufficient history for (2024010704, 1678, 43700)\n",
      "Skipping sequence with insufficient history for (2024010704, 1678, 54475)\n",
      "Skipping sequence with insufficient history for (2024010707, 3669, 55880)\n",
      "Skipping sequence with insufficient history for (2024010707, 3669, 56109)\n",
      "\n",
      "âœ… å…±ç”Ÿæˆ 45956 ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« 114 ä¸ªç‰¹å¾ã€‚\n",
      "âœ… å·²å‡†å¤‡å¥½ 45956 ä¸ªæ—¶åºæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« 114 ä¸ªç‰¹å¾ã€‚\n",
      "ğŸ“ æ¯ä¸ªæ ·æœ¬çš„çª—å£é•¿åº¦ä¸º 12 å¸§ã€‚\n",
      "ğŸ“Š ç¤ºä¾‹æ ·æœ¬å½¢çŠ¶ï¼š(12, 114)\n",
      "ğŸ¯ è®­ç»ƒç›®æ ‡ç¤ºä¾‹ï¼šdx=(21,), dy=(21,)\n",
      "ğŸ†” æ ·æœ¬ç´¢å¼•æ•°é‡ï¼š45956\n"
     ]
    }
   ],
   "source": [
    "# ç”Ÿæˆæ—¶åºæ ·æœ¬ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols = prepare_sequences(\n",
    "    train_input,\n",
    "    output_df=train_output,\n",
    "    test_template=test_template,\n",
    "    is_training=True,\n",
    "    window_size=Config.WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# è¾“å‡ºç»“æœä¿¡æ¯\n",
    "print(f\"âœ… å·²å‡†å¤‡å¥½ {len(sequences)} ä¸ªæ—¶åºæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« {len(feature_cols)} ä¸ªç‰¹å¾ã€‚\")\n",
    "print(f\"ğŸ“ æ¯ä¸ªæ ·æœ¬çš„çª—å£é•¿åº¦ä¸º {Config.WINDOW_SIZE} å¸§ã€‚\")\n",
    "print(f\"ğŸ“Š ç¤ºä¾‹æ ·æœ¬å½¢çŠ¶ï¼š{sequences[0].shape}\")\n",
    "print(f\"ğŸ¯ è®­ç»ƒç›®æ ‡ç¤ºä¾‹ï¼šdx={targets_dx[0].shape}, dy={targets_dy[0].shape}\")\n",
    "print(f\"ğŸ†” æ ·æœ¬ç´¢å¼•æ•°é‡ï¼š{len(sequence_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49a609a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ä½¿ç”¨ pickle ä¿å­˜å¯¹è±¡åˆ° D:\\æ•°æ®\\Kaggle\\2026 å¹´ NFL å¤§æ•°æ®ç¢— - é¢„æµ‹\\DATA_DIR000\\train_data_cache_unpad.pkl\n"
     ]
    }
   ],
   "source": [
    "# === ä¿å­˜ pkl æ–‡ä»¶ ===\n",
    "save_path = Config.DATA_DIR / \"train_data_cache_unpad.pkl\"\n",
    "\n",
    "# ä¿å­˜ï¼ˆä¿ç•™ numpy å¯¹è±¡ç±»å‹ï¼‰\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"sequences\": sequences,\n",
    "        \"targets_dx\": targets_dx,\n",
    "        \"targets_dy\": targets_dy,\n",
    "        \"targets_frame_ids\": targets_frame_ids,\n",
    "        \"sequence_ids\": sequence_ids,\n",
    "        \"feature_cols\": feature_cols\n",
    "    }, f)\n",
    "\n",
    "print(f\"âœ… å·²ä½¿ç”¨ pickle ä¿å­˜å¯¹è±¡åˆ° {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c943f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®åŠ è½½æˆåŠŸåˆ° *_2 å˜é‡ï¼\n"
     ]
    }
   ],
   "source": [
    "# === åŠ è½½ pkl æ–‡ä»¶ ===\n",
    "data_path = Config.DATA_DIR / \"train_data_cache_unpad.pkl\"\n",
    "\n",
    "with open(save_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "sequences_2 = data[\"sequences\"]\n",
    "targets_dx_2 = data[\"targets_dx\"]\n",
    "targets_dy_2 = data[\"targets_dy\"]\n",
    "targets_frame_ids_2 = data[\"targets_frame_ids\"]\n",
    "sequence_ids_2 = data[\"sequence_ids\"]\n",
    "feature_cols_2 = data[\"feature_cols\"]\n",
    "\n",
    "print(\"âœ… æ•°æ®åŠ è½½æˆåŠŸåˆ° *_2 å˜é‡ï¼\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db5ab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å“ˆå¸ŒéªŒè¯\n",
    "def get_obj_type(obj):\n",
    "    \"\"\"è¯†åˆ«å¯¹è±¡ç±»å‹ï¼ˆæ”¯æŒåµŒå¥—ç»“æ„ï¼‰\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return f\"numpy.ndarray shape={obj.shape} dtype={obj.dtype}\"\n",
    "    elif isinstance(obj, list):\n",
    "        if len(obj) == 0:\n",
    "            return \"list (empty)\"\n",
    "        inner_type = get_obj_type(obj[0])\n",
    "        return f\"list[{inner_type}] len={len(obj)}\"\n",
    "    elif isinstance(obj, dict):\n",
    "        return f\"dict (keys={len(obj.keys())})\"\n",
    "    elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "        return type(obj).__name__\n",
    "    else:\n",
    "        return f\"object ({type(obj).__name__})\"\n",
    "\n",
    "\n",
    "def hash_object(obj) -> str:\n",
    "    \"\"\"ç”Ÿæˆå¯¹è±¡çš„ç¨³å®šå“ˆå¸Œç­¾åï¼ˆæ”¯æŒ list / dict / ndarray / æ ‡é‡ï¼‰\"\"\"\n",
    "    m = hashlib.sha256()\n",
    "\n",
    "    def _update(o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            m.update(o.tobytes())\n",
    "            m.update(str(o.shape).encode())\n",
    "            m.update(str(o.dtype).encode())\n",
    "        elif isinstance(o, (list, tuple)):\n",
    "            m.update(f\"len={len(o)}\".encode())\n",
    "            for item in o:\n",
    "                _update(item)\n",
    "        elif isinstance(o, dict):\n",
    "            for k in sorted(o.keys()):\n",
    "                m.update(str(k).encode())\n",
    "                _update(o[k])\n",
    "        elif isinstance(o, (str, int, float, bool, type(None))):\n",
    "            m.update(str(o).encode())\n",
    "        else:\n",
    "            m.update(repr(o).encode())\n",
    "\n",
    "    _update(obj)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def verify_objects(obj1, obj2, name=\"å˜é‡\"):\n",
    "    \"\"\"\n",
    "    æ¯”è¾ƒä¸¤ä¸ªå¯¹è±¡å†…å®¹æ˜¯å¦ä¸€è‡´ï¼Œå¹¶è¾“å‡ºç®€æ´å•è¡Œä¿¡æ¯ã€‚\n",
    "    æ”¯æŒï¼šlist / dict / numpy.ndarray / æ ‡é‡\n",
    "    \"\"\"\n",
    "    type1 = get_obj_type(obj1)\n",
    "    type2 = get_obj_type(obj2)\n",
    "    hash1 = hash_object(obj1)\n",
    "    hash2 = hash_object(obj2)\n",
    "    same = hash1 == hash2\n",
    "\n",
    "    print(f\"[{name}]\")\n",
    "    print(f\"{type1}\")\n",
    "    print(f\"{type2}\")\n",
    "    print(f\"{hash1[:24]}\")\n",
    "    print(f\"{hash2[:24]}\")\n",
    "    print(f\"{'âœ…ä¸€è‡´' if same else 'âŒä¸åŒ'}\")\n",
    "    return same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f21e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sequences]\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=45956\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=45956\n",
      "f6eb874159c60cf6c9905132\n",
      "f6eb874159c60cf6c9905132\n",
      "âœ…ä¸€è‡´\n",
      "[targets_dx]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "2fcee115f7b268256baf2a46\n",
      "2fcee115f7b268256baf2a46\n",
      "âœ…ä¸€è‡´\n",
      "[targets_dy]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "1aaf324c0fe11c32418be172\n",
      "1aaf324c0fe11c32418be172\n",
      "âœ…ä¸€è‡´\n",
      "[targets_frame_ids]\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=45956\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=45956\n",
      "22c0bb381dffffe48d1cb725\n",
      "22c0bb381dffffe48d1cb725\n",
      "âœ…ä¸€è‡´\n",
      "[sequence_ids]\n",
      "list[dict (keys=4)] len=45956\n",
      "list[dict (keys=4)] len=45956\n",
      "e39ef11edaf968ab84a15ffe\n",
      "e39ef11edaf968ab84a15ffe\n",
      "âœ…ä¸€è‡´\n",
      "[feature_cols]\n",
      "list[str] len=114\n",
      "list[str] len=114\n",
      "44cf06a02b1e47da769fea6a\n",
      "44cf06a02b1e47da769fea6a\n",
      "âœ…ä¸€è‡´\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_objects(sequences, sequences_2, \"sequences\")\n",
    "verify_objects(targets_dx, targets_dx_2, \"targets_dx\")\n",
    "verify_objects(targets_dy, targets_dy_2, \"targets_dy\")\n",
    "verify_objects(targets_frame_ids, targets_frame_ids_2, \"targets_frame_ids\")\n",
    "verify_objects(sequence_ids, sequence_ids_2, \"sequence_ids\")\n",
    "verify_objects(feature_cols, feature_cols_2, \"feature_cols\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdfe27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68b0ed12",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ•°æ®è‹¥å¸§æ•°ä¸è¶³ å¡«å……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2fa5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(input_df, output_df=None, test_template=None,\n",
    "                      is_training=True, window_size=8,\n",
    "                      use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n",
    "    \"\"\"\n",
    "    æ„å»ºåŒ…å«æ‰€æœ‰é«˜çº§ç‰¹å¾çš„æ—¶åºæ ·æœ¬åºåˆ—ï¼ˆæ”¯æŒè®­ç»ƒä¸æµ‹è¯•ï¼‰ã€‚\n",
    "    input_dfï¼šè¾“å…¥ç‰¹å¾æ•°æ®\n",
    "    output_dfï¼šè®­ç»ƒæ ‡ç­¾ï¼ˆä»…åœ¨ is_training=True æ—¶ä½¿ç”¨ï¼‰\n",
    "    test_templateï¼šæµ‹è¯•æ¨¡æ¿ï¼ˆä»…åœ¨é¢„æµ‹æ—¶ä½¿ç”¨ï¼‰\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"ğŸš€ å¼€å§‹æ„å»ºæ—¶åºæ ·æœ¬ï¼ˆåŒ…å«é«˜çº§ç‰¹å¾ï¼‰\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"çª—å£å¤§å°ï¼ˆwindow_sizeï¼‰: {window_size}\")\n",
    "\n",
    "    input_df = input_df.copy()\n",
    "\n",
    "    # Step 1ï¼šåŸºç¡€ç‰¹å¾æ„å»º\n",
    "    print(\"æ­¥éª¤ 1/4 â–¶ æ·»åŠ åŸºç¡€ç‰¹å¾...\")\n",
    "\n",
    "    # å°†çƒå‘˜èº«é«˜ä»â€œè‹±å°º-è‹±å¯¸â€æ ¼å¼è½¬æ¢ä¸ºè‹±å°ºå°æ•°\n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "\n",
    "    # è®¡ç®—é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„ xã€y åˆ†é‡\n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "\n",
    "    # è§’åº¦ç‰¹å¾ï¼šå°†æœå‘è§’(o)ä¸è¿åŠ¨æ–¹å‘(dir)ç¼–ç ä¸ºæ­£å¼¦/ä½™å¼¦å½¢å¼\n",
    "    input_df['o_sin']  = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['o_cos']  = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "\n",
    "    # è§’è‰²ç‰¹å¾ï¼šè¿›æ”»/é˜²å®ˆ/ä¼ çƒ/æ¥çƒ/é˜²å®ˆè¦†ç›–\n",
    "    input_df['is_offense']  = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense']  = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer']   = (input_df['player_role'] == 'Passer').astype(int)\n",
    "\n",
    "    # ç‰©ç†ç‰¹å¾ï¼šåŠ¨é‡ä¸åŠ¨èƒ½\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462  # ç£…â†’åƒå…‹\n",
    "    input_df['momentum_x']     = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y']     = input_df['velocity_y'] * mass_kg\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "\n",
    "    # çƒä¸çƒå‘˜ä¹‹é—´çš„ç©ºé—´ç‰¹å¾\n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball']   = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball']      = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x']   = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y']   = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed']      = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "\n",
    "    # æ—¶é—´æ’åºï¼ˆç¡®ä¿å¸§åºä¸€è‡´ï¼‰\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "\n",
    "    # æ·»åŠ æ»åç‰¹å¾ï¼ˆå†å² 1~3 å¸§ï¼‰\n",
    "    for lag in [1, 2, 3]:\n",
    "        input_df[f'x_lag{lag}']          = input_df.groupby(gcols)['x'].shift(lag)\n",
    "        input_df[f'y_lag{lag}']          = input_df.groupby(gcols)['y'].shift(lag)\n",
    "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "\n",
    "    # EMAï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰å¹³æ»‘é€Ÿåº¦å˜åŒ–\n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # Step 2ï¼šé«˜çº§ç‰¹å¾\n",
    "    print(\"æ­¥éª¤ 2/4 â–¶ æ·»åŠ é«˜çº§ç‰¹å¾...\")\n",
    "    input_df = add_advanced_features(input_df)\n",
    "\n",
    "    # Step 3ï¼šçƒå‘˜äº¤äº’ç‰¹å¾\n",
    "    print(\"æ­¥éª¤ 3/4 â–¶ æ·»åŠ çƒå‘˜äº¤äº’ç‰¹å¾...\")\n",
    "    if use_players_interactions:\n",
    "        print(\"âœ… å·²å¯ç”¨çƒå‘˜äº¤äº’ç‰¹å¾è®¡ç®—ï¼ˆuse_players_interactions=Trueï¼‰\")\n",
    "\n",
    "        agg_rows = []  # ç”¨äºä¿å­˜æ¯ä¸€å¸§çš„äº¤äº’ç»Ÿè®¡ç»“æœ\n",
    "\n",
    "        # æŒ‰æ¯”èµ› (game_id)ã€å›åˆ (play_id)ã€å¸§ (frame_id) åˆ†ç»„\n",
    "        # æ¯ä¸€ç»„åŒ…å«åŒä¸€å¸§ä¸­æ‰€æœ‰çƒå‘˜çš„ç©ºé—´çŠ¶æ€\n",
    "        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "            n = len(grp)\n",
    "            nfl_ids = grp['nfl_id'].to_numpy()\n",
    "\n",
    "            # å¦‚æœå­˜åœ¨ player_to_predictï¼Œåˆ™åªå¯¹éœ€è¦é¢„æµ‹çš„çƒå‘˜è®¡ç®—äº¤äº’ç‰¹å¾\n",
    "            compute_mask = (\n",
    "                grp['player_to_predict'].to_numpy().astype(bool)\n",
    "                if 'player_to_predict' in grp.columns\n",
    "                else np.ones(n, dtype=bool)\n",
    "            )\n",
    "\n",
    "            # è‹¥è¯¥å¸§çƒå‘˜ä¸è¶³ 2 äººï¼Œåˆ™æ„é€ ç©ºè®°å½•ï¼ˆNaNï¼‰\n",
    "            if n < 2:\n",
    "                for nid in nfl_ids[compute_mask]:\n",
    "                    agg_rows.append({\n",
    "                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                        'distance_to_player_mean_offense': np.nan,\n",
    "                        'distance_to_player_min_offense': np.nan,\n",
    "                        'distance_to_player_max_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                        'angle_to_player_mean_offense': np.nan,\n",
    "                        'angle_to_player_min_offense': np.nan,\n",
    "                        'angle_to_player_max_offense': np.nan,\n",
    "                        'distance_to_player_mean_defense': np.nan,\n",
    "                        'distance_to_player_min_defense': np.nan,\n",
    "                        'distance_to_player_max_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                        'angle_to_player_mean_defense': np.nan,\n",
    "                        'angle_to_player_min_defense': np.nan,\n",
    "                        'angle_to_player_max_defense': np.nan,\n",
    "                        'nearest_opponent_dist': np.nan,\n",
    "                        'nearest_opponent_angle': np.nan,\n",
    "                        'nearest_opponent_rel_speed': np.nan,\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # è·å–çƒå‘˜ä½ç½®ä¸é€Ÿåº¦ä¿¡æ¯\n",
    "            x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "            y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "            is_offense = grp['is_offense'].to_numpy()\n",
    "            is_defense = grp['is_defense'].to_numpy()\n",
    "\n",
    "            # --- è®¡ç®—ä¸¤ä¸¤é—´çš„å‡ ä½•å…³ç³»çŸ©é˜µ ---\n",
    "            dx = x[None, :] - x[:, None]      # Xæ–¹å‘å·®å€¼çŸ©é˜µ\n",
    "            dy = y[None, :] - y[:, None]      # Yæ–¹å‘å·®å€¼çŸ©é˜µ\n",
    "            dist = np.sqrt(dx ** 2 + dy ** 2) # æ¬§æ°è·ç¦»çŸ©é˜µ (nÃ—n)\n",
    "            angle_mat = np.arctan2(-dy, -dx)  # ä»çƒå‘˜ i æŒ‡å‘ j çš„è§’åº¦\n",
    "\n",
    "            # --- ç›¸å¯¹é€Ÿåº¦çŸ©é˜µ ---\n",
    "            dvx = vx[:, None] - vx[None, :]\n",
    "            dvy = vy[:, None] - vy[None, :]\n",
    "            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "\n",
    "            # --- å„ç±»æ©ç  ---\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            np.fill_diagonal(offense_mask, False)  # è‡ªèº«ä¸å‚ä¸è®¡ç®—\n",
    "\n",
    "            defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "            np.fill_diagonal(defense_mask, False)\n",
    "\n",
    "            opp_mask = (is_offense[:, None] != is_offense[None, :])  # å¯¹æ‰‹é˜µè¥\n",
    "            np.fill_diagonal(opp_mask, False)\n",
    "\n",
    "            # --- å°†è‡ªèº«çš„å€¼ç½®ä¸º NaNï¼Œé¿å…å¹²æ‰°ç»Ÿè®¡ ---\n",
    "            dist_diag_nan  = dist.copy();      np.fill_diagonal(dist_diag_nan,  np.nan)\n",
    "            rel_diag_nan   = rel_speed.copy(); np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "            angle_diag_nan = angle_mat.copy(); np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "\n",
    "            # --- å®šä¹‰ç»Ÿè®¡å‡½æ•°ï¼šè®¡ç®—å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ ---\n",
    "            def masked_stats(mat, mask):\n",
    "                masked = np.where(mask, mat, np.nan)\n",
    "                cnt  = mask.sum(axis=1)\n",
    "                mean = np.nanmean(masked, axis=1)\n",
    "                amin = np.nanmin(masked, axis=1)\n",
    "                amax = np.nanmax(masked, axis=1)\n",
    "                zero = cnt == 0  # è‹¥æ— æœ‰æ•ˆæ•°æ®åˆ™ç½® NaN\n",
    "                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "                return mean, amin, amax\n",
    "\n",
    "            # --- è®¡ç®—è¿›æ”»æ–¹ä¹‹é—´çš„è·ç¦»ã€ç›¸å¯¹é€Ÿåº¦ã€è§’åº¦ç»Ÿè®¡ ---\n",
    "            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "\n",
    "            # --- è®¡ç®—é˜²å®ˆæ–¹ä¹‹é—´çš„ç»Ÿè®¡ ---\n",
    "            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "\n",
    "            # --- è®¡ç®—æœ€è¿‘å¯¹æ‰‹è·ç¦»/è§’åº¦/ç›¸å¯¹é€Ÿåº¦ ---\n",
    "            masked_dist_opp = np.where(opp_mask, dist_diag_nan, np.nan)\n",
    "            nearest_dist = np.nanmin(masked_dist_opp, axis=1)\n",
    "            nearest_idx  = np.nanargmin(masked_dist_opp, axis=1)\n",
    "            all_nan = ~np.isfinite(nearest_dist)\n",
    "            nearest_idx_safe = nearest_idx.copy()\n",
    "            nearest_idx_safe[all_nan] = 0\n",
    "\n",
    "            nearest_angle = np.take_along_axis(angle_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_rel   = np.take_along_axis(rel_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_angle[all_nan] = np.nan\n",
    "            nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "            # --- æ±‡æ€»æ¯ä½çƒå‘˜çš„äº¤äº’ç‰¹å¾ ---\n",
    "            for idx, nid in enumerate(nfl_ids):\n",
    "                if not compute_mask[idx]:\n",
    "                    continue\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                    'distance_to_player_min_offense':  d_min_o[idx],\n",
    "                    'distance_to_player_max_offense':  d_max_o[idx],\n",
    "                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                    'relative_velocity_magnitude_min_offense':  v_min_o[idx],\n",
    "                    'relative_velocity_magnitude_max_offense':  v_max_o[idx],\n",
    "                    'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                    'angle_to_player_min_offense':  a_min_o[idx],\n",
    "                    'angle_to_player_max_offense':  a_max_o[idx],\n",
    "\n",
    "                    'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                    'distance_to_player_min_defense':  d_min_d[idx],\n",
    "                    'distance_to_player_max_defense':  d_max_d[idx],\n",
    "                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                    'relative_velocity_magnitude_min_defense':  v_min_d[idx],\n",
    "                    'relative_velocity_magnitude_max_defense':  v_max_d[idx],\n",
    "                    'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                    'angle_to_player_min_defense':  a_min_d[idx],\n",
    "                    'angle_to_player_max_defense':  a_max_d[idx],\n",
    "\n",
    "                    'nearest_opponent_dist':      nearest_dist[idx],\n",
    "                    'nearest_opponent_angle':     nearest_angle[idx],\n",
    "                    'nearest_opponent_rel_speed': nearest_rel[idx],\n",
    "                })\n",
    "\n",
    "        # åˆå¹¶äº¤äº’ç‰¹å¾å›ä¸»è¡¨\n",
    "        interaction_agg = pd.DataFrame(agg_rows)\n",
    "        input_df = input_df.merge(\n",
    "            interaction_agg,\n",
    "            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        print(\"âœ… çƒå‘˜äº¤äº’ç‰¹å¾æ·»åŠ å®Œæˆã€‚\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš ï¸ è·³è¿‡çƒå‘˜äº¤äº’ç‰¹å¾è®¡ç®—ï¼ˆuse_players_interactions=Falseï¼‰ã€‚\")\n",
    "\n",
    "\n",
    "    # Step 4ï¼šæ„å»ºè¾“å…¥åºåˆ—\n",
    "    print(\"æ­¥éª¤ 4/4 â–¶ æ„å»ºè¾“å…¥åºåˆ—æ ·æœ¬...\")\n",
    "\n",
    "\n",
    "    feature_cols = [\n",
    "        # â€”â€” åŸºç¡€æ ¸å¿ƒç‰¹å¾ï¼ˆCore, 6ï¼‰â€”â€”\n",
    "        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n",
    "\n",
    "        # â€”â€” è§’åº¦ç¼–ç ç‰¹å¾ï¼ˆAngles encoded, 4ï¼‰â€”â€”\n",
    "        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n",
    "\n",
    "        # â€”â€” çƒå‘˜é™æ€ç‰¹å¾ï¼ˆPlayer, 2ï¼‰â€”â€”\n",
    "        'player_height_feet', 'player_weight',\n",
    "\n",
    "        # â€”â€” åŠ¨æ€è¿åŠ¨ç‰¹å¾ï¼ˆMotion, 7ï¼‰â€”â€”\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "\n",
    "        # â€”â€” è§’è‰²èº«ä»½ç‰¹å¾ï¼ˆRoles, 5ï¼‰â€”â€”\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "\n",
    "        # â€”â€” çƒä¸çƒå‘˜ç©ºé—´ç‰¹å¾ï¼ˆBall relation, 5ï¼‰â€”â€”\n",
    "        'distance_to_ball', 'angle_to_ball',\n",
    "        'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "\n",
    "        # â€”â€” åŸå§‹æ—¶åºæ»åç‰¹å¾ï¼ˆOriginal temporal lags, 15ï¼‰â€”â€”\n",
    "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "\n",
    "        # â€”â€” è·ç¦»å˜åŒ–é€Ÿç‡ç‰¹å¾ï¼ˆDistance rate, 3ï¼‰â€”â€”\n",
    "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "\n",
    "        # â€”â€” ç›®æ ‡æ–¹å‘å¯¹é½ç‰¹å¾ï¼ˆTarget alignment, 3ï¼‰â€”â€”\n",
    "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "\n",
    "        # â€”â€” å¤šçª—å£æ»šåŠ¨ç‰¹å¾ï¼ˆMulti-window rolling, 24ï¼‰â€”â€”\n",
    "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "\n",
    "        # â€”â€” æ‰©å±•æ—¶åºæ»åç‰¹å¾ï¼ˆExtended lags, 8ï¼‰â€”â€”\n",
    "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "\n",
    "        # â€”â€” é€Ÿåº¦å˜åŒ–ç‰¹å¾ï¼ˆVelocity change, 4ï¼‰â€”â€”\n",
    "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "\n",
    "        # â€”â€” åœºåœ°ä½ç½®ç‰¹å¾ï¼ˆField position, 2ï¼‰â€”â€”\n",
    "        'dist_from_sideline', 'dist_from_endzone',\n",
    "\n",
    "        # â€”â€” è§’è‰²ç›¸å…³ç‰¹å¾ï¼ˆRole-specific, 3ï¼‰â€”â€”\n",
    "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "\n",
    "        # â€”â€” æ—¶é—´è¿›ç¨‹ç‰¹å¾ï¼ˆTime, 2ï¼‰â€”â€”\n",
    "        'frames_elapsed', 'normalized_time',\n",
    "\n",
    "        # â€”â€” çƒå‘˜äº¤äº’ç‰¹å¾ï¼ˆPlayer interactions, 21ï¼‰â€”â€”\n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n",
    "        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n",
    "    ]\n",
    "\n",
    "    # ä¿ç•™å½“å‰æ•°æ®é›†ä¸­ç¡®å®å­˜åœ¨çš„ç‰¹å¾åˆ—ï¼ˆé¿å… KeyErrorï¼‰\n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "\n",
    "    # è¾“å‡ºç‰¹å¾æ•°é‡ä¿¡æ¯\n",
    "    print(f\"âœ… ä½¿ç”¨çš„ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)} ä¸ª\")\n",
    "\n",
    "\n",
    "    # CREATE SEQUENCES\n",
    "\n",
    "    # è®¾ç½®ç´¢å¼•å¹¶æŒ‰çƒå‘˜åˆ†ç»„\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "\n",
    "    # é€‰æ‹©ç›®æ ‡æ•°æ®æºï¼ˆè®­ç»ƒæˆ–æµ‹è¯•ï¼‰\n",
    "    target_rows = output_df if is_training else test_template\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "\n",
    "    # å­˜å‚¨å®¹å™¨\n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "\n",
    "    # éå†æ¯ä¸ªçƒå‘˜çš„æ—¶é—´åºåˆ—\n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"â³ æ­£åœ¨åˆ›å»ºåºåˆ—\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "\n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # æå–æ—¶é—´çª—å£ï¼ˆæœ€å window_size å¸§ï¼‰\n",
    "        input_window = group_df.tail(window_size)\n",
    "\n",
    "        # è‹¥å¸§æ•°ä¸è¶³åˆ™è¿›è¡Œå¡«å……\n",
    "        if len(input_window) < window_size:\n",
    "            # if is_training:\n",
    "            #     print(f\"Skipping sequence with insufficient history for {key}\")\n",
    "            #     continue\n",
    "            print(f\"âš ï¸ åºåˆ—ä¸è¶³ {window_size} å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š{key}\")\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df  = pd.concat([input_window.iloc[0:1]] * pad_len, ignore_index=True)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "\n",
    "        # ç¼ºå¤±å€¼å‰å‘/åå‘å¡«å……\n",
    "        input_window = input_window.ffill().bfill().fillna(0.0)\n",
    "\n",
    "        seq = input_window[feature_cols].values\n",
    "        if np.isnan(seq).any():\n",
    "            print(f\"âš ï¸ åœ¨ {key} çš„åºåˆ—ä¸­å‘ç° NaNï¼Œå·²ç”¨ 0.0 æ›¿æ¢ã€‚\")\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "\n",
    "        sequences.append(seq)\n",
    "\n",
    "        # è‹¥ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œåˆ™è®¡ç®—é¢„æµ‹ç›®æ ‡ï¼ˆÎ”x, Î”yï¼‰\n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id'] == row['game_id']) &\n",
    "                (output_df['play_id'] == row['play_id']) &\n",
    "                (output_df['nfl_id']  == row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "\n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "\n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "\n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "\n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "\n",
    "    print(f\"\\nâœ… å…±ç”Ÿæˆ {len(sequences)} ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« {len(feature_cols)} ä¸ªç‰¹å¾ã€‚\")\n",
    "\n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n",
    "    return sequences, sequence_ids, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc5970f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ å¼€å§‹æ„å»ºæ—¶åºæ ·æœ¬ï¼ˆåŒ…å«é«˜çº§ç‰¹å¾ï¼‰\n",
      "================================================================================\n",
      "çª—å£å¤§å°ï¼ˆwindow_sizeï¼‰: 12\n",
      "æ­¥éª¤ 1/4 â–¶ æ·»åŠ åŸºç¡€ç‰¹å¾...\n",
      "æ­¥éª¤ 2/4 â–¶ æ·»åŠ é«˜çº§ç‰¹å¾...\n",
      "æ­£åœ¨æ·»åŠ é«˜çº§ç‰¹å¾...\n",
      "ç‰¹å¾å¢å¼ºåæ€»åˆ—æ•°: 112\n",
      "æ­¥éª¤ 3/4 â–¶ æ·»åŠ çƒå‘˜äº¤äº’ç‰¹å¾...\n",
      "âœ… å·²å¯ç”¨çƒå‘˜äº¤äº’ç‰¹å¾è®¡ç®—ï¼ˆuse_players_interactions=Trueï¼‰\n",
      "âœ… çƒå‘˜äº¤äº’ç‰¹å¾æ·»åŠ å®Œæˆã€‚\n",
      "æ­¥éª¤ 4/4 â–¶ æ„å»ºè¾“å…¥åºåˆ—æ ·æœ¬...\n",
      "âœ… ä½¿ç”¨çš„ç‰¹å¾åˆ—æ•°é‡: 114 ä¸ª\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a73525e6cb448bb139fd128b7917e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "â³ æ­£åœ¨åˆ›å»ºåºåˆ—:   0%|          | 0/46045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091004, 1594, 52453)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091004, 1594, 52430)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091004, 4122, 41233)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091008, 1292, 48516)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091008, 1292, 55921)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091008, 1292, 54597)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091400, 318, 52430)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091700, 1728, 46087)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091700, 1728, 54473)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091709, 493, 53531)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091709, 493, 53601)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091709, 493, 56042)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091712, 81, 43327)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091712, 81, 42357)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091712, 2480, 43299)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023091712, 2480, 42357)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023092406, 3048, 44819)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023092410, 1352, 46150)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023092410, 2619, 53565)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023092410, 2619, 52425)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100110, 2747, 53476)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100110, 2747, 54679)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100110, 2747, 55919)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100110, 2747, 41282)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100807, 4264, 43986)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100807, 4264, 55005)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100810, 2151, 54583)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100810, 2151, 54519)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100900, 2328, 42381)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100900, 2328, 47807)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100900, 2962, 45038)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023100900, 2962, 54597)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023101509, 1300, 42361)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023101509, 1300, 53549)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023101509, 3424, 46902)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023101509, 3424, 45185)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023101511, 3940, 43351)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023101511, 3940, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102205, 3383, 47877)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102205, 3383, 54473)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102300, 353, 55887)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 236, 47859)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 39984)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 43351)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 43353)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 54548)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 54808)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 55931)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102908, 3160, 47859)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102909, 3040, 54533)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102909, 3040, 47847)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102913, 950, 53476)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023102913, 950, 46093)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023110509, 1738, 44830)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023110509, 1738, 41282)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023110600, 412, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023110600, 2558, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111204, 341, 47816)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111204, 341, 56527)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111204, 341, 43336)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111210, 1200, 43373)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111210, 1200, 55884)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 679, 52547)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 679, 47974)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 679, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 2937, 55969)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 2937, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 3659, 55969)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111211, 3659, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111903, 1604, 45571)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111903, 1604, 56220)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111906, 587, 43327)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023111906, 587, 41282)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023112604, 1355, 55888)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023112604, 1355, 56052)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023112604, 1588, 55888)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023112604, 1588, 42357)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023120400, 351, 44872)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023120400, 351, 46095)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023121007, 2198, 54583)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023121007, 2198, 41282)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023121705, 174, 56042)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023123102, 3589, 56117)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023123102, 3589, 52495)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2023123102, 3589, 42489)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2024010704, 1678, 43700)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2024010704, 1678, 54475)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2024010707, 3669, 55880)\n",
      "âš ï¸ åºåˆ—ä¸è¶³ 12 å¸§ï¼Œè‡ªåŠ¨å¡«å……ï¼š(2024010707, 3669, 56109)\n",
      "\n",
      "âœ… å…±ç”Ÿæˆ 46045 ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« 114 ä¸ªç‰¹å¾ã€‚\n",
      "âœ… å·²å‡†å¤‡å¥½ 46045 ä¸ªæ—¶åºæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« 114 ä¸ªç‰¹å¾ã€‚\n",
      "ğŸ“ æ¯ä¸ªæ ·æœ¬çš„çª—å£é•¿åº¦ä¸º 12 å¸§ã€‚\n",
      "ğŸ“Š ç¤ºä¾‹æ ·æœ¬å½¢çŠ¶ï¼š(12, 114)\n",
      "ğŸ¯ è®­ç»ƒç›®æ ‡ç¤ºä¾‹ï¼šdx=(21,), dy=(21,)\n",
      "ğŸ†” æ ·æœ¬ç´¢å¼•æ•°é‡ï¼š46045\n"
     ]
    }
   ],
   "source": [
    "# ç”Ÿæˆæ—¶åºæ ·æœ¬ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols = prepare_sequences(\n",
    "    train_input,\n",
    "    output_df=train_output,\n",
    "    test_template=test_template,\n",
    "    is_training=True,\n",
    "    window_size=Config.WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# è¾“å‡ºç»“æœä¿¡æ¯\n",
    "print(f\"âœ… å·²å‡†å¤‡å¥½ {len(sequences)} ä¸ªæ—¶åºæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« {len(feature_cols)} ä¸ªç‰¹å¾ã€‚\")\n",
    "print(f\"ğŸ“ æ¯ä¸ªæ ·æœ¬çš„çª—å£é•¿åº¦ä¸º {Config.WINDOW_SIZE} å¸§ã€‚\")\n",
    "print(f\"ğŸ“Š ç¤ºä¾‹æ ·æœ¬å½¢çŠ¶ï¼š{sequences[0].shape}\")\n",
    "print(f\"ğŸ¯ è®­ç»ƒç›®æ ‡ç¤ºä¾‹ï¼šdx={targets_dx[0].shape}, dy={targets_dy[0].shape}\")\n",
    "print(f\"ğŸ†” æ ·æœ¬ç´¢å¼•æ•°é‡ï¼š{len(sequence_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e66d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ä½¿ç”¨ pickle ä¿å­˜å¯¹è±¡åˆ° D:\\æ•°æ®\\Kaggle\\2026 å¹´ NFL å¤§æ•°æ®ç¢— - é¢„æµ‹\\DATA_DIR000\\train_data_cache_pad.pkl\n"
     ]
    }
   ],
   "source": [
    "# === ä¿å­˜ pkl æ–‡ä»¶ ===\n",
    "save_path = Config.DATA_DIR / \"train_data_cache_pad.pkl\"\n",
    "\n",
    "# ä¿å­˜ï¼ˆä¿ç•™ numpy å¯¹è±¡ç±»å‹ï¼‰\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"sequences\": sequences,\n",
    "        \"targets_dx\": targets_dx,\n",
    "        \"targets_dy\": targets_dy,\n",
    "        \"targets_frame_ids\": targets_frame_ids,\n",
    "        \"sequence_ids\": sequence_ids,\n",
    "        \"feature_cols\": feature_cols\n",
    "    }, f)\n",
    "\n",
    "print(f\"âœ… å·²ä½¿ç”¨ pickle ä¿å­˜å¯¹è±¡åˆ° {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c89e84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®åŠ è½½æˆåŠŸåˆ° *_2 å˜é‡ï¼\n"
     ]
    }
   ],
   "source": [
    "# === åŠ è½½ pkl æ–‡ä»¶ ===\n",
    "data_path = Config.DATA_DIR / \"train_data_cache_pad.pkl\"\n",
    "\n",
    "with open(save_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "sequences_2 = data[\"sequences\"]\n",
    "targets_dx_2 = data[\"targets_dx\"]\n",
    "targets_dy_2 = data[\"targets_dy\"]\n",
    "targets_frame_ids_2 = data[\"targets_frame_ids\"]\n",
    "sequence_ids_2 = data[\"sequence_ids\"]\n",
    "feature_cols_2 = data[\"feature_cols\"]\n",
    "\n",
    "print(\"âœ… æ•°æ®åŠ è½½æˆåŠŸåˆ° *_2 å˜é‡ï¼\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ead9625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sequences]\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=46045\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=46045\n",
      "5d2684f49c9ca71f64c41636\n",
      "5d2684f49c9ca71f64c41636\n",
      "âœ…ä¸€è‡´\n",
      "[targets_dx]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "d99185f0ebe88b2786faf5c8\n",
      "d99185f0ebe88b2786faf5c8\n",
      "âœ…ä¸€è‡´\n",
      "[targets_dy]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "73183fc597e548240f481540\n",
      "73183fc597e548240f481540\n",
      "âœ…ä¸€è‡´\n",
      "[targets_frame_ids]\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=46045\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=46045\n",
      "2b505c057b7da08aed772945\n",
      "2b505c057b7da08aed772945\n",
      "âœ…ä¸€è‡´\n",
      "[sequence_ids]\n",
      "list[dict (keys=4)] len=46045\n",
      "list[dict (keys=4)] len=46045\n",
      "9a1568866c31a8c79c11dfc4\n",
      "9a1568866c31a8c79c11dfc4\n",
      "âœ…ä¸€è‡´\n",
      "[feature_cols]\n",
      "list[str] len=114\n",
      "list[str] len=114\n",
      "44cf06a02b1e47da769fea6a\n",
      "44cf06a02b1e47da769fea6a\n",
      "âœ…ä¸€è‡´\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_objects(sequences, sequences_2, \"sequences\")\n",
    "verify_objects(targets_dx, targets_dx_2, \"targets_dx\")\n",
    "verify_objects(targets_dy, targets_dy_2, \"targets_dy\")\n",
    "verify_objects(targets_frame_ids, targets_frame_ids_2, \"targets_frame_ids\")\n",
    "verify_objects(sequence_ids, sequence_ids_2, \"sequence_ids\")\n",
    "verify_objects(feature_cols, feature_cols_2, \"feature_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58acad49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e645de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917ea26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fcffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n",
    "models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348c5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf76bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
