{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9248debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# NFL BIG DATA BOWL 2026 - 完整解决方案\n",
    "# 使用时间序列特征预测传球战术中球员移动\n",
    "# ================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98da3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 标准库 ---\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 第三方库 ---\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 深度学习（PyTorch）---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    LinearLR,\n",
    "    OneCycleLR,\n",
    "    SequentialLR,\n",
    ")\n",
    "\n",
    "# 忽略警告信息（可选）\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f0e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    全局配置类\n",
    "    用于控制模型训练、数据路径、设备参数等全局设置\n",
    "    \"\"\"\n",
    "\n",
    "    # 📁 数据路径设置\n",
    "    DATA_DIR            = Path(r\"D:\\数据\\Kaggle\\2026 年 NFL 大数据碗 - 预测\\DATA_DIR000\")\n",
    "    NN_PRETRAIN_DIR     = Path(r\"D:\\数据\\Kaggle\\2026 年 NFL 大数据碗 - 预测\\DATA_DIR000\\bigru-public\")\n",
    "\n",
    "    # ⚙️ 通用参数\n",
    "    SEED                = 42                               # 随机种子，保证结果可复现\n",
    "    N_FOLDS             = 5                                # 交叉验证折数\n",
    "    USE_PLAYERS_INTERACTIONS = True                        # 是否使用球员交互特征\n",
    "\n",
    "    # 🏟️ 球场参数（单位：码）\n",
    "    FIELD_X_MIN         = 0.0\n",
    "    FIELD_X_MAX         = 120.0\n",
    "    FIELD_Y_MIN         = 0.0\n",
    "    FIELD_Y_MAX         = 53.3\n",
    "\n",
    "    # 🧠 模型相关参数\n",
    "    WINDOW_SIZE         = 12                               # 时序窗口长度\n",
    "    DEVICE              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \"\"\"设置随机种子以确保实验结果可复现。\"\"\"\n",
    "    import random, os\n",
    "    random.seed(SEED)                               # Python 内置随机数模块\n",
    "    np.random.seed(SEED)                            # NumPy 随机数生成器\n",
    "    torch.manual_seed(SEED)                         # PyTorch CPU 随机数\n",
    "    torch.cuda.manual_seed_all(SEED)                # PyTorch GPU 随机数（全部 GPU）\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)        # 控制哈希随机化（字典等结构）\n",
    "    torch.backends.cudnn.deterministic = True       # 使用确定性算法，保证结果一致\n",
    "    torch.backends.cudnn.benchmark = False          # 关闭自动优化，避免结果不稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Config 中定义的随机种子\n",
    "def set_global_seeds(seed: int = 42):\n",
    "    \"\"\"设置随机种子以确保实验结果可复现。\"\"\"\n",
    "    import random, os\n",
    "    random.seed(seed)                               # Python 内置随机数模块\n",
    "    np.random.seed(seed)                            # NumPy 随机数生成器\n",
    "    torch.manual_seed(seed)                         # PyTorch CPU 随机数\n",
    "    torch.cuda.manual_seed_all(seed)                # PyTorch GPU 随机数（全部 GPU）\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)        # 控制哈希随机化（字典等结构）\n",
    "    torch.backends.cudnn.deterministic = True       # 使用确定性算法，保证结果一致\n",
    "    torch.backends.cudnn.benchmark = False          # 关闭自动优化，避免结果不稳定\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8797b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(debug_fraction: float = 1.0):\n",
    "    \"\"\"加载训练集与测试集数据，可按比例加载部分数据用于调试。\"\"\"\n",
    "    print(\"开始加载数据...\")\n",
    "\n",
    "    # 构造训练数据文件路径（共 18 周）\n",
    "    train_input_files  = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\"  for w in range(1, 19)]\n",
    "    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "\n",
    "    # 过滤掉不存在的文件\n",
    "    train_input_files  = [f for f in train_input_files  if f.exists()]\n",
    "    train_output_files = [f for f in train_output_files if f.exists()]\n",
    "    print(f\"检测到 {len(train_input_files)} 周的有效训练数据。\")\n",
    "\n",
    "    # 读取并合并训练数据，同时添加 week 列区分周次\n",
    "    train_input = pd.concat(\n",
    "        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_input_files, start=1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    train_output = pd.concat(\n",
    "        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_output_files, start=1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # 读取测试集数据\n",
    "    test_input    = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n",
    "    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n",
    "    print(f\"已加载 {len(train_input):,} 条输入记录，{len(train_output):,} 条输出记录。\")\n",
    "\n",
    "    # 若开启调试模式，仅使用部分比赛数据\n",
    "    if debug_fraction < 1.0:\n",
    "        unique_game_ids  = train_input[\"game_id\"].unique()  # 所有比赛 ID\n",
    "        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n",
    "        train_input  = train_input[train_input[\"game_id\"].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        train_output = train_output[train_output[\"game_id\"].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        print(f\"调试模式：使用 {len(train_input):,} 条输入记录，共 {len(sampled_game_ids)} 场比赛。\")\n",
    "\n",
    "    return train_input, train_output, test_input, test_template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90308916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"自定义异常类：用于在评分时向参赛者提示错误信息。\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    计算 NFL 竞赛的预测得分（RMSE）。\n",
    "    \n",
    "    参数：\n",
    "        solution:   官方真实数据（DataFrame）\n",
    "        submission: 选手提交的预测结果（DataFrame）\n",
    "        row_id_column_name: 唯一标识列名（通常为 'id'）\n",
    "\n",
    "    返回：\n",
    "        float 类型的 RMSE（Root Mean Squared Error，均方根误差）\n",
    "\n",
    "    要求：\n",
    "        solution 和 submission 必须都包含：\n",
    "            - 'id'：每条样本唯一标识（由 game_id, play_id, nfl_id, frame_id 组成）\n",
    "            - 'x' 和 'y'：球员在场上的位置坐标\n",
    "\n",
    "    示例：\n",
    "        >>> row_id_column_name = 'id'\n",
    "        >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n",
    "        >>> submission = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n",
    "        >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "        0.0913\n",
    "    \"\"\"\n",
    "\n",
    "    TARGET = ['x', 'y']  # 需要计算误差的目标列\n",
    "\n",
    "    # 检查唯一标识列是否存在\n",
    "    if row_id_column_name not in solution.columns:\n",
    "        raise ParticipantVisibleError(f\"Solution 文件缺少必要列: '{row_id_column_name}'\")\n",
    "    if row_id_column_name not in submission.columns:\n",
    "        raise ParticipantVisibleError(f\"Submission 文件缺少必要列: '{row_id_column_name}'\")\n",
    "\n",
    "    # 检查预测与真实文件中是否包含目标列 'x'、'y'\n",
    "    missing_in_solution = set(TARGET) - set(solution.columns)\n",
    "    missing_in_submission = set(TARGET) - set(submission.columns)\n",
    "\n",
    "    if missing_in_solution:\n",
    "        raise ParticipantVisibleError(f\"Solution 文件缺少列: {missing_in_solution}\")\n",
    "    if missing_in_submission:\n",
    "        raise ParticipantVisibleError(f\"Submission 文件缺少列: {missing_in_submission}\")\n",
    "\n",
    "    # 只保留 id、x、y 列（防止额外无关列干扰）\n",
    "    submission = submission[['id'] + TARGET]\n",
    "\n",
    "    # 按 id 合并真实值与预测值\n",
    "    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n",
    "\n",
    "    # 检查预测结果中是否存在 NaN\n",
    "    nanx_in_pred = merged_df['x_pred'].isna().sum()\n",
    "    nany_in_pred = merged_df['y_pred'].isna().sum()\n",
    "    if nanx_in_pred > 0:\n",
    "        print(f\"警告：预测结果中 x_pred 存在 {nanx_in_pred} 个 NaN 值。\")\n",
    "    if nany_in_pred > 0:\n",
    "        print(f\"警告：预测结果中 y_pred 存在 {nany_in_pred} 个 NaN 值。\")\n",
    "\n",
    "    # 检查真实值中对应 NaN 预测的样本是否也有缺失\n",
    "    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n",
    "    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n",
    "    if nanx_in_true > 0:\n",
    "        print(f\"警告：真实值中 x_true 存在 {nanx_in_true} 个与 NaN 预测对应的缺失。\")\n",
    "    if nany_in_true > 0:\n",
    "        print(f\"警告：真实值中 y_true 存在 {nany_in_true} 个与 NaN 预测对应的缺失。\")\n",
    "\n",
    "    # 计算 RMSE（对 x 与 y 分别计算 MSE 后取平均再开方）\n",
    "    rmse = np.sqrt(\n",
    "        0.5 * (\n",
    "            mean_squared_error(merged_df['x_true'], merged_df['x_pred']) +\n",
    "            mean_squared_error(merged_df['y_true'], merged_df['y_pred'])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return float(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeec1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_interactions_for_play_frames(df_play_frames: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    计算每个 (game_id, play_id, frame_id, nfl_id) 的球员交互特征。\n",
    "\n",
    "    输出特征包括：\n",
    "      - 距离统计（进攻方/防守方的平均、最小、最大距离）\n",
    "      - 相对速度统计（平均、最小、最大）\n",
    "      - 角度统计（进攻方/防守方的平均、最小、最大角度，平均角度为圆均值）\n",
    "      - 最近对手的距离、角度、相对速度\n",
    "\n",
    "    注意：\n",
    "      - 若存在列 'player_to_predict'，则只计算该列为 True 的球员；\n",
    "      - 否则，对所有球员计算。\n",
    "    \"\"\"\n",
    "\n",
    "    out_rows = []  # 存储每一帧的计算结果\n",
    "\n",
    "    # 遍历每个 (比赛, 战术, 帧)\n",
    "    for (g, p, f), grp in df_play_frames.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "        n = len(grp)\n",
    "        if n == 0:\n",
    "            continue  # 跳过空帧\n",
    "\n",
    "        # 提取基本信息\n",
    "        nfl_ids = grp['nfl_id'].to_numpy()\n",
    "        x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "        y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "        vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "        vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "        is_off = grp['is_offense'].to_numpy().astype(bool)\n",
    "\n",
    "        # 若存在 player_to_predict，则仅计算标记为 True 的球员\n",
    "        compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n",
    "\n",
    "        # 计算两两球员间的几何关系（n×n矩阵）\n",
    "        dx = x[None, :] - x[:, None]\n",
    "        dy = y[None, :] - y[:, None]\n",
    "        dist = np.sqrt(dx * dx + dy * dy)                # 距离矩阵\n",
    "        angle_mat = np.arctan2(-dy, -dx)                 # 从球员 i 指向 j 的角度\n",
    "        dvx = vx[:, None] - vx[None, :]\n",
    "        dvy = vy[:, None] - vy[None, :]\n",
    "        rel_speed = np.sqrt(dvx * dvx + dvy * dvy)       # 相对速度矩阵\n",
    "\n",
    "        # 掩码矩阵：进攻方、 防守方、 对手方\n",
    "        opp_mask = (is_off[:, None] != is_off[None, :])  # True 表示对手关系\n",
    "        np.fill_diagonal(opp_mask, False)                # 自身置 False\n",
    "\n",
    "        mask_off = np.broadcast_to(is_off[None, :], (n, n)).copy()   # 进攻方矩阵\n",
    "        mask_def = np.broadcast_to(~is_off[None, :], (n, n)).copy()  # 防守方矩阵\n",
    "        np.fill_diagonal(mask_off, False)\n",
    "        np.fill_diagonal(mask_def, False)\n",
    "\n",
    "        # 最近对手（基于最小距离）\n",
    "        dist_opp = np.where(opp_mask, dist, np.nan)\n",
    "        nearest_dist = np.nanmin(dist_opp, axis=1)\n",
    "        nearest_idx = np.nanargmin(dist_opp, axis=1)\n",
    "        all_nan = ~np.isfinite(nearest_dist)\n",
    "        nearest_idx_safe = nearest_idx.copy()\n",
    "        nearest_idx_safe[all_nan] = 0  # 防止 nanargmin 报错\n",
    "        nearest_angle = np.take_along_axis(angle_mat, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "        nearest_rel   = np.take_along_axis(rel_speed, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "        nearest_angle[all_nan] = np.nan\n",
    "        nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "        # 计算距离的统计特征\n",
    "        d_off = np.where(mask_off, dist, np.nan)\n",
    "        d_def = np.where(mask_def, dist, np.nan)\n",
    "        d_mean_o = np.nanmean(d_off, axis=1); d_min_o = np.nanmin(d_off, axis=1); d_max_o = np.nanmax(d_off, axis=1)\n",
    "        d_mean_d = np.nanmean(d_def, axis=1); d_min_d = np.nanmin(d_def, axis=1); d_max_d = np.nanmax(d_def, axis=1)\n",
    "\n",
    "        # 计算相对速度的统计特征\n",
    "        v_off = np.where(mask_off, rel_speed, np.nan)\n",
    "        v_def = np.where(mask_def, rel_speed, np.nan)\n",
    "        v_mean_o = np.nanmean(v_off, axis=1); v_min_o = np.nanmin(v_off, axis=1); v_max_o = np.nanmax(v_off, axis=1)\n",
    "        v_mean_d = np.nanmean(v_def, axis=1); v_min_d = np.nanmin(v_def, axis=1); v_max_d = np.nanmax(v_def, axis=1)\n",
    "\n",
    "        # 角度的平均值（圆均值）与最小/最大角\n",
    "        sinA = np.sin(angle_mat); cosA = np.cos(angle_mat)\n",
    "        cnt_off = mask_off.sum(axis=1).astype(np.float32)\n",
    "        cnt_def = mask_def.sum(axis=1).astype(np.float32)\n",
    "        denom_off = np.where(cnt_off > 0, cnt_off, np.nan)\n",
    "        denom_def = np.where(cnt_def > 0, cnt_def, np.nan)\n",
    "        sin_sum_off = (sinA * mask_off).sum(axis=1)\n",
    "        cos_sum_off = (cosA * mask_off).sum(axis=1)\n",
    "        sin_sum_def = (sinA * mask_def).sum(axis=1)\n",
    "        cos_sum_def = (cosA * mask_def).sum(axis=1)\n",
    "        a_mean_o = np.arctan2(sin_sum_off / denom_off, cos_sum_off / denom_off)\n",
    "        a_mean_d = np.arctan2(sin_sum_def / denom_def, cos_sum_def / denom_def)\n",
    "        a_off = np.where(mask_off, angle_mat, np.nan)\n",
    "        a_def = np.where(mask_def, angle_mat, np.nan)\n",
    "        a_min_o = np.nanmin(a_off, axis=1); a_max_o = np.nanmax(a_off, axis=1)\n",
    "        a_min_d = np.nanmin(a_def, axis=1); a_max_d = np.nanmax(a_def, axis=1)\n",
    "\n",
    "        # 生成输出结果，仅对 compute_mask 为 True 的球员输出\n",
    "        for idx, nid in enumerate(nfl_ids):\n",
    "            if not compute_mask[idx]:\n",
    "                continue\n",
    "            out_rows.append({\n",
    "                'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': int(nid),\n",
    "\n",
    "                # 进攻方距离和速度特征\n",
    "                'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                'distance_to_player_min_offense':  d_min_o[idx],\n",
    "                'distance_to_player_max_offense':  d_max_o[idx],\n",
    "                'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                'relative_velocity_magnitude_min_offense':  v_min_o[idx],\n",
    "                'relative_velocity_magnitude_max_offense':  v_max_o[idx],\n",
    "                'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                'angle_to_player_min_offense':  a_min_o[idx],\n",
    "                'angle_to_player_max_offense':  a_max_o[idx],\n",
    "\n",
    "                # 防守方距离和速度特征\n",
    "                'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                'distance_to_player_min_defense':  d_min_d[idx],\n",
    "                'distance_to_player_max_defense':  d_max_d[idx],\n",
    "                'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                'relative_velocity_magnitude_min_defense':  v_min_d[idx],\n",
    "                'relative_velocity_magnitude_max_defense':  v_max_d[idx],\n",
    "                'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                'angle_to_player_min_defense':  a_min_d[idx],\n",
    "                'angle_to_player_max_defense':  a_max_d[idx],\n",
    "\n",
    "                # 最近对手的特征\n",
    "                'nearest_opponent_dist': float(nearest_dist[idx]) if np.isfinite(nearest_dist[idx]) else np.nan,\n",
    "                'nearest_opponent_angle': float(nearest_angle[idx]) if np.isfinite(nearest_angle[idx]) else np.nan,\n",
    "                'nearest_opponent_rel_speed': float(nearest_rel[idx]) if np.isfinite(nearest_rel[idx]) else np.nan,\n",
    "            })\n",
    "\n",
    "    # 返回包含所有特征的数据表\n",
    "    return pd.DataFrame(\n",
    "        out_rows,\n",
    "        columns=[\n",
    "            'game_id', 'play_id', 'frame_id', 'nfl_id',\n",
    "\n",
    "            # 进攻方距离特征\n",
    "            'distance_to_player_mean_offense',\n",
    "            'distance_to_player_min_offense',\n",
    "            'distance_to_player_max_offense',\n",
    "\n",
    "            # 进攻方相对速度特征\n",
    "            'relative_velocity_magnitude_mean_offense',\n",
    "            'relative_velocity_magnitude_min_offense',\n",
    "            'relative_velocity_magnitude_max_offense',\n",
    "\n",
    "            # 进攻方角度特征\n",
    "            'angle_to_player_mean_offense',\n",
    "            'angle_to_player_min_offense',\n",
    "            'angle_to_player_max_offense',\n",
    "\n",
    "            # 防守方距离特征\n",
    "            'distance_to_player_mean_defense',\n",
    "            'distance_to_player_min_defense',\n",
    "            'distance_to_player_max_defense',\n",
    "\n",
    "            # 防守方相对速度特征\n",
    "            'relative_velocity_magnitude_mean_defense',\n",
    "            'relative_velocity_magnitude_min_defense',\n",
    "            'relative_velocity_magnitude_max_defense',\n",
    "\n",
    "            # 防守方角度特征\n",
    "            'angle_to_player_mean_defense',\n",
    "            'angle_to_player_min_defense',\n",
    "            'angle_to_player_max_defense',\n",
    "\n",
    "            # 最近对手特征\n",
    "            'nearest_opponent_dist',\n",
    "            'nearest_opponent_angle',\n",
    "            'nearest_opponent_rel_speed'\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dc5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def height_to_feet(height_str: str) -> Optional[float]:\n",
    "    \"\"\"将身高从 '英尺-英寸' 格式（如 '6-2'）转换为以英尺为单位的小数。\"\"\"\n",
    "    try:\n",
    "        ft, inches = map(int, height_str.split('-'))\n",
    "        return ft + inches / 12\n",
    "    except Exception:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f38eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"添加 30~40 个高级特征，以提升模型预测性能。\"\"\"\n",
    "    print(\"正在添加高级特征...\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols =             ['game_id', 'play_id', 'nfl_id']  # 分组键（每名球员的时间序列）\n",
    "\n",
    "    # 1. 距离变化特征（Distance Rate Features）\n",
    "    if 'distance_to_ball' in df.columns:\n",
    "        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)  # 距离变化\n",
    "        df['distance_to_ball_accel']  = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)  # 距离加速度\n",
    "        df['time_to_intercept']       = (df['distance_to_ball'] /\n",
    "                                        (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)  # 估算到球时间\n",
    "\n",
    "    # 2. 方向对齐特征（Target Alignment Features）\n",
    "    if 'ball_direction_x' in df.columns:\n",
    "        # 速度在球方向上的投影（对齐程度）\n",
    "        df['velocity_alignment'] = (\n",
    "            df['velocity_x'] * df['ball_direction_x'] +\n",
    "            df['velocity_y'] * df['ball_direction_y']\n",
    "        )\n",
    "        # 垂直于球方向的速度分量\n",
    "        df['velocity_perpendicular'] = (\n",
    "            df['velocity_x'] * (-df['ball_direction_y']) +\n",
    "            df['velocity_y'] * df['ball_direction_x']\n",
    "        )\n",
    "        # 加速度在球方向上的分量\n",
    "        if 'acceleration_x' in df.columns:\n",
    "            df['accel_alignment'] = (\n",
    "                df['acceleration_x'] * df['ball_direction_x'] +\n",
    "                df['acceleration_y'] * df['ball_direction_y']\n",
    "            )\n",
    "\n",
    "    # 3. 多窗口滚动特征（Multi-Window Rolling）\n",
    "    for window in [3, 5, 10]:\n",
    "        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in df.columns:\n",
    "                # 滚动平均值（平滑趋势）\n",
    "                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
    "                )\n",
    "                # 滚动标准差（短期波动性）\n",
    "                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).std()\n",
    "                ).fillna(0)\n",
    "\n",
    "    # 4. 滞后特征（Extended Lag Features）\n",
    "    for lag in [4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n",
    "\n",
    "    # 5. 速度变化特征（Velocity Change Features）\n",
    "    if 'velocity_x' in df.columns:\n",
    "        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)  # X方向速度变化\n",
    "        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)  # Y方向速度变化\n",
    "        df['speed_change']      = df.groupby(gcols)['s'].diff().fillna(0)           # 速度模变化\n",
    "        df['direction_change']  = df.groupby(gcols)['dir'].diff().fillna(0)         # 方向变化（角度）\n",
    "        # 修正角度跳变（如 -179° → 181°）\n",
    "        df['direction_change']  = df['direction_change'].apply(\n",
    "            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n",
    "        )\n",
    "\n",
    "    # 6. 场地位置特征（Field Position Features）\n",
    "    df['dist_from_left']      = df['y']                         # 距离左边线\n",
    "    df['dist_from_right']     = 53.3 - df['y']                  # 距离右边线\n",
    "    df['dist_from_sideline']  = np.minimum(df['dist_from_left'], df['dist_from_right'])  # 距离最近边线\n",
    "    df['dist_from_endzone']   = np.minimum(df['x'], 120 - df['x'])  # 距离端区（前后方向）\n",
    "\n",
    "    # 7. 角色特征（Role-Specific Features）\n",
    "    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n",
    "        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']              # 接球手运动方向匹配度\n",
    "        df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))  # 偏离球方向程度\n",
    "    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n",
    "        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']               # 防守球员逼近速度\n",
    "\n",
    "    # 8. 时间特征（Time Features）\n",
    "    df['frames_elapsed']  = df.groupby(gcols).cumcount()  # 当前帧序号\n",
    "    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n",
    "        lambda x: x / (x.max() + 1)                       # 归一化时间（0~1）\n",
    "    )\n",
    "\n",
    "    print(f\"特征增强后总列数: {len(df.columns)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b02322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    高斯噪声层（Gaussian Noise Layer）\n",
    "\n",
    "    作用：\n",
    "        在训练阶段向输入张量添加随机高斯噪声，用于数据增强或正则化，防止过拟合。\n",
    "        在推理（评估）阶段则不添加噪声。\n",
    "\n",
    "    参数：\n",
    "        stddev (float): 高斯噪声的标准差，控制噪声强度。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stddev: float):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播：\n",
    "            若处于训练模式，则向输入添加噪声；\n",
    "            若处于评估模式（model.eval()），则直接返回原输入。\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.stddev   # 生成与输入同形状的高斯噪声\n",
    "            return x + noise                            # 输入加噪\n",
    "        return x                                        # 测试时不加噪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f3b528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SeqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    时序预测模型（Sequence Model）\n",
    "    ---------------------------------------------------------\n",
    "    模型结构：\n",
    "        [输入序列特征] → [双向GRU] → [注意力池化] → [全连接预测头]\n",
    "                       → [累积输出（预测轨迹/位移）]\n",
    "\n",
    "    参数说明：\n",
    "        input_dim (int): 输入特征维度（每帧的特征数）\n",
    "        horizon (int):   预测时间步长（输出长度，例如未来N帧）\n",
    "\n",
    "    模块说明：\n",
    "        - GRU层：提取时序动态特征（双向）\n",
    "        - LayerNorm：规范化隐藏状态，提升数值稳定性\n",
    "        - Multi-Head Attention：基于全局上下文的加权汇聚（池化层）\n",
    "        - Linear Head：多层感知器输出预测值\n",
    "        - torch.cumsum：对预测结果进行累积求和（例如预测位移序列）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, horizon: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # ① 双向 GRU 层：输入→隐藏→输出\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim,              # 输入维度\n",
    "            128,                    # 隐藏层维度\n",
    "            num_layers=2,           # 堆叠层数\n",
    "            batch_first=True,       # 输入格式 (B, T, D)\n",
    "            dropout=0.1,            # 层间 dropout\n",
    "            bidirectional=True      # 双向 GRU\n",
    "        )\n",
    "\n",
    "        # ② 层归一化 LayerNorm：稳定双向拼接后的输出\n",
    "        self.pool_ln = nn.LayerNorm(256)  # 128×2 = 256\n",
    "\n",
    "        # ③ 注意力池化 Multi-Head Attention\n",
    "        self.pool_attn = nn.MultiheadAttention(\n",
    "            embed_dim=256,          # 输入维度与 LayerNorm 输出相同\n",
    "            num_heads=4,            # 多头注意力数\n",
    "            batch_first=True        # 支持 (B, T, D) 格式\n",
    "        )\n",
    "\n",
    "        # 注意力查询向量（可学习参数）\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 256))\n",
    "\n",
    "        # ④ 输出预测头（MLP）\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        参数：\n",
    "            x: 输入张量 [B, T, input_dim]\n",
    "        返回：\n",
    "            y: 输出张量 [B, horizon]\n",
    "        \"\"\"\n",
    "        # ① 提取时间特征\n",
    "        h, _ = self.gru(x)                # h: [B, T, 256]\n",
    "        B = h.size(0)\n",
    "\n",
    "        # ② 复制查询向量（每个batch一份）\n",
    "        q = self.pool_query.expand(B, -1, -1)  # [B, 1, 256]\n",
    "\n",
    "        # ③ 注意力池化：让查询q关注整段序列h\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))  # ctx: [B, 1, 256]\n",
    "\n",
    "        # ④ 通过预测头映射输出（逐步预测）\n",
    "        out = self.head(ctx.squeeze(1))   # [B, horizon]\n",
    "\n",
    "        # ⑤ 对预测结果进行累积求和，生成平滑轨迹\n",
    "        return torch.cumsum(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf8ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 模型构建、保存与加载工具函数\n",
    "# =========================================================\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "\n",
    "def build_axis_model_from_config(cfg):\n",
    "    \"\"\"\n",
    "    根据配置字典实例化 SeqModel 模型。\n",
    "\n",
    "    参数:\n",
    "        cfg (dict): 包含模型超参数的配置，例如：\n",
    "            {\n",
    "                'input_dim': 128,\n",
    "                'horizon': 20\n",
    "            }\n",
    "\n",
    "    返回:\n",
    "        model (SeqModel): 构建好的序列模型实例。\n",
    "    \"\"\"\n",
    "    input_dim = cfg['input_dim']\n",
    "    horizon = cfg['horizon']\n",
    "    print(f\"📦 从配置构建 SeqModel (input_dim={input_dim}, horizon={horizon})\")\n",
    "    return SeqModel(input_dim=input_dim, horizon=horizon)\n",
    "\n",
    "\n",
    "def _model_tag_from_instance(model):\n",
    "    \"\"\"\n",
    "    根据模型实例返回其类型标签（tag）。\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 模型实例。\n",
    "\n",
    "    返回:\n",
    "        str: 模型类型标签，例如 'seq'。\n",
    "    \"\"\"\n",
    "    if isinstance(model, SeqModel):\n",
    "        return 'seq'\n",
    "    return model.__class__.__name__.lower()\n",
    "\n",
    "\n",
    "def create_model_save_config(model, input_dim, horizon):\n",
    "    \"\"\"\n",
    "    生成一个最小化的模型配置，用于模型重建。\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 模型实例（目前仅支持 SeqModel）\n",
    "        input_dim (int): 输入维度\n",
    "        horizon (int): 预测步长\n",
    "\n",
    "    返回:\n",
    "        dict: 可保存到 checkpoint 的配置字典\n",
    "    \"\"\"\n",
    "    print(f\"🧩 创建模型配置: input_dim={input_dim}, horizon={horizon}\")\n",
    "    return {\n",
    "        'model': 'seq',\n",
    "        'input_dim': int(input_dim),\n",
    "        'horizon': int(horizon),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_axis_checkpoint(model, cfg, fold_dir, axis_name='x'):\n",
    "    \"\"\"\n",
    "    保存 SeqModel 的模型权重与配置到指定路径。\n",
    "\n",
    "    参数:\n",
    "        model (SeqModel): 模型实例\n",
    "        cfg (dict): 模型配置\n",
    "        fold_dir (str | Path): 保存目录\n",
    "        axis_name (str): 坐标轴名称 ('x' 或 'y')\n",
    "\n",
    "    保存内容:\n",
    "        fold_dir/axis_x.pt\n",
    "        fold_dir/axis_y.pt\n",
    "    \"\"\"\n",
    "    cfg = dict(cfg or {})\n",
    "    cfg['model'] = 'seq'\n",
    "    path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    torch.save({'state_dict': model.state_dict(), 'config': cfg}, str(path))\n",
    "    print(f\"💾 模型已保存至: {path}\")\n",
    "\n",
    "\n",
    "def load_axis_checkpoint(fold_dir, axis_name='x', device=None):\n",
    "    \"\"\"\n",
    "    从指定路径加载单个 SeqModel 检查点。\n",
    "\n",
    "    参数:\n",
    "        fold_dir (str | Path): 模型文件所在目录\n",
    "        axis_name (str): 模型轴标签 ('x' 或 'y')\n",
    "        device (torch.device | None): 加载目标设备\n",
    "\n",
    "    返回:\n",
    "        model (SeqModel): 已加载参数的模型实例\n",
    "        cfg (dict): 模型配置\n",
    "    \"\"\"\n",
    "    device = device or Config.DEVICE\n",
    "    ckpt_path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    print(f\"📂 正在加载模型 [{axis_name}] 来自 {ckpt_path}\")\n",
    "\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=device)\n",
    "    cfg = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "\n",
    "    try:\n",
    "        # 尝试严格匹配加载\n",
    "        model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        model.eval()\n",
    "        print(f\"✅ [{axis_name}] 模型加载成功 (strict=True)\")\n",
    "        return model, cfg\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{axis_name}] 严格加载失败: {e}，尝试使用 non-strict 模式...\")\n",
    "\n",
    "    # 宽松匹配加载（适用于参数名或结构略有差异的情况）\n",
    "    model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"⚠️ [{axis_name}] 非严格加载: 缺失参数={len(missing)}, 意外参数={len(unexpected)}\")\n",
    "    model.eval()\n",
    "    print(f\"✅ [{axis_name}] 模型加载成功 (strict=False)\")\n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "def load_folds_xy(num_folds, models_dir=None, device=None):\n",
    "    \"\"\"\n",
    "    加载所有折次 (folds) 的 X/Y 方向模型及其对应特征标准化器。\n",
    "\n",
    "    参数:\n",
    "        num_folds (int): 交叉验证折数\n",
    "        models_dir (str | Path | None): 模型文件目录\n",
    "        device (torch.device | None): 加载目标设备\n",
    "\n",
    "    返回:\n",
    "        (models_x, models_y, scalers, cfgs): 四个列表，分别为：\n",
    "            - models_x: 所有 x 方向模型\n",
    "            - models_y: 所有 y 方向模型\n",
    "            - scalers:  每折的特征标准化器（joblib 对象）\n",
    "            - cfgs:     每折的配置字典\n",
    "    \"\"\"\n",
    "    device = device or Config.DEVICE\n",
    "    base = Path(models_dir) if models_dir else Path('.')\n",
    "\n",
    "    models_x, models_y, scalers, cfgs = [], [], [], []\n",
    "\n",
    "    print(f\"📁 开始加载 {num_folds} 折模型 (路径: {base})\")\n",
    "\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        fold_dir = base / f'fold_{fold}'\n",
    "        try:\n",
    "            # 分别加载 X/Y 模型与 scaler\n",
    "            mx, cfgx = load_axis_checkpoint(fold_dir, 'x', device=device)\n",
    "            my, cfgy = load_axis_checkpoint(fold_dir, 'y', device=device)\n",
    "            scaler = joblib.load(str(fold_dir / 'lstm_feature_scaler_fold.joblib'))\n",
    "\n",
    "            models_x.append(mx)\n",
    "            models_y.append(my)\n",
    "            scalers.append(scaler)\n",
    "            cfgs.append(cfgx)\n",
    "            print(f\"✅ Fold {fold} 加载完成\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Fold {fold} 加载失败: {e}\")\n",
    "\n",
    "    print(f\"📦 共加载 {len(models_x)} 个 fold 模型（X/Y 方向）\")\n",
    "    return models_x, models_y, scalers, cfgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3da5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_predictions_xy(\n",
    "    models_x, models_y, scalers, X_test_unscaled, test_seq_ids, test_template, batch_size=1024\n",
    "):\n",
    "    \"\"\"\n",
    "    基于多个折（fold）的 X/Y 轴模型进行集成预测（Ensemble），输出最终坐标预测结果。\n",
    "\n",
    "    参数说明：\n",
    "        models_x, models_y : list\n",
    "            不同折的 SeqModel 模型（x 轴与 y 轴预测模型）。\n",
    "        scalers : list | None\n",
    "            对应每个 fold 的 StandardScaler 对象（或 None）。\n",
    "        X_test_unscaled : list[np.ndarray]\n",
    "            未缩放的测试序列列表，每个元素形状为 (T, F)。\n",
    "        test_seq_ids : list[dict]\n",
    "            每个序列对应的元数据，包含：\n",
    "            {'game_id', 'play_id', 'nfl_id', 'frame_id'（最后一帧）}\n",
    "        test_template : pd.DataFrame\n",
    "            官方提交格式的模板 DataFrame，包含所有 (game_id, play_id, nfl_id, frame_id)。\n",
    "        batch_size : int\n",
    "            批量大小，默认 1024。\n",
    "\n",
    "    返回：\n",
    "        submission : pd.DataFrame\n",
    "            最终预测结果，包含列 ['id', 'x', 'y']。\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================\n",
    "    # Step 0. 模型与数据检查\n",
    "    # ============================\n",
    "    if len(models_x) == 0 or len(models_x) != len(models_y):\n",
    "        print(f\"⚠️ 模型数量错误：len(models_x)={len(models_x)}, len(models_y)={len(models_y)}\")\n",
    "        print(\"❌ 没有可用的模型或折数不匹配。\")\n",
    "        return None\n",
    "\n",
    "    if scalers is not None and len(scalers) != len(models_x):\n",
    "        raise ValueError(\"❌ scalers 的数量必须与模型折数相同。\")\n",
    "\n",
    "    print(f\"🧩 共检测到 {len(models_x)} 个折 (fold) 模型，将进行集成预测。\")\n",
    "\n",
    "    # ============================\n",
    "    # Step 1. 数据预处理\n",
    "    # ============================\n",
    "    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n",
    "    N = len(X_test_unscaled)\n",
    "    print(f\"📦 测试序列数量: {N}\")\n",
    "\n",
    "    # 获取每个序列最后一帧的原始坐标 (x, y)\n",
    "    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n",
    "    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n",
    "\n",
    "    per_fold_dx, per_fold_dy = [], []\n",
    "\n",
    "    # ============================\n",
    "    # Step 2. 各折模型预测\n",
    "    # ============================\n",
    "    for i in range(len(models_x)):\n",
    "        print(f\"\\n🚀 第 {i + 1}/{len(models_x)} 折模型预测中...\")\n",
    "        model_x = models_x[i]\n",
    "        model_y = models_y[i]\n",
    "        scaler = scalers[i] if scalers is not None else None\n",
    "\n",
    "        # 如果存在标准化器，则逐序列进行缩放\n",
    "        if scaler is not None:\n",
    "            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n",
    "        else:\n",
    "            scaled = X_test_unscaled\n",
    "\n",
    "        # 拼接为 (N, T, F)\n",
    "        X = np.stack(scaled.astype(np.float32))\n",
    "        device = next(model_x.parameters()).device\n",
    "\n",
    "        # 构建 DataLoader\n",
    "        ds = TensorDataset(torch.from_numpy(X))\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        dx_list, dy_list = [], []\n",
    "        model_x.eval()\n",
    "        model_y.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in dl:\n",
    "                batch = batch.to(device)\n",
    "                dx = model_x(batch)   # (B, H)\n",
    "                dy = model_y(batch)   # (B, H)\n",
    "                dx_list.append(dx.cpu().numpy())\n",
    "                dy_list.append(dy.cpu().numpy())\n",
    "\n",
    "        # 拼接所有 batch 输出\n",
    "        dx_cum = np.vstack(dx_list)  # (N, H)\n",
    "        dy_cum = np.vstack(dy_list)  # (N, H)\n",
    "\n",
    "        per_fold_dx.append(dx_cum)\n",
    "        per_fold_dy.append(dy_cum)\n",
    "        print(f\"✅ 第 {i + 1} 折预测完成 (形状: dx={dx_cum.shape}, dy={dy_cum.shape})\")\n",
    "\n",
    "    # ============================\n",
    "    # Step 3. 模型集成 (取均值)\n",
    "    # ============================\n",
    "    print(\"\\n🔗 开始模型集成 (平均融合各折预测)...\")\n",
    "    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)  # (N, H)\n",
    "    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)  # (N, H)\n",
    "    print(f\"✅ 集成完成 (ens_dx={ens_dx.shape}, ens_dy={ens_dy.shape})\")\n",
    "\n",
    "    # ============================\n",
    "    # Step 4. 构建提交结果\n",
    "    # ============================\n",
    "    test_meta = pd.DataFrame(test_seq_ids)\n",
    "    out_rows = []\n",
    "    H = ens_dx.shape[1]\n",
    "\n",
    "    print(\"\\n🧮 正在生成最终坐标预测结果...\")\n",
    "\n",
    "    for i, seq_info in test_meta.iterrows():\n",
    "        game_id = int(seq_info['game_id'])\n",
    "        play_id = int(seq_info['play_id'])\n",
    "        nfl_id = int(seq_info['nfl_id'])\n",
    "\n",
    "        # 找出该球员的预测帧序列\n",
    "        frame_ids = (\n",
    "            test_template[\n",
    "                (test_template['game_id'] == game_id) &\n",
    "                (test_template['play_id'] == play_id) &\n",
    "                (test_template['nfl_id'] == nfl_id)\n",
    "            ]['frame_id'].sort_values()\n",
    "        )\n",
    "\n",
    "        # 生成每帧预测坐标\n",
    "        for t, frame_id in enumerate(frame_ids):\n",
    "            tt = t if t < H else H - 1\n",
    "            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            out_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n",
    "                'x': px,\n",
    "                'y': py\n",
    "            })\n",
    "\n",
    "    submission = pd.DataFrame(out_rows)\n",
    "    print(f\"\\n✅ 共生成 {len(submission)} 条预测结果。\")\n",
    "\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcea68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_val_predictions(\n",
    "    models, scalers, X_val_unscaled, val_ids,\n",
    "    y_val_dx_fold, y_val_dy_fold, val_data,\n",
    "    exclude_fold=None\n",
    "):\n",
    "    \"\"\"\n",
    "    在验证集上生成集成预测（ensemble），并准备真实值与预测结果以供评分。\n",
    "\n",
    "    功能：\n",
    "        - 对每个验证序列，使用其他折（fold）的模型进行预测；\n",
    "        - 防止数据泄露（排除当前fold对应的模型）；\n",
    "        - 返回真实轨迹与预测轨迹的 DataFrame。\n",
    "\n",
    "    参数：\n",
    "        models : list[nn.Module]\n",
    "            训练好的模型列表（通常为多个折的模型）。\n",
    "        scalers : list[StandardScaler]\n",
    "            与模型对应的特征标准化器。\n",
    "        X_val_unscaled : list[np.ndarray]\n",
    "            未缩放的验证序列，每个元素为 (T, F)。\n",
    "        val_ids : list[dict]\n",
    "            每条序列的元信息，包含 ['game_id', 'play_id', 'nfl_id']。\n",
    "        y_val_dx_fold, y_val_dy_fold : list[np.ndarray]\n",
    "            每条验证样本的真实位移（Δx, Δy）。\n",
    "        val_data : pd.DataFrame\n",
    "            包含验证集中每个样本的 x_last、y_last。\n",
    "        exclude_fold : int | None\n",
    "            需要排除的折号（0-based），用于避免模型泄露。\n",
    "\n",
    "    返回：\n",
    "        tuple(pd.DataFrame, pd.DataFrame)\n",
    "            (ensemble_pred_df, ensemble_true_df)\n",
    "            其中：\n",
    "            - ensemble_pred_df：预测结果 DataFrame（id, x, y）\n",
    "            - ensemble_true_df：真实结果 DataFrame（id, x, y）\n",
    "    \"\"\"\n",
    "\n",
    "    pred_rows = []\n",
    "    true_rows = []\n",
    "\n",
    "    print(\"🔍 开始生成验证集集成预测...\")\n",
    "    if exclude_fold is not None:\n",
    "        print(f\"⚙️  当前排除第 {exclude_fold + 1} 折的模型，以防止数据泄露。\")\n",
    "\n",
    "    for i, seq_info in enumerate(val_ids):\n",
    "        game_id = seq_info[\"game_id\"]\n",
    "        play_id = seq_info[\"play_id\"]\n",
    "        nfl_id = seq_info[\"nfl_id\"]\n",
    "\n",
    "        x_last = val_data.iloc[i][\"x_last\"]\n",
    "        y_last = val_data.iloc[i][\"y_last\"]\n",
    "\n",
    "        # ----------------------------\n",
    "        # 真实轨迹 (Ground Truth)\n",
    "        # ----------------------------\n",
    "        dx_true = y_val_dx_fold[i]\n",
    "        dy_true = y_val_dy_fold[i]\n",
    "        for t in range(len(dx_true)):\n",
    "            frame_rel = t + 1\n",
    "            true_x = x_last + dx_true[t]\n",
    "            true_y = y_last + dy_true[t]\n",
    "            true_rows.append({\n",
    "                \"id\": f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n",
    "                \"x\": true_x,\n",
    "                \"y\": true_y\n",
    "            })\n",
    "\n",
    "        # ----------------------------\n",
    "        # 模型预测（排除当前 fold）\n",
    "        # ----------------------------\n",
    "        per_model_dx = []\n",
    "        per_model_dy = []\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            if exclude_fold is not None and j == exclude_fold:\n",
    "                continue  # 排除当前验证 fold 对应模型\n",
    "\n",
    "            scaler = scalers[j]\n",
    "            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n",
    "            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(scaled_seq).cpu().numpy()[0]  # (H, 2)\n",
    "            per_model_dx.append(output[:, 0])\n",
    "            per_model_dy.append(output[:, 1])\n",
    "\n",
    "        # ----------------------------\n",
    "        # 集成（取平均）\n",
    "        # ----------------------------\n",
    "        if per_model_dx:  # 至少一个模型可用\n",
    "            ens_dx = np.mean(per_model_dx, axis=0)\n",
    "            ens_dy = np.mean(per_model_dy, axis=0)\n",
    "        else:\n",
    "            # 理论上不会发生，除非只有一个 fold\n",
    "            ens_dx = np.zeros(len(dx_true))\n",
    "            ens_dy = np.zeros(len(dy_true))\n",
    "            print(f\"⚠️ 样本 {i} 没有可用模型，使用零预测。\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # 生成预测轨迹\n",
    "        # ----------------------------\n",
    "        for t in range(len(dx_true)):\n",
    "            pred_x = x_last + ens_dx[t]\n",
    "            pred_y = y_last + ens_dy[t]\n",
    "            pred_rows.append({\n",
    "                \"id\": f\"{game_id}_{play_id}_{nfl_id}_{t + 1}\",\n",
    "                \"x\": np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n",
    "                \"y\": np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            })\n",
    "\n",
    "    print(f\"✅ 集成预测完成，共生成 {len(pred_rows)} 条预测样本。\")\n",
    "\n",
    "    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8c7e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据...\n",
      "检测到 18 周的有效训练数据。\n",
      "已加载 4,880,579 条输入记录，562,936 条输出记录。\n"
     ]
    }
   ],
   "source": [
    "set_global_seeds(Config.SEED)\n",
    "# print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n",
    "# models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n",
    "\n",
    "\n",
    "train_input, train_output, test_input, test_template = load_data(debug_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9cb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成时序样本（测试模式）\n",
    "# test_sequences, test_seq_ids, feature_cols = prepare_sequences(\n",
    "#     test_input, test_template=test_template, is_training=False, window_size=Config.WINDOW_SIZE\n",
    "# )\n",
    "\n",
    "# print(f\"✅ 已准备好 {len(test_sequences)} 个测试序列，每个样本包含 {len(feature_cols)} 个特征。\")\n",
    "# print(f\"📏 示例序列形状：{test_sequences[0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a7244",
   "metadata": {},
   "source": [
    "训练数据若帧数不足 不填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de5f46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(input_df, output_df=None, test_template=None,\n",
    "                      is_training=True, window_size=8,\n",
    "                      use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n",
    "    \"\"\"\n",
    "    构建包含所有高级特征的时序样本序列（支持训练与测试）。\n",
    "    input_df：输入特征数据\n",
    "    output_df：训练标签（仅在 is_training=True 时使用）\n",
    "    test_template：测试模板（仅在预测时使用）\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"🚀 开始构建时序样本（包含高级特征）\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"窗口大小（window_size）: {window_size}\")\n",
    "\n",
    "    input_df = input_df.copy()\n",
    "\n",
    "    # Step 1：基础特征构建\n",
    "    print(\"步骤 1/4 ▶ 添加基础特征...\")\n",
    "\n",
    "    # 将球员身高从“英尺-英寸”格式转换为英尺小数\n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "\n",
    "    # 计算速度和加速度的 x、y 分量\n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "\n",
    "    # 角度特征：将朝向角(o)与运动方向(dir)编码为正弦/余弦形式\n",
    "    input_df['o_sin']  = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['o_cos']  = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "\n",
    "    # 角色特征：进攻/防守/传球/接球/防守覆盖\n",
    "    input_df['is_offense']  = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense']  = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer']   = (input_df['player_role'] == 'Passer').astype(int)\n",
    "\n",
    "    # 物理特征：动量与动能\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462  # 磅→千克\n",
    "    input_df['momentum_x']     = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y']     = input_df['velocity_y'] * mass_kg\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "\n",
    "    # 球与球员之间的空间特征\n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball']   = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball']      = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x']   = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y']   = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed']      = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "\n",
    "    # 时间排序（确保帧序一致）\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "\n",
    "    # 添加滞后特征（历史 1~3 帧）\n",
    "    for lag in [1, 2, 3]:\n",
    "        input_df[f'x_lag{lag}']          = input_df.groupby(gcols)['x'].shift(lag)\n",
    "        input_df[f'y_lag{lag}']          = input_df.groupby(gcols)['y'].shift(lag)\n",
    "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "\n",
    "    # EMA（指数滑动平均）平滑速度变化\n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # Step 2：高级特征\n",
    "    print(\"步骤 2/4 ▶ 添加高级特征...\")\n",
    "    input_df = add_advanced_features(input_df)\n",
    "\n",
    "    # Step 3：球员交互特征\n",
    "    print(\"步骤 3/4 ▶ 添加球员交互特征...\")\n",
    "    if use_players_interactions:\n",
    "        print(\"✅ 已启用球员交互特征计算（use_players_interactions=True）\")\n",
    "\n",
    "        agg_rows = []  # 用于保存每一帧的交互统计结果\n",
    "\n",
    "        # 按比赛 (game_id)、回合 (play_id)、帧 (frame_id) 分组\n",
    "        # 每一组包含同一帧中所有球员的空间状态\n",
    "        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "            n = len(grp)\n",
    "            nfl_ids = grp['nfl_id'].to_numpy()\n",
    "\n",
    "            # 如果存在 player_to_predict，则只对需要预测的球员计算交互特征\n",
    "            compute_mask = (\n",
    "                grp['player_to_predict'].to_numpy().astype(bool)\n",
    "                if 'player_to_predict' in grp.columns\n",
    "                else np.ones(n, dtype=bool)\n",
    "            )\n",
    "\n",
    "            # 若该帧球员不足 2 人，则构造空记录（NaN）\n",
    "            if n < 2:\n",
    "                for nid in nfl_ids[compute_mask]:\n",
    "                    agg_rows.append({\n",
    "                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                        'distance_to_player_mean_offense': np.nan,\n",
    "                        'distance_to_player_min_offense': np.nan,\n",
    "                        'distance_to_player_max_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                        'angle_to_player_mean_offense': np.nan,\n",
    "                        'angle_to_player_min_offense': np.nan,\n",
    "                        'angle_to_player_max_offense': np.nan,\n",
    "                        'distance_to_player_mean_defense': np.nan,\n",
    "                        'distance_to_player_min_defense': np.nan,\n",
    "                        'distance_to_player_max_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                        'angle_to_player_mean_defense': np.nan,\n",
    "                        'angle_to_player_min_defense': np.nan,\n",
    "                        'angle_to_player_max_defense': np.nan,\n",
    "                        'nearest_opponent_dist': np.nan,\n",
    "                        'nearest_opponent_angle': np.nan,\n",
    "                        'nearest_opponent_rel_speed': np.nan,\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # 获取球员位置与速度信息\n",
    "            x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "            y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "            is_offense = grp['is_offense'].to_numpy()\n",
    "            is_defense = grp['is_defense'].to_numpy()\n",
    "\n",
    "            # --- 计算两两间的几何关系矩阵 ---\n",
    "            dx = x[None, :] - x[:, None]      # X方向差值矩阵\n",
    "            dy = y[None, :] - y[:, None]      # Y方向差值矩阵\n",
    "            dist = np.sqrt(dx ** 2 + dy ** 2) # 欧氏距离矩阵 (n×n)\n",
    "            angle_mat = np.arctan2(-dy, -dx)  # 从球员 i 指向 j 的角度\n",
    "\n",
    "            # --- 相对速度矩阵 ---\n",
    "            dvx = vx[:, None] - vx[None, :]\n",
    "            dvy = vy[:, None] - vy[None, :]\n",
    "            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "\n",
    "            # --- 各类掩码 ---\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            np.fill_diagonal(offense_mask, False)  # 自身不参与计算\n",
    "\n",
    "            defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "            np.fill_diagonal(defense_mask, False)\n",
    "\n",
    "            opp_mask = (is_offense[:, None] != is_offense[None, :])  # 对手阵营\n",
    "            np.fill_diagonal(opp_mask, False)\n",
    "\n",
    "            # --- 将自身的值置为 NaN，避免干扰统计 ---\n",
    "            dist_diag_nan  = dist.copy();      np.fill_diagonal(dist_diag_nan,  np.nan)\n",
    "            rel_diag_nan   = rel_speed.copy(); np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "            angle_diag_nan = angle_mat.copy(); np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "\n",
    "            # --- 定义统计函数：计算均值、最小值、最大值 ---\n",
    "            def masked_stats(mat, mask):\n",
    "                masked = np.where(mask, mat, np.nan)\n",
    "                cnt  = mask.sum(axis=1)\n",
    "                mean = np.nanmean(masked, axis=1)\n",
    "                amin = np.nanmin(masked, axis=1)\n",
    "                amax = np.nanmax(masked, axis=1)\n",
    "                zero = cnt == 0  # 若无有效数据则置 NaN\n",
    "                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "                return mean, amin, amax\n",
    "\n",
    "            # --- 计算进攻方之间的距离、相对速度、角度统计 ---\n",
    "            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "\n",
    "            # --- 计算防守方之间的统计 ---\n",
    "            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "\n",
    "            # --- 计算最近对手距离/角度/相对速度 ---\n",
    "            masked_dist_opp = np.where(opp_mask, dist_diag_nan, np.nan)\n",
    "            nearest_dist = np.nanmin(masked_dist_opp, axis=1)\n",
    "            nearest_idx  = np.nanargmin(masked_dist_opp, axis=1)\n",
    "            all_nan = ~np.isfinite(nearest_dist)\n",
    "            nearest_idx_safe = nearest_idx.copy()\n",
    "            nearest_idx_safe[all_nan] = 0\n",
    "\n",
    "            nearest_angle = np.take_along_axis(angle_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_rel   = np.take_along_axis(rel_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_angle[all_nan] = np.nan\n",
    "            nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "            # --- 汇总每位球员的交互特征 ---\n",
    "            for idx, nid in enumerate(nfl_ids):\n",
    "                if not compute_mask[idx]:\n",
    "                    continue\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                    'distance_to_player_min_offense':  d_min_o[idx],\n",
    "                    'distance_to_player_max_offense':  d_max_o[idx],\n",
    "                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                    'relative_velocity_magnitude_min_offense':  v_min_o[idx],\n",
    "                    'relative_velocity_magnitude_max_offense':  v_max_o[idx],\n",
    "                    'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                    'angle_to_player_min_offense':  a_min_o[idx],\n",
    "                    'angle_to_player_max_offense':  a_max_o[idx],\n",
    "\n",
    "                    'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                    'distance_to_player_min_defense':  d_min_d[idx],\n",
    "                    'distance_to_player_max_defense':  d_max_d[idx],\n",
    "                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                    'relative_velocity_magnitude_min_defense':  v_min_d[idx],\n",
    "                    'relative_velocity_magnitude_max_defense':  v_max_d[idx],\n",
    "                    'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                    'angle_to_player_min_defense':  a_min_d[idx],\n",
    "                    'angle_to_player_max_defense':  a_max_d[idx],\n",
    "\n",
    "                    'nearest_opponent_dist':      nearest_dist[idx],\n",
    "                    'nearest_opponent_angle':     nearest_angle[idx],\n",
    "                    'nearest_opponent_rel_speed': nearest_rel[idx],\n",
    "                })\n",
    "\n",
    "        # 合并交互特征回主表\n",
    "        interaction_agg = pd.DataFrame(agg_rows)\n",
    "        input_df = input_df.merge(\n",
    "            interaction_agg,\n",
    "            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        print(\"✅ 球员交互特征添加完成。\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ 跳过球员交互特征计算（use_players_interactions=False）。\")\n",
    "\n",
    "\n",
    "    # Step 4：构建输入序列\n",
    "    print(\"步骤 4/4 ▶ 构建输入序列样本...\")\n",
    "\n",
    "\n",
    "    feature_cols = [\n",
    "        # —— 基础核心特征（Core, 6）——\n",
    "        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n",
    "\n",
    "        # —— 角度编码特征（Angles encoded, 4）——\n",
    "        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n",
    "\n",
    "        # —— 球员静态特征（Player, 2）——\n",
    "        'player_height_feet', 'player_weight',\n",
    "\n",
    "        # —— 动态运动特征（Motion, 7）——\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "\n",
    "        # —— 角色身份特征（Roles, 5）——\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "\n",
    "        # —— 球与球员空间特征（Ball relation, 5）——\n",
    "        'distance_to_ball', 'angle_to_ball',\n",
    "        'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "\n",
    "        # —— 原始时序滞后特征（Original temporal lags, 15）——\n",
    "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "\n",
    "        # —— 距离变化速率特征（Distance rate, 3）——\n",
    "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "\n",
    "        # —— 目标方向对齐特征（Target alignment, 3）——\n",
    "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "\n",
    "        # —— 多窗口滚动特征（Multi-window rolling, 24）——\n",
    "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "\n",
    "        # —— 扩展时序滞后特征（Extended lags, 8）——\n",
    "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "\n",
    "        # —— 速度变化特征（Velocity change, 4）——\n",
    "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "\n",
    "        # —— 场地位置特征（Field position, 2）——\n",
    "        'dist_from_sideline', 'dist_from_endzone',\n",
    "\n",
    "        # —— 角色相关特征（Role-specific, 3）——\n",
    "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "\n",
    "        # —— 时间进程特征（Time, 2）——\n",
    "        'frames_elapsed', 'normalized_time',\n",
    "\n",
    "        # —— 球员交互特征（Player interactions, 21）——\n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n",
    "        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n",
    "    ]\n",
    "\n",
    "    # 保留当前数据集中确实存在的特征列（避免 KeyError）\n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "\n",
    "    # 输出特征数量信息\n",
    "    print(f\"✅ 使用的特征列数量: {len(feature_cols)} 个\")\n",
    "\n",
    "\n",
    "    # CREATE SEQUENCES\n",
    "\n",
    "    # 设置索引并按球员分组\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "\n",
    "    # 选择目标数据源（训练或测试）\n",
    "    target_rows = output_df if is_training else test_template\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "\n",
    "    # 存储容器\n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "\n",
    "    # 遍历每个球员的时间序列\n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"⏳ 正在创建序列\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "\n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # 提取时间窗口（最后 window_size 帧）\n",
    "        input_window = group_df.tail(window_size)\n",
    "\n",
    "        # 若帧数不足则进行填充\n",
    "        if len(input_window) < window_size:\n",
    "            if is_training:\n",
    "                print(f\"Skipping sequence with insufficient history for {key}\")\n",
    "                continue\n",
    "            print(f\"⚠️ 序列不足 {window_size} 帧，自动填充：{key}\")\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df  = pd.concat([input_window.iloc[0:1]] * pad_len, ignore_index=True)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "\n",
    "        # 缺失值前向/后向填充\n",
    "        input_window = input_window.ffill().bfill().fillna(0.0)\n",
    "\n",
    "        seq = input_window[feature_cols].values\n",
    "        if np.isnan(seq).any():\n",
    "            print(f\"⚠️ 在 {key} 的序列中发现 NaN，已用 0.0 替换。\")\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "\n",
    "        sequences.append(seq)\n",
    "\n",
    "        # 若为训练模式，则计算预测目标（Δx, Δy）\n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id'] == row['game_id']) &\n",
    "                (output_df['play_id'] == row['play_id']) &\n",
    "                (output_df['nfl_id']  == row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "\n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "\n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "\n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "\n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "\n",
    "    print(f\"\\n✅ 共生成 {len(sequences)} 个序列，每个序列包含 {len(feature_cols)} 个特征。\")\n",
    "\n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n",
    "    return sequences, sequence_ids, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bb9c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 开始构建时序样本（包含高级特征）\n",
      "================================================================================\n",
      "窗口大小（window_size）: 12\n",
      "步骤 1/4 ▶ 添加基础特征...\n",
      "步骤 2/4 ▶ 添加高级特征...\n",
      "正在添加高级特征...\n",
      "特征增强后总列数: 112\n",
      "步骤 3/4 ▶ 添加球员交互特征...\n",
      "✅ 已启用球员交互特征计算（use_players_interactions=True）\n",
      "✅ 球员交互特征添加完成。\n",
      "步骤 4/4 ▶ 构建输入序列样本...\n",
      "✅ 使用的特征列数量: 114 个\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9cbc86ed24bbc85859be3e78a8707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "⏳ 正在创建序列:   0%|          | 0/46045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sequence with insufficient history for (2023091004, 1594, 52453)\n",
      "Skipping sequence with insufficient history for (2023091004, 1594, 52430)\n",
      "Skipping sequence with insufficient history for (2023091004, 4122, 41233)\n",
      "Skipping sequence with insufficient history for (2023091008, 1292, 48516)\n",
      "Skipping sequence with insufficient history for (2023091008, 1292, 55921)\n",
      "Skipping sequence with insufficient history for (2023091008, 1292, 54597)\n",
      "Skipping sequence with insufficient history for (2023091400, 318, 52430)\n",
      "Skipping sequence with insufficient history for (2023091700, 1728, 46087)\n",
      "Skipping sequence with insufficient history for (2023091700, 1728, 54473)\n",
      "Skipping sequence with insufficient history for (2023091709, 493, 53531)\n",
      "Skipping sequence with insufficient history for (2023091709, 493, 53601)\n",
      "Skipping sequence with insufficient history for (2023091709, 493, 56042)\n",
      "Skipping sequence with insufficient history for (2023091712, 81, 43327)\n",
      "Skipping sequence with insufficient history for (2023091712, 81, 42357)\n",
      "Skipping sequence with insufficient history for (2023091712, 2480, 43299)\n",
      "Skipping sequence with insufficient history for (2023091712, 2480, 42357)\n",
      "Skipping sequence with insufficient history for (2023092406, 3048, 44819)\n",
      "Skipping sequence with insufficient history for (2023092410, 1352, 46150)\n",
      "Skipping sequence with insufficient history for (2023092410, 2619, 53565)\n",
      "Skipping sequence with insufficient history for (2023092410, 2619, 52425)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 53476)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 54679)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 55919)\n",
      "Skipping sequence with insufficient history for (2023100110, 2747, 41282)\n",
      "Skipping sequence with insufficient history for (2023100807, 4264, 43986)\n",
      "Skipping sequence with insufficient history for (2023100807, 4264, 55005)\n",
      "Skipping sequence with insufficient history for (2023100810, 2151, 54583)\n",
      "Skipping sequence with insufficient history for (2023100810, 2151, 54519)\n",
      "Skipping sequence with insufficient history for (2023100900, 2328, 42381)\n",
      "Skipping sequence with insufficient history for (2023100900, 2328, 47807)\n",
      "Skipping sequence with insufficient history for (2023100900, 2962, 45038)\n",
      "Skipping sequence with insufficient history for (2023100900, 2962, 54597)\n",
      "Skipping sequence with insufficient history for (2023101509, 1300, 42361)\n",
      "Skipping sequence with insufficient history for (2023101509, 1300, 53549)\n",
      "Skipping sequence with insufficient history for (2023101509, 3424, 46902)\n",
      "Skipping sequence with insufficient history for (2023101509, 3424, 45185)\n",
      "Skipping sequence with insufficient history for (2023101511, 3940, 43351)\n",
      "Skipping sequence with insufficient history for (2023101511, 3940, 54475)\n",
      "Skipping sequence with insufficient history for (2023102205, 3383, 47877)\n",
      "Skipping sequence with insufficient history for (2023102205, 3383, 54473)\n",
      "Skipping sequence with insufficient history for (2023102300, 353, 55887)\n",
      "Skipping sequence with insufficient history for (2023102908, 236, 47859)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 39984)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 43351)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 43353)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 54548)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 54808)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 55931)\n",
      "Skipping sequence with insufficient history for (2023102908, 3160, 47859)\n",
      "Skipping sequence with insufficient history for (2023102909, 3040, 54533)\n",
      "Skipping sequence with insufficient history for (2023102909, 3040, 47847)\n",
      "Skipping sequence with insufficient history for (2023102913, 950, 53476)\n",
      "Skipping sequence with insufficient history for (2023102913, 950, 46093)\n",
      "Skipping sequence with insufficient history for (2023110509, 1738, 44830)\n",
      "Skipping sequence with insufficient history for (2023110509, 1738, 41282)\n",
      "Skipping sequence with insufficient history for (2023110600, 412, 54475)\n",
      "Skipping sequence with insufficient history for (2023110600, 2558, 54475)\n",
      "Skipping sequence with insufficient history for (2023111204, 341, 47816)\n",
      "Skipping sequence with insufficient history for (2023111204, 341, 56527)\n",
      "Skipping sequence with insufficient history for (2023111204, 341, 43336)\n",
      "Skipping sequence with insufficient history for (2023111210, 1200, 43373)\n",
      "Skipping sequence with insufficient history for (2023111210, 1200, 55884)\n",
      "Skipping sequence with insufficient history for (2023111211, 679, 52547)\n",
      "Skipping sequence with insufficient history for (2023111211, 679, 47974)\n",
      "Skipping sequence with insufficient history for (2023111211, 679, 54475)\n",
      "Skipping sequence with insufficient history for (2023111211, 2937, 55969)\n",
      "Skipping sequence with insufficient history for (2023111211, 2937, 54475)\n",
      "Skipping sequence with insufficient history for (2023111211, 3659, 55969)\n",
      "Skipping sequence with insufficient history for (2023111211, 3659, 54475)\n",
      "Skipping sequence with insufficient history for (2023111903, 1604, 45571)\n",
      "Skipping sequence with insufficient history for (2023111903, 1604, 56220)\n",
      "Skipping sequence with insufficient history for (2023111906, 587, 43327)\n",
      "Skipping sequence with insufficient history for (2023111906, 587, 41282)\n",
      "Skipping sequence with insufficient history for (2023112604, 1355, 55888)\n",
      "Skipping sequence with insufficient history for (2023112604, 1355, 56052)\n",
      "Skipping sequence with insufficient history for (2023112604, 1588, 55888)\n",
      "Skipping sequence with insufficient history for (2023112604, 1588, 42357)\n",
      "Skipping sequence with insufficient history for (2023120400, 351, 44872)\n",
      "Skipping sequence with insufficient history for (2023120400, 351, 46095)\n",
      "Skipping sequence with insufficient history for (2023121007, 2198, 54583)\n",
      "Skipping sequence with insufficient history for (2023121007, 2198, 41282)\n",
      "Skipping sequence with insufficient history for (2023121705, 174, 56042)\n",
      "Skipping sequence with insufficient history for (2023123102, 3589, 56117)\n",
      "Skipping sequence with insufficient history for (2023123102, 3589, 52495)\n",
      "Skipping sequence with insufficient history for (2023123102, 3589, 42489)\n",
      "Skipping sequence with insufficient history for (2024010704, 1678, 43700)\n",
      "Skipping sequence with insufficient history for (2024010704, 1678, 54475)\n",
      "Skipping sequence with insufficient history for (2024010707, 3669, 55880)\n",
      "Skipping sequence with insufficient history for (2024010707, 3669, 56109)\n",
      "\n",
      "✅ 共生成 45956 个序列，每个序列包含 114 个特征。\n",
      "✅ 已准备好 45956 个时序样本，每个样本包含 114 个特征。\n",
      "📏 每个样本的窗口长度为 12 帧。\n",
      "📊 示例样本形状：(12, 114)\n",
      "🎯 训练目标示例：dx=(21,), dy=(21,)\n",
      "🆔 样本索引数量：45956\n"
     ]
    }
   ],
   "source": [
    "# 生成时序样本（训练模式）\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols = prepare_sequences(\n",
    "    train_input,\n",
    "    output_df=train_output,\n",
    "    test_template=test_template,\n",
    "    is_training=True,\n",
    "    window_size=Config.WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# 输出结果信息\n",
    "print(f\"✅ 已准备好 {len(sequences)} 个时序样本，每个样本包含 {len(feature_cols)} 个特征。\")\n",
    "print(f\"📏 每个样本的窗口长度为 {Config.WINDOW_SIZE} 帧。\")\n",
    "print(f\"📊 示例样本形状：{sequences[0].shape}\")\n",
    "print(f\"🎯 训练目标示例：dx={targets_dx[0].shape}, dy={targets_dy[0].shape}\")\n",
    "print(f\"🆔 样本索引数量：{len(sequence_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49a609a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已使用 pickle 保存对象到 D:\\数据\\Kaggle\\2026 年 NFL 大数据碗 - 预测\\DATA_DIR000\\train_data_cache_unpad.pkl\n"
     ]
    }
   ],
   "source": [
    "# === 保存 pkl 文件 ===\n",
    "save_path = Config.DATA_DIR / \"train_data_cache_unpad.pkl\"\n",
    "\n",
    "# 保存（保留 numpy 对象类型）\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"sequences\": sequences,\n",
    "        \"targets_dx\": targets_dx,\n",
    "        \"targets_dy\": targets_dy,\n",
    "        \"targets_frame_ids\": targets_frame_ids,\n",
    "        \"sequence_ids\": sequence_ids,\n",
    "        \"feature_cols\": feature_cols\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ 已使用 pickle 保存对象到 {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c943f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据加载成功到 *_2 变量！\n"
     ]
    }
   ],
   "source": [
    "# === 加载 pkl 文件 ===\n",
    "data_path = Config.DATA_DIR / \"train_data_cache_unpad.pkl\"\n",
    "\n",
    "with open(save_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "sequences_2 = data[\"sequences\"]\n",
    "targets_dx_2 = data[\"targets_dx\"]\n",
    "targets_dy_2 = data[\"targets_dy\"]\n",
    "targets_frame_ids_2 = data[\"targets_frame_ids\"]\n",
    "sequence_ids_2 = data[\"sequence_ids\"]\n",
    "feature_cols_2 = data[\"feature_cols\"]\n",
    "\n",
    "print(\"✅ 数据加载成功到 *_2 变量！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db5ab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 哈希验证\n",
    "def get_obj_type(obj):\n",
    "    \"\"\"识别对象类型（支持嵌套结构）\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return f\"numpy.ndarray shape={obj.shape} dtype={obj.dtype}\"\n",
    "    elif isinstance(obj, list):\n",
    "        if len(obj) == 0:\n",
    "            return \"list (empty)\"\n",
    "        inner_type = get_obj_type(obj[0])\n",
    "        return f\"list[{inner_type}] len={len(obj)}\"\n",
    "    elif isinstance(obj, dict):\n",
    "        return f\"dict (keys={len(obj.keys())})\"\n",
    "    elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "        return type(obj).__name__\n",
    "    else:\n",
    "        return f\"object ({type(obj).__name__})\"\n",
    "\n",
    "\n",
    "def hash_object(obj) -> str:\n",
    "    \"\"\"生成对象的稳定哈希签名（支持 list / dict / ndarray / 标量）\"\"\"\n",
    "    m = hashlib.sha256()\n",
    "\n",
    "    def _update(o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            m.update(o.tobytes())\n",
    "            m.update(str(o.shape).encode())\n",
    "            m.update(str(o.dtype).encode())\n",
    "        elif isinstance(o, (list, tuple)):\n",
    "            m.update(f\"len={len(o)}\".encode())\n",
    "            for item in o:\n",
    "                _update(item)\n",
    "        elif isinstance(o, dict):\n",
    "            for k in sorted(o.keys()):\n",
    "                m.update(str(k).encode())\n",
    "                _update(o[k])\n",
    "        elif isinstance(o, (str, int, float, bool, type(None))):\n",
    "            m.update(str(o).encode())\n",
    "        else:\n",
    "            m.update(repr(o).encode())\n",
    "\n",
    "    _update(obj)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def verify_objects(obj1, obj2, name=\"变量\"):\n",
    "    \"\"\"\n",
    "    比较两个对象内容是否一致，并输出简洁单行信息。\n",
    "    支持：list / dict / numpy.ndarray / 标量\n",
    "    \"\"\"\n",
    "    type1 = get_obj_type(obj1)\n",
    "    type2 = get_obj_type(obj2)\n",
    "    hash1 = hash_object(obj1)\n",
    "    hash2 = hash_object(obj2)\n",
    "    same = hash1 == hash2\n",
    "\n",
    "    print(f\"[{name}]\")\n",
    "    print(f\"{type1}\")\n",
    "    print(f\"{type2}\")\n",
    "    print(f\"{hash1[:24]}\")\n",
    "    print(f\"{hash2[:24]}\")\n",
    "    print(f\"{'✅一致' if same else '❌不同'}\")\n",
    "    return same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f21e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sequences]\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=45956\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=45956\n",
      "f6eb874159c60cf6c9905132\n",
      "f6eb874159c60cf6c9905132\n",
      "✅一致\n",
      "[targets_dx]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "2fcee115f7b268256baf2a46\n",
      "2fcee115f7b268256baf2a46\n",
      "✅一致\n",
      "[targets_dy]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=45956\n",
      "1aaf324c0fe11c32418be172\n",
      "1aaf324c0fe11c32418be172\n",
      "✅一致\n",
      "[targets_frame_ids]\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=45956\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=45956\n",
      "22c0bb381dffffe48d1cb725\n",
      "22c0bb381dffffe48d1cb725\n",
      "✅一致\n",
      "[sequence_ids]\n",
      "list[dict (keys=4)] len=45956\n",
      "list[dict (keys=4)] len=45956\n",
      "e39ef11edaf968ab84a15ffe\n",
      "e39ef11edaf968ab84a15ffe\n",
      "✅一致\n",
      "[feature_cols]\n",
      "list[str] len=114\n",
      "list[str] len=114\n",
      "44cf06a02b1e47da769fea6a\n",
      "44cf06a02b1e47da769fea6a\n",
      "✅一致\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_objects(sequences, sequences_2, \"sequences\")\n",
    "verify_objects(targets_dx, targets_dx_2, \"targets_dx\")\n",
    "verify_objects(targets_dy, targets_dy_2, \"targets_dy\")\n",
    "verify_objects(targets_frame_ids, targets_frame_ids_2, \"targets_frame_ids\")\n",
    "verify_objects(sequence_ids, sequence_ids_2, \"sequence_ids\")\n",
    "verify_objects(feature_cols, feature_cols_2, \"feature_cols\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdfe27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68b0ed12",
   "metadata": {},
   "source": [
    "训练数据若帧数不足 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2fa5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(input_df, output_df=None, test_template=None,\n",
    "                      is_training=True, window_size=8,\n",
    "                      use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n",
    "    \"\"\"\n",
    "    构建包含所有高级特征的时序样本序列（支持训练与测试）。\n",
    "    input_df：输入特征数据\n",
    "    output_df：训练标签（仅在 is_training=True 时使用）\n",
    "    test_template：测试模板（仅在预测时使用）\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"🚀 开始构建时序样本（包含高级特征）\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"窗口大小（window_size）: {window_size}\")\n",
    "\n",
    "    input_df = input_df.copy()\n",
    "\n",
    "    # Step 1：基础特征构建\n",
    "    print(\"步骤 1/4 ▶ 添加基础特征...\")\n",
    "\n",
    "    # 将球员身高从“英尺-英寸”格式转换为英尺小数\n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "\n",
    "    # 计算速度和加速度的 x、y 分量\n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y']     = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "\n",
    "    # 角度特征：将朝向角(o)与运动方向(dir)编码为正弦/余弦形式\n",
    "    input_df['o_sin']  = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['o_cos']  = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "\n",
    "    # 角色特征：进攻/防守/传球/接球/防守覆盖\n",
    "    input_df['is_offense']  = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense']  = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer']   = (input_df['player_role'] == 'Passer').astype(int)\n",
    "\n",
    "    # 物理特征：动量与动能\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462  # 磅→千克\n",
    "    input_df['momentum_x']     = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y']     = input_df['velocity_y'] * mass_kg\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "\n",
    "    # 球与球员之间的空间特征\n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball']   = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball']      = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x']   = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y']   = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed']      = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "\n",
    "    # 时间排序（确保帧序一致）\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "\n",
    "    # 添加滞后特征（历史 1~3 帧）\n",
    "    for lag in [1, 2, 3]:\n",
    "        input_df[f'x_lag{lag}']          = input_df.groupby(gcols)['x'].shift(lag)\n",
    "        input_df[f'y_lag{lag}']          = input_df.groupby(gcols)['y'].shift(lag)\n",
    "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "\n",
    "    # EMA（指数滑动平均）平滑速度变化\n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # Step 2：高级特征\n",
    "    print(\"步骤 2/4 ▶ 添加高级特征...\")\n",
    "    input_df = add_advanced_features(input_df)\n",
    "\n",
    "    # Step 3：球员交互特征\n",
    "    print(\"步骤 3/4 ▶ 添加球员交互特征...\")\n",
    "    if use_players_interactions:\n",
    "        print(\"✅ 已启用球员交互特征计算（use_players_interactions=True）\")\n",
    "\n",
    "        agg_rows = []  # 用于保存每一帧的交互统计结果\n",
    "\n",
    "        # 按比赛 (game_id)、回合 (play_id)、帧 (frame_id) 分组\n",
    "        # 每一组包含同一帧中所有球员的空间状态\n",
    "        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "            n = len(grp)\n",
    "            nfl_ids = grp['nfl_id'].to_numpy()\n",
    "\n",
    "            # 如果存在 player_to_predict，则只对需要预测的球员计算交互特征\n",
    "            compute_mask = (\n",
    "                grp['player_to_predict'].to_numpy().astype(bool)\n",
    "                if 'player_to_predict' in grp.columns\n",
    "                else np.ones(n, dtype=bool)\n",
    "            )\n",
    "\n",
    "            # 若该帧球员不足 2 人，则构造空记录（NaN）\n",
    "            if n < 2:\n",
    "                for nid in nfl_ids[compute_mask]:\n",
    "                    agg_rows.append({\n",
    "                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                        'distance_to_player_mean_offense': np.nan,\n",
    "                        'distance_to_player_min_offense': np.nan,\n",
    "                        'distance_to_player_max_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                        'angle_to_player_mean_offense': np.nan,\n",
    "                        'angle_to_player_min_offense': np.nan,\n",
    "                        'angle_to_player_max_offense': np.nan,\n",
    "                        'distance_to_player_mean_defense': np.nan,\n",
    "                        'distance_to_player_min_defense': np.nan,\n",
    "                        'distance_to_player_max_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                        'angle_to_player_mean_defense': np.nan,\n",
    "                        'angle_to_player_min_defense': np.nan,\n",
    "                        'angle_to_player_max_defense': np.nan,\n",
    "                        'nearest_opponent_dist': np.nan,\n",
    "                        'nearest_opponent_angle': np.nan,\n",
    "                        'nearest_opponent_rel_speed': np.nan,\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # 获取球员位置与速度信息\n",
    "            x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "            y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "            is_offense = grp['is_offense'].to_numpy()\n",
    "            is_defense = grp['is_defense'].to_numpy()\n",
    "\n",
    "            # --- 计算两两间的几何关系矩阵 ---\n",
    "            dx = x[None, :] - x[:, None]      # X方向差值矩阵\n",
    "            dy = y[None, :] - y[:, None]      # Y方向差值矩阵\n",
    "            dist = np.sqrt(dx ** 2 + dy ** 2) # 欧氏距离矩阵 (n×n)\n",
    "            angle_mat = np.arctan2(-dy, -dx)  # 从球员 i 指向 j 的角度\n",
    "\n",
    "            # --- 相对速度矩阵 ---\n",
    "            dvx = vx[:, None] - vx[None, :]\n",
    "            dvy = vy[:, None] - vy[None, :]\n",
    "            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "\n",
    "            # --- 各类掩码 ---\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            np.fill_diagonal(offense_mask, False)  # 自身不参与计算\n",
    "\n",
    "            defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "            np.fill_diagonal(defense_mask, False)\n",
    "\n",
    "            opp_mask = (is_offense[:, None] != is_offense[None, :])  # 对手阵营\n",
    "            np.fill_diagonal(opp_mask, False)\n",
    "\n",
    "            # --- 将自身的值置为 NaN，避免干扰统计 ---\n",
    "            dist_diag_nan  = dist.copy();      np.fill_diagonal(dist_diag_nan,  np.nan)\n",
    "            rel_diag_nan   = rel_speed.copy(); np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "            angle_diag_nan = angle_mat.copy(); np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "\n",
    "            # --- 定义统计函数：计算均值、最小值、最大值 ---\n",
    "            def masked_stats(mat, mask):\n",
    "                masked = np.where(mask, mat, np.nan)\n",
    "                cnt  = mask.sum(axis=1)\n",
    "                mean = np.nanmean(masked, axis=1)\n",
    "                amin = np.nanmin(masked, axis=1)\n",
    "                amax = np.nanmax(masked, axis=1)\n",
    "                zero = cnt == 0  # 若无有效数据则置 NaN\n",
    "                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "                return mean, amin, amax\n",
    "\n",
    "            # --- 计算进攻方之间的距离、相对速度、角度统计 ---\n",
    "            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "\n",
    "            # --- 计算防守方之间的统计 ---\n",
    "            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "\n",
    "            # --- 计算最近对手距离/角度/相对速度 ---\n",
    "            masked_dist_opp = np.where(opp_mask, dist_diag_nan, np.nan)\n",
    "            nearest_dist = np.nanmin(masked_dist_opp, axis=1)\n",
    "            nearest_idx  = np.nanargmin(masked_dist_opp, axis=1)\n",
    "            all_nan = ~np.isfinite(nearest_dist)\n",
    "            nearest_idx_safe = nearest_idx.copy()\n",
    "            nearest_idx_safe[all_nan] = 0\n",
    "\n",
    "            nearest_angle = np.take_along_axis(angle_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_rel   = np.take_along_axis(rel_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_angle[all_nan] = np.nan\n",
    "            nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "            # --- 汇总每位球员的交互特征 ---\n",
    "            for idx, nid in enumerate(nfl_ids):\n",
    "                if not compute_mask[idx]:\n",
    "                    continue\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                    'distance_to_player_min_offense':  d_min_o[idx],\n",
    "                    'distance_to_player_max_offense':  d_max_o[idx],\n",
    "                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                    'relative_velocity_magnitude_min_offense':  v_min_o[idx],\n",
    "                    'relative_velocity_magnitude_max_offense':  v_max_o[idx],\n",
    "                    'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                    'angle_to_player_min_offense':  a_min_o[idx],\n",
    "                    'angle_to_player_max_offense':  a_max_o[idx],\n",
    "\n",
    "                    'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                    'distance_to_player_min_defense':  d_min_d[idx],\n",
    "                    'distance_to_player_max_defense':  d_max_d[idx],\n",
    "                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                    'relative_velocity_magnitude_min_defense':  v_min_d[idx],\n",
    "                    'relative_velocity_magnitude_max_defense':  v_max_d[idx],\n",
    "                    'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                    'angle_to_player_min_defense':  a_min_d[idx],\n",
    "                    'angle_to_player_max_defense':  a_max_d[idx],\n",
    "\n",
    "                    'nearest_opponent_dist':      nearest_dist[idx],\n",
    "                    'nearest_opponent_angle':     nearest_angle[idx],\n",
    "                    'nearest_opponent_rel_speed': nearest_rel[idx],\n",
    "                })\n",
    "\n",
    "        # 合并交互特征回主表\n",
    "        interaction_agg = pd.DataFrame(agg_rows)\n",
    "        input_df = input_df.merge(\n",
    "            interaction_agg,\n",
    "            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        print(\"✅ 球员交互特征添加完成。\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ 跳过球员交互特征计算（use_players_interactions=False）。\")\n",
    "\n",
    "\n",
    "    # Step 4：构建输入序列\n",
    "    print(\"步骤 4/4 ▶ 构建输入序列样本...\")\n",
    "\n",
    "\n",
    "    feature_cols = [\n",
    "        # —— 基础核心特征（Core, 6）——\n",
    "        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n",
    "\n",
    "        # —— 角度编码特征（Angles encoded, 4）——\n",
    "        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n",
    "\n",
    "        # —— 球员静态特征（Player, 2）——\n",
    "        'player_height_feet', 'player_weight',\n",
    "\n",
    "        # —— 动态运动特征（Motion, 7）——\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "\n",
    "        # —— 角色身份特征（Roles, 5）——\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "\n",
    "        # —— 球与球员空间特征（Ball relation, 5）——\n",
    "        'distance_to_ball', 'angle_to_ball',\n",
    "        'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "\n",
    "        # —— 原始时序滞后特征（Original temporal lags, 15）——\n",
    "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "\n",
    "        # —— 距离变化速率特征（Distance rate, 3）——\n",
    "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "\n",
    "        # —— 目标方向对齐特征（Target alignment, 3）——\n",
    "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "\n",
    "        # —— 多窗口滚动特征（Multi-window rolling, 24）——\n",
    "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "\n",
    "        # —— 扩展时序滞后特征（Extended lags, 8）——\n",
    "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "\n",
    "        # —— 速度变化特征（Velocity change, 4）——\n",
    "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "\n",
    "        # —— 场地位置特征（Field position, 2）——\n",
    "        'dist_from_sideline', 'dist_from_endzone',\n",
    "\n",
    "        # —— 角色相关特征（Role-specific, 3）——\n",
    "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "\n",
    "        # —— 时间进程特征（Time, 2）——\n",
    "        'frames_elapsed', 'normalized_time',\n",
    "\n",
    "        # —— 球员交互特征（Player interactions, 21）——\n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n",
    "        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n",
    "    ]\n",
    "\n",
    "    # 保留当前数据集中确实存在的特征列（避免 KeyError）\n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "\n",
    "    # 输出特征数量信息\n",
    "    print(f\"✅ 使用的特征列数量: {len(feature_cols)} 个\")\n",
    "\n",
    "\n",
    "    # CREATE SEQUENCES\n",
    "\n",
    "    # 设置索引并按球员分组\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "\n",
    "    # 选择目标数据源（训练或测试）\n",
    "    target_rows = output_df if is_training else test_template\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "\n",
    "    # 存储容器\n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "\n",
    "    # 遍历每个球员的时间序列\n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"⏳ 正在创建序列\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "\n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # 提取时间窗口（最后 window_size 帧）\n",
    "        input_window = group_df.tail(window_size)\n",
    "\n",
    "        # 若帧数不足则进行填充\n",
    "        if len(input_window) < window_size:\n",
    "            # if is_training:\n",
    "            #     print(f\"Skipping sequence with insufficient history for {key}\")\n",
    "            #     continue\n",
    "            print(f\"⚠️ 序列不足 {window_size} 帧，自动填充：{key}\")\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df  = pd.concat([input_window.iloc[0:1]] * pad_len, ignore_index=True)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "\n",
    "        # 缺失值前向/后向填充\n",
    "        input_window = input_window.ffill().bfill().fillna(0.0)\n",
    "\n",
    "        seq = input_window[feature_cols].values\n",
    "        if np.isnan(seq).any():\n",
    "            print(f\"⚠️ 在 {key} 的序列中发现 NaN，已用 0.0 替换。\")\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "\n",
    "        sequences.append(seq)\n",
    "\n",
    "        # 若为训练模式，则计算预测目标（Δx, Δy）\n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id'] == row['game_id']) &\n",
    "                (output_df['play_id'] == row['play_id']) &\n",
    "                (output_df['nfl_id']  == row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "\n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "\n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "\n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "\n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "\n",
    "    print(f\"\\n✅ 共生成 {len(sequences)} 个序列，每个序列包含 {len(feature_cols)} 个特征。\")\n",
    "\n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n",
    "    return sequences, sequence_ids, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc5970f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 开始构建时序样本（包含高级特征）\n",
      "================================================================================\n",
      "窗口大小（window_size）: 12\n",
      "步骤 1/4 ▶ 添加基础特征...\n",
      "步骤 2/4 ▶ 添加高级特征...\n",
      "正在添加高级特征...\n",
      "特征增强后总列数: 112\n",
      "步骤 3/4 ▶ 添加球员交互特征...\n",
      "✅ 已启用球员交互特征计算（use_players_interactions=True）\n",
      "✅ 球员交互特征添加完成。\n",
      "步骤 4/4 ▶ 构建输入序列样本...\n",
      "✅ 使用的特征列数量: 114 个\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a73525e6cb448bb139fd128b7917e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "⏳ 正在创建序列:   0%|          | 0/46045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 序列不足 12 帧，自动填充：(2023091004, 1594, 52453)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091004, 1594, 52430)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091004, 4122, 41233)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091008, 1292, 48516)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091008, 1292, 55921)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091008, 1292, 54597)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091400, 318, 52430)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091700, 1728, 46087)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091700, 1728, 54473)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091709, 493, 53531)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091709, 493, 53601)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091709, 493, 56042)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091712, 81, 43327)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091712, 81, 42357)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091712, 2480, 43299)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023091712, 2480, 42357)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023092406, 3048, 44819)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023092410, 1352, 46150)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023092410, 2619, 53565)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023092410, 2619, 52425)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100110, 2747, 53476)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100110, 2747, 54679)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100110, 2747, 55919)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100110, 2747, 41282)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100807, 4264, 43986)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100807, 4264, 55005)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100810, 2151, 54583)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100810, 2151, 54519)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100900, 2328, 42381)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100900, 2328, 47807)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100900, 2962, 45038)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023100900, 2962, 54597)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023101509, 1300, 42361)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023101509, 1300, 53549)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023101509, 3424, 46902)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023101509, 3424, 45185)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023101511, 3940, 43351)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023101511, 3940, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102205, 3383, 47877)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102205, 3383, 54473)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102300, 353, 55887)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 236, 47859)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 39984)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 43351)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 43353)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 54548)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 54808)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 55931)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102908, 3160, 47859)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102909, 3040, 54533)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102909, 3040, 47847)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102913, 950, 53476)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023102913, 950, 46093)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023110509, 1738, 44830)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023110509, 1738, 41282)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023110600, 412, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023110600, 2558, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111204, 341, 47816)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111204, 341, 56527)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111204, 341, 43336)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111210, 1200, 43373)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111210, 1200, 55884)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 679, 52547)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 679, 47974)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 679, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 2937, 55969)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 2937, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 3659, 55969)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111211, 3659, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111903, 1604, 45571)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111903, 1604, 56220)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111906, 587, 43327)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023111906, 587, 41282)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023112604, 1355, 55888)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023112604, 1355, 56052)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023112604, 1588, 55888)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023112604, 1588, 42357)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023120400, 351, 44872)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023120400, 351, 46095)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023121007, 2198, 54583)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023121007, 2198, 41282)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023121705, 174, 56042)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023123102, 3589, 56117)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023123102, 3589, 52495)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2023123102, 3589, 42489)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2024010704, 1678, 43700)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2024010704, 1678, 54475)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2024010707, 3669, 55880)\n",
      "⚠️ 序列不足 12 帧，自动填充：(2024010707, 3669, 56109)\n",
      "\n",
      "✅ 共生成 46045 个序列，每个序列包含 114 个特征。\n",
      "✅ 已准备好 46045 个时序样本，每个样本包含 114 个特征。\n",
      "📏 每个样本的窗口长度为 12 帧。\n",
      "📊 示例样本形状：(12, 114)\n",
      "🎯 训练目标示例：dx=(21,), dy=(21,)\n",
      "🆔 样本索引数量：46045\n"
     ]
    }
   ],
   "source": [
    "# 生成时序样本（训练模式）\n",
    "sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols = prepare_sequences(\n",
    "    train_input,\n",
    "    output_df=train_output,\n",
    "    test_template=test_template,\n",
    "    is_training=True,\n",
    "    window_size=Config.WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# 输出结果信息\n",
    "print(f\"✅ 已准备好 {len(sequences)} 个时序样本，每个样本包含 {len(feature_cols)} 个特征。\")\n",
    "print(f\"📏 每个样本的窗口长度为 {Config.WINDOW_SIZE} 帧。\")\n",
    "print(f\"📊 示例样本形状：{sequences[0].shape}\")\n",
    "print(f\"🎯 训练目标示例：dx={targets_dx[0].shape}, dy={targets_dy[0].shape}\")\n",
    "print(f\"🆔 样本索引数量：{len(sequence_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e66d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已使用 pickle 保存对象到 D:\\数据\\Kaggle\\2026 年 NFL 大数据碗 - 预测\\DATA_DIR000\\train_data_cache_pad.pkl\n"
     ]
    }
   ],
   "source": [
    "# === 保存 pkl 文件 ===\n",
    "save_path = Config.DATA_DIR / \"train_data_cache_pad.pkl\"\n",
    "\n",
    "# 保存（保留 numpy 对象类型）\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"sequences\": sequences,\n",
    "        \"targets_dx\": targets_dx,\n",
    "        \"targets_dy\": targets_dy,\n",
    "        \"targets_frame_ids\": targets_frame_ids,\n",
    "        \"sequence_ids\": sequence_ids,\n",
    "        \"feature_cols\": feature_cols\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ 已使用 pickle 保存对象到 {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c89e84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据加载成功到 *_2 变量！\n"
     ]
    }
   ],
   "source": [
    "# === 加载 pkl 文件 ===\n",
    "data_path = Config.DATA_DIR / \"train_data_cache_pad.pkl\"\n",
    "\n",
    "with open(save_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "sequences_2 = data[\"sequences\"]\n",
    "targets_dx_2 = data[\"targets_dx\"]\n",
    "targets_dy_2 = data[\"targets_dy\"]\n",
    "targets_frame_ids_2 = data[\"targets_frame_ids\"]\n",
    "sequence_ids_2 = data[\"sequence_ids\"]\n",
    "feature_cols_2 = data[\"feature_cols\"]\n",
    "\n",
    "print(\"✅ 数据加载成功到 *_2 变量！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ead9625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sequences]\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=46045\n",
      "list[numpy.ndarray shape=(12, 114) dtype=float64] len=46045\n",
      "5d2684f49c9ca71f64c41636\n",
      "5d2684f49c9ca71f64c41636\n",
      "✅一致\n",
      "[targets_dx]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "d99185f0ebe88b2786faf5c8\n",
      "d99185f0ebe88b2786faf5c8\n",
      "✅一致\n",
      "[targets_dy]\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "list[numpy.ndarray shape=(21,) dtype=float64] len=46045\n",
      "73183fc597e548240f481540\n",
      "73183fc597e548240f481540\n",
      "✅一致\n",
      "[targets_frame_ids]\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=46045\n",
      "list[numpy.ndarray shape=(21,) dtype=int64] len=46045\n",
      "2b505c057b7da08aed772945\n",
      "2b505c057b7da08aed772945\n",
      "✅一致\n",
      "[sequence_ids]\n",
      "list[dict (keys=4)] len=46045\n",
      "list[dict (keys=4)] len=46045\n",
      "9a1568866c31a8c79c11dfc4\n",
      "9a1568866c31a8c79c11dfc4\n",
      "✅一致\n",
      "[feature_cols]\n",
      "list[str] len=114\n",
      "list[str] len=114\n",
      "44cf06a02b1e47da769fea6a\n",
      "44cf06a02b1e47da769fea6a\n",
      "✅一致\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_objects(sequences, sequences_2, \"sequences\")\n",
    "verify_objects(targets_dx, targets_dx_2, \"targets_dx\")\n",
    "verify_objects(targets_dy, targets_dy_2, \"targets_dy\")\n",
    "verify_objects(targets_frame_ids, targets_frame_ids_2, \"targets_frame_ids\")\n",
    "verify_objects(sequence_ids, sequence_ids_2, \"sequence_ids\")\n",
    "verify_objects(feature_cols, feature_cols_2, \"feature_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58acad49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e645de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917ea26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fcffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n",
    "models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348c5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf76bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
