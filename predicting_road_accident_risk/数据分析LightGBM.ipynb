{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç³»ç»Ÿåº“\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "# ç¬¬ä¸‰æ–¹ç§‘å­¦è®¡ç®— & å¯è§†åŒ–\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…ä¹±ç \n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # é»‘ä½“\n",
    "plt.rcParams['axes.unicode_minus'] = False          # è§£å†³è´Ÿå·æ˜¾ç¤ºæˆæ–¹å—çš„é—®é¢˜\n",
    "\n",
    "# æœºå™¨å­¦ä¹  & ä¼˜åŒ–\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–\n",
    "\n",
    "\n",
    "host = \"10.162.147.95\"\n",
    "user = \"user1\"\n",
    "password = \"123456\"\n",
    "\n",
    "database_name = 'predicting_road_accident_risk'  # æ•°æ®åº“åç§°\n",
    "competition = database_name  # ç«èµ›åç§°\n",
    "kaggle_competition_name = \"playground-series-s5e10\"\n",
    "study_save_name = \"LightGBM_model\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = rf'D:\\æ•°æ®\\Kaggle_\\{competition}'\n",
    "else:\n",
    "    dir = os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "DIRS = {\n",
    "    \"dir\":              dir,                                       \n",
    "    \"DATA_DIR000\":      os.path.join(dir, \"DATA_DIR000\"),\n",
    "    \"HISTORY\":          os.path.join(dir, \"HISTORY\", f\"{study_save_name}\"),\n",
    "    \"SUBMISSION\":       os.path.join(dir, \"SUBMISSION\", f\"{study_save_name}\"),\n",
    "}\n",
    "\n",
    "# è‡ªåŠ¨åˆ›å»ºç›®å½•\n",
    "for key, path in DIRS.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# æ‰“å°æ—¶ä¸€è¡Œä¸€ä¸ªåœ°å€\n",
    "print(\"âœ… è·¯å¾„å·²åˆ›å»ºï¼š\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f677d",
   "metadata": {},
   "source": [
    "# æ•°æ®æå–å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½Kaggle è®­ç»ƒé›†å’Œ Bradley ç†”ç‚¹å…¬å¼€æ•°æ®é›†\n",
    "\n",
    "# Kaggle æä¾›çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "# è¾“å‡ºæ•°æ®é›†è§„æ¨¡ï¼Œç¡®è®¤åŠ è½½æˆåŠŸ\n",
    "print(\"Train                        shape:\", train_df.shape)\n",
    "print(\"Test                         shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æµ‹ DataFrame æ¯åˆ—çš„æ•°æ®ç±»å‹ã€ç¼ºå¤±å€¼æƒ…å†µã€å”¯ä¸€å€¼æ•°é‡ï¼Œå¹¶ç»™å‡ºéƒ¨åˆ†ç¤ºä¾‹å€¼ã€‚\n",
    "def dataframe_info(df: pd.DataFrame, n_sample: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    æ£€æµ‹ DataFrame æ¯åˆ—çš„æ•°æ®ç±»å‹ã€ç¼ºå¤±å€¼æƒ…å†µã€å”¯ä¸€å€¼æ•°é‡ï¼Œå¹¶ç»™å‡ºéƒ¨åˆ†ç¤ºä¾‹å€¼ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        df       : è¾“å…¥çš„ pandas DataFrame\n",
    "        n_sample : æ¯åˆ—å±•ç¤ºçš„æ ·æœ¬å€¼æ•°é‡ (é»˜è®¤ 3)\n",
    "    \n",
    "    è¿”å›:\n",
    "        summary_df : åŒ…å«æ¯åˆ—ä¿¡æ¯çš„ DataFrame\n",
    "    \"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        \"æ•°æ®ç±»å‹\": df.dtypes,\n",
    "        \"ç¼ºå¤±å€¼æ•°é‡\": df.isnull().sum(),\n",
    "        \"ç¼ºå¤±å€¼æ¯”ä¾‹\": df.isnull().mean(),\n",
    "        \"å”¯ä¸€å€¼æ•°é‡\": df.nunique()\n",
    "    })\n",
    "    \n",
    "    # æ·»åŠ ç¤ºä¾‹å€¼ï¼ˆå‰ n_sample ä¸ªå”¯ä¸€å€¼ï¼‰\n",
    "    summary[\"ç¤ºä¾‹å€¼ (samples)\"] = df.apply(\n",
    "        lambda col: col.dropna().unique()[:n_sample] if col.notnull().any() else []\n",
    "    )\n",
    "    \n",
    "    # æŒ‰ç¼ºå¤±æ¯”ä¾‹æ’åº\n",
    "    summary = summary.sort_values(\"ç¼ºå¤±å€¼æ¯”ä¾‹\", ascending=False)\n",
    "    print(df.columns.values)\n",
    "    display(summary)\n",
    "    # print(summary)\n",
    "\n",
    "\n",
    "# ä½¿ç”¨æ–¹æ³•\n",
    "dataframe_info(train_df, n_sample = 6)\n",
    "dataframe_info(test_df, n_sample = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88ece8",
   "metadata": {},
   "source": [
    "# æ•°æ®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c871162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å°æ¸…å•\n",
    "def config_to_str(config: dict, indent: int = 0) -> str:\n",
    "    \"\"\"é€’å½’ç”Ÿæˆé…ç½®å­—ç¬¦ä¸²\"\"\"\n",
    "    prefix = \"     \" * indent\n",
    "    lines = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}ğŸ”¹ {key}:\")\n",
    "            lines.append(config_to_str(value, indent + 1))  # é€’å½’æ‹¼æ¥å­å­—å…¸\n",
    "        else:\n",
    "            lines.append(f\"{prefix}- {key:<20}: {value}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f737573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒé…ç½®å•\n",
    "config = {\n",
    "    # å›ºå®šå¼€å…³\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"use_feature_gen\"   : True,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 10,\n",
    "\n",
    "\n",
    "\n",
    "    \"study_save_name\"    : study_save_name,\n",
    "\n",
    "\n",
    "    \n",
    "    # ç‰¹å¾é€‰æ‹© XGBoost å‚æ•°\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:squarederror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"0*mean\",   \n",
    "\n",
    "    # # è®­ç»ƒè®¾ç½®\n",
    "    # \"xgb_train_model_params\": {\n",
    "    #     'max_depth'   : 6,\n",
    "    #     'eta'         : 0.1,\n",
    "    #     'tree_method' : 'hist',\n",
    "    #     'eval_metric' : 'rmse',\n",
    "    # },\n",
    "    # \"num_boost_round\": 15000,\n",
    "\n",
    "\n",
    "    # è®­ç»ƒè®¾ç½®ï¼ˆLightGBMï¼‰\n",
    "    \"lgb_train_model_params\": {\n",
    "        \"objective\": \"regression\",        # å›å½’ä»»åŠ¡\n",
    "        \"metric\": \"rmse\",                 # ä½¿ç”¨ RMSE ä½œä¸ºè¯„ä¼°æŒ‡æ ‡\n",
    "        \"boosting_type\": \"gbdt\",          # æ¢¯åº¦æå‡æ ‘ï¼ˆé»˜è®¤ï¼‰\n",
    "        \"learning_rate\": 0.05,            # å­¦ä¹ ç‡ï¼ˆåŸæ¥ XGB çš„ etaï¼‰\n",
    "        \"num_leaves\": 31,                 # æ§åˆ¶æ ‘çš„å¤æ‚åº¦\n",
    "        \"max_depth\": -1,                  # -1 è¡¨ç¤ºä¸é™åˆ¶æ·±åº¦\n",
    "        \"min_data_in_leaf\": 20,           # æ¯ä¸ªå¶å­æœ€å°‘æ ·æœ¬æ•°\n",
    "        \"feature_fraction\": 0.9,          # æ¯æ¬¡å»ºæ ‘æ—¶ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹\n",
    "        \"bagging_fraction\": 0.8,          # æ¯æ¬¡å»ºæ ‘æ—¶ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹\n",
    "        \"bagging_freq\": 5,                # æ¯ 5 æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡ bagging\n",
    "        \"lambda_l1\": 0.1,                 # L1 æ­£åˆ™åŒ–\n",
    "        \"lambda_l2\": 0.1,                 # L2 æ­£åˆ™åŒ–\n",
    "        \"verbosity\": -1                   # ä¸è¾“å‡ºè¯¦ç»†æ—¥å¿—\n",
    "    },\n",
    "    \"num_boost_round\": 15000,             # ä¸ XGB ç›¸åŒï¼Œç”¨äºæ§åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ‹†åˆ† (ç‰¹å¾çŸ©é˜µ ä¸ ç›®æ ‡å‘é‡)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_features_and_target(train_df: pd.DataFrame, test_df: pd.DataFrame, config: dict):\n",
    "    \"\"\"\n",
    "    æ•°æ®æ‹†åˆ†å‡½æ•°ï¼šæ„é€ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾çŸ©é˜µä¸ç›®æ ‡å‘é‡\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # 2. æ„é€ ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å‘é‡\n",
    "    features_train = train_df.drop(columns=['id', 'accident_risk'])   # è®­ç»ƒé›†ç‰¹å¾ (X)\n",
    "    target_train   = train_df['accident_risk']                            # è®­ç»ƒé›†ç›®æ ‡ (y, ç†”ç‚¹)\n",
    "    features_test  = test_df.drop(columns=['id'])    # æµ‹è¯•é›†ç‰¹å¾ (æ—  Tm)\n",
    "\n",
    "\n",
    "\n",
    "    # éšæœºé€‰å–éƒ¨åˆ†ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼š50 ä¸ªï¼‰\n",
    "    if config[\"ISTEST\"]:\n",
    "\n",
    "        sample_len = 100\n",
    "        features_train = train_df.drop(columns=['id', 'accident_risk']).iloc[:sample_len]  # è®­ç»ƒç‰¹å¾ (å‰ 1000 æ¡)\n",
    "        target_train = train_df.iloc[:sample_len]['accident_risk']               # è®­ç»ƒç›®æ ‡\n",
    "        features_test = test_df.drop(columns=['id'])                # æµ‹è¯•ç‰¹å¾ (åŒæ ·çš„ç‰¹å¾åˆ—)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3. æ‰“å°ç»´åº¦ä¿¡æ¯\n",
    "    print(\"ğŸ“Š æ•°æ®æ‹†åˆ†å®Œæˆ\")\n",
    "    print(f\"è®­ç»ƒé›†ç‰¹å¾ features_train  shape   : {features_train.shape}\")\n",
    "    print(f\"è®­ç»ƒé›†ç›®æ ‡   target_train  shape   : {target_train.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†ç‰¹å¾  features_test  shape   : {features_test.shape}\")\n",
    "    print(f\"           features_train  ç±»å‹    : {type(features_train)}\")\n",
    "\n",
    "    return features_train, target_train, features_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9f7ae",
   "metadata": {},
   "source": [
    "### ç‰¹å¾ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32770063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    é’ˆå¯¹é“è·¯äº‹æ•…é¢„æµ‹æ„é€ æ–°çš„è¡ç”Ÿç‰¹å¾\n",
    "    è¾“å…¥:\n",
    "        df : pd.DataFrame\n",
    "    è¾“å‡º:\n",
    "        df_new : pd.DataFrameï¼ŒåŒ…å«æ–°å¢ç‰¹å¾\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    shape_before = df.shape  # åŸå§‹ç»´åº¦\n",
    "\n",
    "    # 1. äº¤äº’é¡¹\n",
    "    df['Speed_x_Curvature'] = df['speed_limit'] * df['curvature']\n",
    "    df['Lanes_x_Speed']     = df['num_lanes'] * df['speed_limit']\n",
    "    df['Accidents_x_Speed'] = df['num_reported_accidents'] * df['speed_limit']\n",
    "\n",
    "    # ç±»åˆ«äº¤äº’é¡¹ï¼ˆéœ€è¦åç»­ OneHotEncoderï¼‰\n",
    "    df['RoadType_Time']     = df['road_type'] + \"_\" + df['time_of_day']\n",
    "    df['Lighting_Weather']  = df['lighting'] + \"_\" + df['weather']\n",
    "\n",
    "    # 2. éçº¿æ€§å˜æ¢\n",
    "    df['Log_Accidents']     = np.log1p(df['num_reported_accidents'])  # log(1+x)\n",
    "\n",
    "    # åˆ†ç®±ï¼ˆè‡ªå®šä¹‰é˜ˆå€¼ï¼Œå¯è°ƒèŠ‚ï¼‰\n",
    "    df['Accident_Bins'] = pd.cut(df['num_reported_accidents'],\n",
    "                                bins=[-1, 0, 2, 5, np.inf],\n",
    "                                labels=['none', 'low', 'medium', 'high'])\n",
    "\n",
    "    # 3. æ¯”ç‡ç‰¹å¾\n",
    "    df['Accidents_per_Lane']   = df['num_reported_accidents'] / (df['num_lanes'] + 1)\n",
    "    df['Curvature_per_Lane']   = df['curvature'] / (df['num_lanes'] + 1)\n",
    "    df['Speed_per_Lane']       = df['speed_limit'] / (df['num_lanes'] + 1)\n",
    "\n",
    "\n",
    "    shape_after = df.shape  # æ–°çš„ç»´åº¦\n",
    "\n",
    "    print(f\"{shape_before[0]} Ã— {shape_before[1]} --> {shape_after[0]} Ã— {shape_after[1]}   æ–°å¢ {shape_after[1] - shape_before[1]} åˆ—\")\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e546d",
   "metadata": {},
   "source": [
    "### æ‰‹åŠ¨ç¼–ç  & å¸ƒå°”è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ae441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def fit_ohe(train_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    åœ¨è®­ç»ƒé›†ä¸Šè‡ªåŠ¨æ£€æµ‹ object/category åˆ—ï¼Œå¹¶æ‹Ÿåˆ OneHotEncoderã€‚\n",
    "    è¿”å› encoder å¯¹è±¡å’Œç±»åˆ«åˆ—åã€‚\n",
    "    \"\"\"\n",
    "    # æ£€æµ‹ç±»åˆ«åˆ—\n",
    "    cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(f\"æ£€æµ‹åˆ°{len(cat_cols)}ç±»: {cat_cols}\")\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    ohe.fit(train_df[cat_cols])\n",
    "\n",
    "    return ohe, cat_cols\n",
    "\n",
    "\n",
    "def transform_ohe(df: pd.DataFrame, ohe: OneHotEncoder, cat_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å·²æ‹Ÿåˆçš„ OneHotEncoder å¯¹ DataFrame è¿›è¡Œç¼–ç ã€‚\n",
    "    è‡ªåŠ¨å¤„ç† bool -> intï¼Œä¿æŒ train/test ä¸€è‡´ã€‚\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    shape_before = df.shape\n",
    "\n",
    "    # å¸ƒå°”å‹è½¬ int\n",
    "    for col in df.select_dtypes('bool').columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "    # OHE\n",
    "    ohe_array = ohe.transform(df[cat_cols])\n",
    "    ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
    "    ohe_df = pd.DataFrame(ohe_array, columns=ohe_cols, index=df.index)\n",
    "\n",
    "    # æ‹¼æ¥\n",
    "    df_enc = pd.concat([df.drop(columns=cat_cols), ohe_df], axis=1)\n",
    "\n",
    "    shape_after = df_enc.shape\n",
    "    print(f\"{shape_before[0]} Ã— {shape_before[1]} --> {shape_after[0]} Ã— {shape_after[1]}   æ–°å¢ {shape_after[1] - shape_before[1]} åˆ—\")\n",
    "\n",
    "    return df_enc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f45be",
   "metadata": {},
   "source": [
    "### ç‰¹å¾ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4582b1",
   "metadata": {},
   "source": [
    "### PCAé™ç»´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ•°æ®é™ç»´\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "def fit_svd(train_df: pd.DataFrame, n_components: int = 100, random_state: int = 42) -> TruncatedSVD:\n",
    "    \"\"\"\n",
    "    åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ TruncatedSVDï¼Œå¹¶è¿”å›æ‹Ÿåˆå¥½çš„æ¨¡å‹ã€‚\n",
    "    \"\"\"\n",
    "    X_sparse = sparse.csr_matrix(train_df.values)\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "    svd.fit(X_sparse)\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "    print(f\"âœ… SVD å·²æ‹Ÿåˆå®Œæˆ (n_components={n_components})ï¼Œè®­ç»ƒé›†ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”: {explained_var:.2%}\")\n",
    "    return svd\n",
    "\n",
    "def transform_svd(df: pd.DataFrame, svd: TruncatedSVD) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ‹Ÿåˆå¥½çš„ SVD å¯¹æ•°æ®é›†è¿›è¡Œé™ç»´ã€‚\n",
    "    \"\"\"\n",
    "    shape_before = df.shape\n",
    "    X_sparse = sparse.csr_matrix(df.values)\n",
    "    X_reduced_array = svd.transform(X_sparse)\n",
    "\n",
    "    reduced_df = pd.DataFrame(\n",
    "        X_reduced_array,\n",
    "        index=df.index,\n",
    "        columns=[f\"SVD_{i+1}\" for i in range(X_reduced_array.shape[1])]\n",
    "    )\n",
    "\n",
    "    shape_after = reduced_df.shape\n",
    "    print(f\"{shape_before[0]} Ã— {shape_before[1]} --> {shape_after[0]} Ã— {shape_after[1]}\")\n",
    "    return reduced_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb982f",
   "metadata": {},
   "source": [
    "# äº¤å‰è®­ç»ƒéªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold + XGBoost è¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œå¹¶ä¿å­˜å®éªŒç»“æœ\n",
    "# ==============================================================\n",
    "def run_kfold_xgb(features_train, target_train, features_test, config, DIRS, K_FOLDS=10, verbose=0):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Stratified K-Fold + XGBoost è¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œå¹¶ä¿å­˜å®éªŒç»“æœ\n",
    "\n",
    "    å‚æ•°:\n",
    "        features_train, target_train        : è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "        features_test      : æµ‹è¯•é›†ç‰¹å¾\n",
    "        params      : XGBoost æœ€ä¼˜å‚æ•° (dict)\n",
    "        DIRS        : ä¿å­˜ç»“æœçš„ç›®å½•å­—å…¸\n",
    "        K_FOLDS     : æŠ˜æ•° (é»˜è®¤=5)\n",
    "        verbose     : æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "    config[\"X shape\"] = features_train.shape\n",
    "    config[\"y shape\"] = target_train.shape\n",
    "    config[\"X_test shape\"] = features_test.shape\n",
    "\n",
    "\n",
    "    # ---------- åˆ›å»ºç›®å½• ----------\n",
    "    for _, path in DIRS.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = os.path.join(DIRS['HISTORY'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"â€”â€”\" * 20)\n",
    "    print(f\"âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: {time_str}\")\n",
    "\n",
    "\n",
    "    # ---------- å®šä¹‰äº¤å‰éªŒè¯ ----------\n",
    "    skfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    yeo = PowerTransformer(method=\"yeo-johnson\")                                # å®šä¹‰ Yeo-Johnson å˜æ¢\n",
    "\n",
    "    # ---------- åˆå§‹åŒ–å­˜å‚¨ ----------\n",
    "    oof_val = np.zeros(len(features_train))       # OOF é¢„æµ‹\n",
    "    train_score, val_score = [], []  # æ¯æŠ˜ LOSS\n",
    "    test_pred = []                   # æ¯æŠ˜ test é¢„æµ‹\n",
    "    fold_records = []                # ä¿å­˜æ¯æŠ˜ä¿¡æ¯\n",
    "    all_importances = []             # ç‰¹å¾é‡è¦æ€§\n",
    "    elapsed_list = []                # è€—æ—¶è®°å½•\n",
    "\n",
    "\n",
    "\n",
    "    # å¾ªç¯æ¯ä¸€æŠ˜\n",
    "    # ==============================================================\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skfold.split(features_train, pd.qcut(target_train, q=10).cat.codes), 1):\n",
    "\n",
    "        # ----- æ‰“å°æ—¶é—´ä¿¡æ¯ -----\n",
    "        start_now = datetime.now()\n",
    "        start_str = start_now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if elapsed_list:\n",
    "            avg_time = np.mean(elapsed_list)\n",
    "            est_end = start_now + timedelta(seconds=avg_time)\n",
    "\n",
    "            # æ¯ 5 ä¸ªä¸€ç»„è¾“å‡ºè€—æ—¶\n",
    "            parts = [f\"{t:6.1f}s\" for t in elapsed_list]\n",
    "            grouped = [\" \".join(parts[j:j+5]) for j in range(0, len(parts), 5)]\n",
    "            elapsed_str = \" /// \".join(grouped)\n",
    "\n",
    "            print(\n",
    "                f\"ğŸ”„{i:2d}/{K_FOLDS} ST {start_str}\"\n",
    "                f\" ET {est_end.strftime('%H:%M:%S')}\"\n",
    "                f\" avg {avg_time:.1f}s\"\n",
    "                f\" [{elapsed_str}]\",\n",
    "                end=\"\\r\", flush=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"ğŸ”„{i:2d}/{K_FOLDS} ST {start_str} ET (æš‚æ— å†å²æ•°æ®)\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "        # ----- å¼€å§‹è®­ç»ƒ -----\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. æ•°æ®é›†åˆ’åˆ†\n",
    "        x_train, x_val = features_train.iloc[train_idx], features_train.iloc[val_idx]\n",
    "        y_train, y_val = target_train[train_idx], target_train[val_idx]\n",
    "\n",
    "        # 2. Yeo-Johnson å˜æ¢\n",
    "        y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "        y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "\n",
    "        # 3. ç‰¹å¾é€‰æ‹©ï¼ˆè½»é‡çº§ XGBoostï¼‰\n",
    "        # ä½¿ç”¨\n",
    "        selector_model = xgb.XGBRegressor(**config[\"xgb_selector_model_params\"])\n",
    "        # selector_model = xgb.XGBRegressor(\n",
    "        #     n_estimators   = 500,\n",
    "        #     max_depth      = 6,\n",
    "        #     learning_rate  = 0.05,\n",
    "        #     random_state   = 2025,\n",
    "        #     device         = \"cpu\",\n",
    "        #     objective      = \"reg:absoluteerror\",\n",
    "        #     tree_method    = \"hist\",\n",
    "        #     verbosity      = 0\n",
    "        # )\n",
    "        \n",
    "        \n",
    "\n",
    "        selector_model.fit(x_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(selector_model, prefit=True, threshold=config[\"selector_threshold\"])\n",
    "        selected_features = x_train.columns[selector.get_support()].tolist()\n",
    "        if verbose > 0:\n",
    "            print(f\"âœ… é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}\")\n",
    "\n",
    "\n",
    "        # 4. ä¿ç•™é‡è¦ç‰¹å¾\n",
    "        x_train_new = x_train[selected_features]\n",
    "        x_val_new   = x_val[selected_features]\n",
    "        x_test_new  = features_test[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 5. æ„é€  LightGBM æ•°æ®é›†\n",
    "        dtrain = lgb.Dataset(x_train_new, label=y_train)\n",
    "        dval   = lgb.Dataset(x_val_new,   label=y_val)\n",
    "        # dtest  = lgb.Dataset(x_test_new,  label=y_test)\n",
    "\n",
    "        early_stop_rounds = 800 if config[\"lgb_train_model_params\"][\"learning_rate\"] < 0.03 else 300\n",
    "        use_early_stopping = (config[\"lgb_train_model_params\"][\"boosting_type\"] != \"dart\")\n",
    "\n",
    "        callbacks = []\n",
    "        if use_early_stopping:\n",
    "            callbacks.append(lgb.early_stopping(stopping_rounds=300))\n",
    "        callbacks.append(lgb.log_evaluation(period=1000))\n",
    "\n",
    "        # 6. LightGBM è®­ç»ƒ\n",
    "        lgb_model = lgb.train(\n",
    "            params=config[\"lgb_train_model_params\"],\n",
    "            train_set=dtrain,\n",
    "            valid_sets=[dtrain, dval],\n",
    "            valid_names=[\"train\", \"valid\"],\n",
    "            num_boost_round=config[\"num_boost_round\"],\n",
    "        callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # # ä¿å­˜æ¨¡å‹\n",
    "        # model_path = os.path.join(history_DIR, f\"lgb_model_fold{i}.txt\")\n",
    "        # lgb_model.save_model(model_path)\n",
    "\n",
    "\n",
    "\n",
    "        # 7. è·å–ç‰¹å¾é‡è¦æ€§\n",
    "        # imp_dict = xgb_model.get_score(importance_type=\"gain\")\n",
    "        # imp_df = pd.DataFrame(imp_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "        # imp_df[\"Fold\"] = i\n",
    "        # all_importances.append(imp_df)\n",
    "\n",
    "\n",
    "        # 8. é¢„æµ‹\n",
    "        y_train_pred = lgb_model.predict(x_train_new, num_iteration=lgb_model.best_iteration)\n",
    "        y_val_pred   = lgb_model.predict(x_val_new,   num_iteration=lgb_model.best_iteration)\n",
    "        y_test_pred  = lgb_model.predict(x_test_new,  num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 9. é€†å˜æ¢\n",
    "        y_train      = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "        y_val        = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "        y_train_pred = yeo.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
    "        y_val_pred   = yeo.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
    "        y_test_pred  = yeo.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        # 10. è®¡ç®— LOSS\n",
    "        train_loss = np.sqrt(np.mean((y_train - y_train_pred) ** 2))\n",
    "        val_loss   = np.sqrt(np.mean((y_val   - y_val_pred) ** 2))\n",
    "\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Fold {i}: Train LOSS={train_loss:.4f}, Val LOSS={val_loss:.4f}ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’\")\n",
    "\n",
    "\n",
    "        # ----- ä¿å­˜ç»“æœ -----\n",
    "        train_score.append(train_loss)\n",
    "        val_score.append(val_loss)\n",
    "        oof_val[val_idx] = y_val_pred\n",
    "        test_pred.append(y_test_pred)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        elapsed_list.append(elapsed)\n",
    "\n",
    "        fold_records.append({\n",
    "            \"Fold\": i,\n",
    "            \"Train_LOSS\": train_loss,\n",
    "            \"Val_LOSS\": val_loss,\n",
    "            \"Num_Features\": len(selected_features),\n",
    "            \"Selected_Features\": selected_features,\n",
    "            \"elapsed\": elapsed\n",
    "        })\n",
    "\n",
    "    # ä¿å­˜æ•´ä½“ç»“æœ\n",
    "    # ==============================================================\n",
    "    if verbose > 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"ğŸ“Š Train LOSS å¹³å‡å€¼ : {np.mean(train_score):.4f}\")\n",
    "        print(f\"ğŸ“Š Val   LOSS å¹³å‡å€¼ : {np.mean(val_score):.4f}\")\n",
    "        print(f\"ğŸ“Š Train LOSS æ ‡å‡†å·® : {np.std(train_score, ddof=0):.4f}\")\n",
    "        print(f\"ğŸ“Š Val   LOSS æ ‡å‡†å·® : {np.std(val_score, ddof=0):.4f}\")\n",
    "\n",
    "    # å‚æ•°\n",
    "    with open(os.path.join(history_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # æ¯æŠ˜ä¿¡æ¯\n",
    "    folds_df = pd.DataFrame(fold_records)\n",
    "    folds_df.to_csv(os.path.join(history_DIR, \"folds_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # # ç‰¹å¾é‡è¦æ€§\n",
    "    # if all_importances:\n",
    "    #     valid_imps = [df for df in all_importances if not df.empty]\n",
    "    #     all_imp_df = pd.concat(valid_imps, axis=0) if valid_imps else pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    # else:\n",
    "    #     all_imp_df = pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    # all_imp_df.to_csv(os.path.join(history_DIR, \"feature_importance_all.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    test_pred_array = np.vstack(test_pred).T\n",
    "    test_pred_df = pd.DataFrame(test_pred_array, columns=[f\"Fold_{j+1}\" for j in range(test_pred_array.shape[1])])\n",
    "    test_pred_df[\"Final_Pred\"] = test_pred_df.mean(axis=1)\n",
    "    test_pred_df.to_csv(os.path.join(history_DIR, \"test_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # æ€»ç»“\n",
    "    with open(os.path.join(history_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train LOSS Mean : {np.mean(train_score):.4f}\\n\")\n",
    "        f.write(f\"Val   LOSS Mean : {np.mean(val_score):.4f}\\n\")\n",
    "        f.write(f\"Train LOSS Std  : {np.std(train_score, ddof=0):.4f}\\n\")\n",
    "        f.write(f\"Val   LOSS Std  : {np.std(val_score, ddof=0):.4f}\\n\")\n",
    "\n",
    "\n",
    "    # æœ€ç»ˆæäº¤\n",
    "    final_score = np.mean(val_score)\n",
    "    submission = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"sample_submission.csv\"))\n",
    "    submission[\"accident_risk\"] = test_pred_df[\"Final_Pred\"]\n",
    "\n",
    "    submission_path = os.path.join(history_DIR, f\"sub_{time_str}_{final_score:.8f}.csv\")\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    submission.to_csv(os.path.join(DIRS['SUBMISSION'], f\"sub_{time_str}_{final_score:.8f}.csv\"), index=False)\n",
    "\n",
    "        \n",
    "    config[\"time_str\"] = time_str\n",
    "    config[\"score\"] = final_score\n",
    "\n",
    "\n",
    "    # ---------- è¿”å›ç»“æœ ----------\n",
    "    return {\n",
    "        \"oof_val\": oof_val,\n",
    "        \"train_score\": train_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_pred\": test_pred_df,\n",
    "        \"folds_info\": folds_df,\n",
    "        # \"feature_importance\": all_imp_df,\n",
    "        \"submission_path\": submission_path,\n",
    "        \"time\": time_str,\n",
    "        \"final_score\": final_score,\n",
    "        \"config\": config\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f752",
   "metadata": {},
   "source": [
    "# å•æ¬¡è®­ç»ƒæ¨å¯¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œä¸€æ¬¡\n",
    "\n",
    "# å‡†å¤‡æµç¨‹---------------------------------------------------------------------------------------------------\n",
    "# æ‰“å°å½“å‰config\n",
    "print(config_to_str(config))\n",
    "\n",
    "# Kaggle æä¾›çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "# æ•°æ®æ‹†åˆ†\n",
    "print(\"æ•°æ®æ‹†åˆ†---------------------------\")\n",
    "features_train, target_train, features_test = prepare_features_and_target(train_df, test_df, config)\n",
    "\n",
    "\n",
    "dataframe_info(features_train, n_sample = 6)\n",
    "\n",
    "\n",
    "# ç‰¹å¾ç”Ÿæˆ\n",
    "print(\"ç‰¹å¾ç”Ÿæˆ---------------------------\")\n",
    "if config[\"use_feature_gen\"]:\n",
    "    features_train = add_new_features(features_train)\n",
    "    features_test  = add_new_features(features_test)\n",
    "\n",
    "\n",
    "dataframe_info(features_train, n_sample = 6)\n",
    "\n",
    "\n",
    "# æ‰‹åŠ¨ç¼–ç  & å¸ƒå°”è½¬æ¢\n",
    "print(\"æ‰‹åŠ¨ç¼–ç  & å¸ƒå°”è½¬æ¢----------------\")\n",
    "# åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ\n",
    "ohe, cat_cols = fit_ohe(features_train)\n",
    "# åˆ†åˆ«ç¼–ç  train/testï¼Œä¿è¯ä¸€è‡´\n",
    "features_train = transform_ohe(features_train, ohe, cat_cols)\n",
    "features_test  = transform_ohe(features_test,  ohe, cat_cols)\n",
    "\n",
    "\n",
    "# æ•°æ®é™ç»´\n",
    "print(\"æ•°æ®é™ç»´---------------------------\")\n",
    "if config[\"use_pca\"]:\n",
    "    # 1. åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ\n",
    "    svd = fit_svd(features_train, n_components = config[\"pca_components\"], random_state=42)\n",
    "\n",
    "    # 2. åˆ†åˆ«å¯¹ train/test transform\n",
    "    features_train_reduced = transform_svd(features_train, svd)\n",
    "    features_test_reduced  = transform_svd(features_test, svd)\n",
    "\n",
    "    shape_before_train = features_train.shape\n",
    "    shape_before_test = features_test.shape\n",
    "\n",
    "    features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "    features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "    shape_after_train = features_train.shape\n",
    "    shape_after_test = features_test.shape\n",
    "\n",
    "    print(f\"Train: {shape_before_train[0]} Ã— {shape_before_train[1]}  -->  {shape_after_train[0]} Ã— {shape_after_train[1]}   æ–°å¢ {shape_after_train[1] - shape_before_train[1]} åˆ—\")\n",
    "    print(f\"Test : {shape_before_test[0]} Ã— {shape_before_test[1]}  -->  {shape_after_test[0]} Ã— {shape_after_test[1]}   æ–°å¢ {shape_after_test[1] - shape_before_test[1]} åˆ—\")\n",
    "\n",
    "\n",
    "\n",
    "X, y, X_test = features_train, target_train, features_test\n",
    "print(\"å¼€å§‹è®­ç»ƒ---------------------------\")\n",
    "\n",
    "# å‡†å¤‡æµç¨‹---------------------------------------------------------------------------------------------------\n",
    "\n",
    "results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "config = results['config']\n",
    "score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "print('\\n',score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3573b32",
   "metadata": {},
   "source": [
    "# æäº¤ kaggle å¹³å°æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ® submission_time å®šä½æ–‡ä»¶è·¯å¾„ æäº¤ kaggle å¹³å°æµ‹è¯•\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def find_submission_file(submission_time, submission_dir):\n",
    "    \"\"\"\n",
    "    åœ¨ submission_dir ä¸‹æŸ¥æ‰¾åŒ…å« submission_time çš„æ–‡ä»¶\n",
    "    ä¸€æ—¦æ‰¾åˆ°ç«‹åˆ»è¿”å›å®Œæ•´è·¯å¾„ï¼›å¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å› None\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(submission_dir):\n",
    "        if submission_time in fname:\n",
    "            file_path = os.path.join(submission_dir, fname)\n",
    "            print(f\"âœ… æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {fname}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(f\"âš ï¸ æœªæ‰¾åˆ°åŒ…å« {submission_time} çš„æ–‡ä»¶\")\n",
    "    return None\n",
    "\n",
    "def submit_and_get_score(file_path, competition_name, message=\"My submission\"):\n",
    "    \"\"\"\n",
    "    å°è£… Kaggle æäº¤å¹¶ç­‰å¾…ç»“æœè¯„åˆ†\n",
    "    --------------------------------------\n",
    "    file_path        : str  æäº¤æ–‡ä»¶è·¯å¾„\n",
    "    competition_name : str  Kaggle æ¯”èµ›åç§° (URL æœ€åä¸€æ®µ)\n",
    "    message          : str  æäº¤å¤‡æ³¨\n",
    "    \"\"\"\n",
    "    # 1. é…ç½® Kaggle API\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = r\"C:\\Users\\Admin\\.kaggle\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"âœ… Kaggle API å·²ç»é…ç½®æˆåŠŸï¼\")\n",
    "\n",
    "    # 2. æäº¤æ–‡ä»¶\n",
    "    api.competition_submit(\n",
    "        file_name=file_path,\n",
    "        competition=competition_name,\n",
    "        message=message\n",
    "    )\n",
    "    print(\"âœ… æäº¤å®Œæˆï¼è¯·ç­‰å¾…è¯„åˆ†...\")\n",
    "\n",
    "    # 3. åŠ¨æ€ç­‰å¾…\n",
    "    spinner = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
    "    while True:\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        latest = submissions[0]\n",
    "        status_str = str(latest._status).lower()\n",
    "\n",
    "        if \"complete\" in status_str and latest._public_score is not None:\n",
    "            print(\"\\nğŸ¯ æœ€ç»ˆç»“æœ:\")\n",
    "            print(f\"Public åˆ†æ•° : {latest._public_score}\")\n",
    "            print(f\"Private åˆ†æ•°: {latest._private_score}\")\n",
    "            print(f\"æäº¤ ID     : {latest._ref}\")\n",
    "            print(f\"æ–‡ä»¶å      : {latest._file_name}\")\n",
    "            print(f\"çŠ¶æ€        : {latest._status}\")\n",
    "            print(f\"æäº¤æ—¶é—´    : {latest._date}\")\n",
    "            print(f\"æè¿°/å¤‡æ³¨   : {latest._description}\")\n",
    "            return latest\n",
    "\n",
    "        if \"pending\" in status_str:\n",
    "            spin_char = next(spinner)\n",
    "            print(f\"å½“å‰çŠ¶æ€: {status_str} , ç­‰å¾…ä¸­ {spin_char}\", end=\"\\r\", flush=True)\n",
    "            time.sleep(0.2)  # æ¯ 0.5 ç§’æ£€æŸ¥ä¸€æ¬¡\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            print(f\"\\næŠ¥é”™\")\n",
    "            print(f\"submissions\")\n",
    "            \n",
    "            break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb1c16",
   "metadata": {},
   "source": [
    "### ä¸è½»æ˜“è¿è¡Œï¼Œå†ä¸‰è€ƒè™‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_time æäº¤\n",
    "submission_time = \"2025-10-24 01-23-41\"\n",
    "competition_name = kaggle_competition_name\n",
    "message =  f\"è¯¥æäº¤æ–‡ä»¶çš„å‚æ•°ï¼š\\n{config_to_str(config)} \"\n",
    "\n",
    "target_file = find_submission_file(submission_time, DIRS['SUBMISSION'] )\n",
    "\n",
    "# submit_and_get_score(target_file, competition_name, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9607e",
   "metadata": {},
   "source": [
    "# å‚æ•°ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒé…ç½®å•\n",
    "base_config = {\n",
    "    # å›ºå®šå¼€å…³\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 10,\n",
    "\n",
    "\n",
    "\n",
    "    \"study_save_name\"    : study_save_name,\n",
    "\n",
    "    \n",
    "    # ç‰¹å¾é€‰æ‹© XGBoost å‚æ•°\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:squarederror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"0*mean\",   \n",
    "\n",
    "    # # è®­ç»ƒè®¾ç½®\n",
    "    # \"xgb_train_model_params\": {\n",
    "    #     'max_depth'   : 6,\n",
    "    #     'eta'         : 0.1,\n",
    "    #     'tree_method' : 'hist',\n",
    "    #     'eval_metric' : 'rmse',\n",
    "    # },\n",
    "    # \"num_boost_round\": 15000,\n",
    "\n",
    "\n",
    "    # è®­ç»ƒè®¾ç½®ï¼ˆLightGBMï¼‰\n",
    "    \"lgb_train_model_params\": {\n",
    "        \"objective\": \"regression\",        # å›å½’ä»»åŠ¡\n",
    "        \"metric\": \"rmse\",                 # ä½¿ç”¨ RMSE ä½œä¸ºè¯„ä¼°æŒ‡æ ‡\n",
    "        \"boosting_type\": \"gbdt\",          # æ¢¯åº¦æå‡æ ‘ï¼ˆé»˜è®¤ï¼‰\n",
    "        \"learning_rate\": 0.05,            # å­¦ä¹ ç‡ï¼ˆåŸæ¥ XGB çš„ etaï¼‰\n",
    "        \"num_leaves\": 31,                 # æ§åˆ¶æ ‘çš„å¤æ‚åº¦\n",
    "        \"max_depth\": -1,                  # -1 è¡¨ç¤ºä¸é™åˆ¶æ·±åº¦\n",
    "        \"min_data_in_leaf\": 20,           # æ¯ä¸ªå¶å­æœ€å°‘æ ·æœ¬æ•°\n",
    "        \"feature_fraction\": 0.9,          # æ¯æ¬¡å»ºæ ‘æ—¶ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹\n",
    "        \"bagging_fraction\": 0.8,          # æ¯æ¬¡å»ºæ ‘æ—¶ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹\n",
    "        \"bagging_freq\": 5,                # æ¯ 5 æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡ bagging\n",
    "        \"lambda_l1\": 0.1,                 # L1 æ­£åˆ™åŒ–\n",
    "        \"lambda_l2\": 0.1,                 # L2 æ­£åˆ™åŒ–\n",
    "        \"verbosity\": -1                   # ä¸è¾“å‡ºè¯¦ç»†æ—¥å¿—\n",
    "    },\n",
    "    \"num_boost_round\": 15000,             # ä¸ XGB ç›¸åŒï¼Œç”¨äºæ§åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¼˜åŒ–ä»»åŠ¡  åŠ å…¥æ ‡è¯†ç¬¦ host: hao-2   ip: 192.168.40.1\n",
    "\n",
    "import copy\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna çš„ç›®æ ‡å‡½æ•° (Objective Function)\n",
    "    æ¯æ¬¡ trial ä¼šç”Ÿæˆä¸€ç»„è¶…å‚æ•°ï¼Œç”¨äºè®­ç»ƒ XGBoost æ¨¡å‹ï¼Œ\n",
    "    å¹¶è¿”å›äº¤å‰éªŒè¯çš„å¹³å‡ RMSE ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. å®šä¹‰ è¶…å‚æ•° æœç´¢ç©ºé—´\n",
    "    # æ‹·è´ä¸€ä»½ configï¼Œé¿å…å…¨å±€æ±¡æŸ“\n",
    "    config = copy.deepcopy(base_config)\n",
    "\n",
    "    # åªä¿®æ”¹éœ€è¦ä¼˜åŒ–çš„å‚æ•°\n",
    "    config[\"use_feature_gen\"]   = trial.suggest_categorical(\"use_feature_gen\", [True, False])\n",
    "    config[\"use_pca\"]           = trial.suggest_categorical(\"use_pca\", [True, False])\n",
    "    config[\"pca_components\"] = trial.suggest_categorical(\"pca_components\", [5, 10, 15, 20])\n",
    "\n",
    "    # config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_categorical(\"selector_random_state\", [2025])\n",
    "    # config[\"xgb_selector_model_params\"][\"device\"]       = trial.suggest_categorical(\"selector_device\", [\"cpu\", \"cuda\"])\n",
    "\n",
    "    # config[\"selector_threshold\"] = trial.suggest_categorical(\"selector_threshold\", [\"0*mean\", \"0.25*mean\", \"0.5*mean\", \"0.75*mean\", \"mean\"])\n",
    "\n",
    "    # config[\"xgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\"train_max_depth\", 3, 12)\n",
    "    # config[\"xgb_train_model_params\"][\"eta\"] = trial.suggest_float(\"train_eta\", 0.01 , 0.3 , log=True)\n",
    "    config[\"lgb_train_model_params\"][\"learning_rate\"] = trial.suggest_float(\n",
    "        \"learning_rate\", 0.005, 0.1, log=True\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"num_leaves\"] = trial.suggest_int(\n",
    "        \"num_leaves\", 16, 256\n",
    "    )\n",
    "    # config[\"lgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\n",
    "    #     \"max_depth\", -1, 15\n",
    "    # )\n",
    "    config[\"lgb_train_model_params\"][\"min_data_in_leaf\"] = trial.suggest_int(\n",
    "        \"min_data_in_leaf\", 10, 100\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"feature_fraction\"] = trial.suggest_float(\n",
    "        \"feature_fraction\", 0.6, 1.0\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"bagging_fraction\"] = trial.suggest_float(\n",
    "        \"bagging_fraction\", 0.6, 1.0\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"bagging_freq\"] = trial.suggest_int(\n",
    "        \"bagging_freq\", 1, 10\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"lambda_l1\"] = trial.suggest_float(\n",
    "        \"lambda_l1\", 1e-3, 10.0, log=True\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"lambda_l2\"] = trial.suggest_float(\n",
    "        \"lambda_l2\", 1e-3, 10.0, log=True\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"boosting_type\"] = trial.suggest_categorical(\n",
    "        \"boosting_type\", [\"gbdt\", \"dart\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # åˆ›å»ºä¸€ä¸ªé»‘æ´ç¼“å†²åŒº\n",
    "    f = io.StringIO()\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # å‡†å¤‡æµç¨‹---------------------------------------------------------------------------------------------------\n",
    "        # æ‰“å°å½“å‰config\n",
    "        print(config_to_str(config))\n",
    "\n",
    "        # Kaggle æä¾›çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "        train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "        test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "        # æ•°æ®æ‹†åˆ†\n",
    "        print(\"æ•°æ®æ‹†åˆ†---------------------------\")\n",
    "        features_train, target_train, features_test = prepare_features_and_target(train_df, test_df, config)\n",
    "\n",
    "        # ç‰¹å¾ç”Ÿæˆ\n",
    "        print(\"ç‰¹å¾ç”Ÿæˆ---------------------------\")\n",
    "        if config[\"use_feature_gen\"]:\n",
    "            features_train = add_new_features(features_train)\n",
    "            features_test  = add_new_features(features_test)\n",
    "\n",
    "\n",
    "        # æ‰‹åŠ¨ç¼–ç  & å¸ƒå°”è½¬æ¢\n",
    "        print(\"æ‰‹åŠ¨ç¼–ç  & å¸ƒå°”è½¬æ¢----------------\")\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ\n",
    "        ohe, cat_cols = fit_ohe(features_train)\n",
    "        # åˆ†åˆ«ç¼–ç  train/testï¼Œä¿è¯ä¸€è‡´\n",
    "        features_train = transform_ohe(features_train, ohe, cat_cols)\n",
    "        features_test  = transform_ohe(features_test,  ohe, cat_cols)\n",
    "\n",
    "\n",
    "        # æ•°æ®é™ç»´\n",
    "        print(\"æ•°æ®é™ç»´---------------------------\")\n",
    "        if config[\"use_pca\"]:\n",
    "            # 1. åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ\n",
    "            svd = fit_svd(features_train, n_components = config[\"pca_components\"], random_state=42)\n",
    "\n",
    "            # 2. åˆ†åˆ«å¯¹ train/test transform\n",
    "            features_train_reduced = transform_svd(features_train, svd)\n",
    "            features_test_reduced  = transform_svd(features_test, svd)\n",
    "\n",
    "            shape_before_train = features_train.shape\n",
    "            shape_before_test = features_test.shape\n",
    "\n",
    "            features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "            features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "            shape_after_train = features_train.shape\n",
    "            shape_after_test = features_test.shape\n",
    "\n",
    "            print(f\"Train: {shape_before_train[0]} Ã— {shape_before_train[1]}  -->  {shape_after_train[0]} Ã— {shape_after_train[1]}   æ–°å¢ {shape_after_train[1] - shape_before_train[1]} åˆ—\")\n",
    "            print(f\"Test : {shape_before_test[0]} Ã— {shape_before_test[1]}  -->  {shape_after_test[0]} Ã— {shape_after_test[1]}   æ–°å¢ {shape_after_test[1] - shape_before_test[1]} åˆ—\")\n",
    "\n",
    "\n",
    "\n",
    "        X, y, X_test = features_train, target_train, features_test\n",
    "        print(\"å¼€å§‹è®­ç»ƒ---------------------------\")\n",
    "\n",
    "        # å‡†å¤‡æµç¨‹---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "    config = results['config']\n",
    "    score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "    HOSTNAME = socket.gethostname()\n",
    "    HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "    trial.set_user_attr(\"host\", HOSTNAME)        # ä½ è‡ªå·±å®šä¹‰ä¸»æœº A/B\n",
    "    trial.set_user_attr(\"ip\", HOST_IP)        # ä½ è‡ªå·±å®šä¹‰è§’è‰² A/B\n",
    "\n",
    "    # 4. è¿”å›å¹³å‡ LOSS\n",
    "    return score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹ä¼˜åŒ–\n",
    "\n",
    "# 1. å®šä¹‰ SQLite æ•°æ®åº“å­˜å‚¨è·¯å¾„\n",
    "\n",
    "storage_url = f\"mysql+pymysql://{user}:{password}@{host}:3306/{database_name}\"\n",
    "\n",
    "STUDY_NAME = F\"test_{study_save_name}\" if base_config[\"ISTEST\"] else study_save_name\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name = STUDY_NAME,\n",
    "    # study_name=\"ghsdjsrtjrswtjhwrt\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# è‡ªåŠ¨è·å–å½“å‰ä¸»æœºå\\å½“å‰ä¸»æœºçš„ IP åœ°å€\n",
    "HOSTNAME = socket.gethostname()\n",
    "HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "print(\"ä¸»æœºå:\", HOSTNAME,\" ä¸»æœº IP:\", HOST_IP)\n",
    "time.sleep(1)\n",
    "\n",
    "# 5. å¯åŠ¨è¶…å‚æ•°æœç´¢\n",
    "print(\"ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢...\")\n",
    "if base_config[\"ISTEST\"]:\n",
    "    study.optimize(objective, n_trials = 3)\n",
    "else:\n",
    "    study.optimize(objective, n_trials = 200)\n",
    "\n",
    "\n",
    "# 6. æ‰“å°æœ€ä¼˜ç»“æœ\n",
    "print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š å·²å®Œæˆè¯•éªŒæ¬¡æ•° : {len(study.trials)}\")\n",
    "print(f\"ğŸ† æœ€ä¼˜è¯•éªŒç¼–å·   : {study.best_trial.number}\")\n",
    "print(f\"ğŸ“‰ æœ€ä¼˜ LOSS       : {study.best_value}\")\n",
    "print(f\"âš™ï¸ æœ€ä¼˜å‚æ•°ç»„åˆ   : {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c00178",
   "metadata": {},
   "source": [
    "# ç®¡ç†æ•°æ®åº“ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cbfc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®åº“ä¸­çš„ study åˆ—è¡¨:\n",
      "- XGBoost_model\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=0.0616985820   , params={'use_feature_gen': False, 'use_pca': False, 'pca_components': 15, 'selector_threshold': 'mean', 'train_max_depth': 11, 'train_eta': 0.1343165303132418}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=0.0561359815   , params={'use_feature_gen': True, 'use_pca': False, 'pca_components': 5, 'selector_threshold': '0*mean', 'train_max_depth': 6, 'train_eta': 0.022101706430071286}\n",
      "    æ€» trial æ•°é‡: 278\n",
      "====================================================================================================\n",
      "- LightGBM_model\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=0.0562367515   , params={'use_feature_gen': True, 'use_pca': True, 'pca_components': 20, 'learning_rate': 0.05855433126993213, 'num_leaves': 242, 'min_data_in_leaf': 77, 'feature_fraction': 0.8454640426355255, 'bagging_fraction': 0.7469011972885259, 'bagging_freq': 7, 'lambda_l1': 6.693584206135924, 'lambda_l2': 0.2765348113468092, 'boosting_type': 'gbdt'}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=0.0562829993   , params={'use_feature_gen': False, 'use_pca': True, 'pca_components': 10, 'learning_rate': 0.007963988791270001, 'num_leaves': 22, 'min_data_in_leaf': 67, 'feature_fraction': 0.7178025892239908, 'bagging_fraction': 0.7126133392111178, 'bagging_freq': 2, 'lambda_l1': 0.0021554917369354, 'lambda_l2': 0.0027367913297240017, 'boosting_type': 'gbdt'}\n",
      "    æ€» trial æ•°é‡: 10\n",
      "====================================================================================================\n",
      "- test_XGBoost_model\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=0.0756462662   , params={'use_feature_gen': False, 'use_pca': False, 'pca_components': 5, 'selector_threshold': '0*mean', 'train_max_depth': 3, 'train_eta': 0.06842384231485825}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=0.0774058397   , params={'use_feature_gen': True, 'use_pca': False, 'pca_components': 20, 'selector_threshold': '0*mean', 'train_max_depth': 4, 'train_eta': 0.029687276929976067}\n",
      "    æ€» trial æ•°é‡: 9\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢æ•°æ®åº“è¯¦ç»†æ•°æ®\n",
    "\n",
    "\n",
    "storage_url = f\"mysql+pymysql://{user}:{password}@{host}:3306/{database_name}\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "if not studies:\n",
    "    print(\"âŒ å½“å‰æ•°æ®åº“é‡Œæ—  study\")\n",
    "else:\n",
    "    print(\"âœ… æ•°æ®åº“ä¸­çš„ study åˆ—è¡¨:\")\n",
    "    for s in studies:\n",
    "\n",
    "        print(\"-\", s.study_name)\n",
    "\n",
    "        study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "\n",
    "        print(\"         Trials:\")\n",
    "        for trial in study.trials[:2]:  # ä»…æ˜¾ç¤ºå‰ 2 ä¸ª trial\n",
    "            host = trial.user_attrs.get(\"host\") or \"unknown\"\n",
    "            ip = trial.user_attrs.get(\"ip\") or \"unknown\"\n",
    "            value = f\"{trial.value:.10f}\" if trial.value is not None else \"None\"\n",
    "\n",
    "            print(\n",
    "                f\"    Trial {trial.number:4d}: \"\n",
    "                f\"host={host:<16}, ip={ip:<15}, \"\n",
    "                f\"value={value:<15}, params={trial.params}\"\n",
    "            )\n",
    "\n",
    "        print(\"    æ€» trial æ•°é‡:\", len(study.trials))\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac370bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç°æœ‰ studyï¼š ['XGBoost_model', 'LightGBM_model', 'test_XGBoost_model']\n",
      "Study:   XGBoost_model                 , Trials:  278\n",
      "Study:   LightGBM_model                , Trials:   10\n",
      "Study:   test_XGBoost_model            , Trials:    9\n"
     ]
    }
   ],
   "source": [
    "# æ¸…ç†å‰ï¼šå…ˆæŸ¥çœ‹æ•°æ®åº“é‡Œå½“å‰æœ‰å“ªäº› study å­˜åœ¨ï¼Œä»¥åŠæ¯ä¸ª study é‡Œæœ‰å¤šå°‘ä¸ª trial\n",
    "\n",
    "storage_url = f\"mysql+pymysql://{user}:{password}@{host}:3306/{database_name}\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "print(\"ç°æœ‰ studyï¼š\", [s.study_name for s in studies])\n",
    "\n",
    "for s in studies:\n",
    "    study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "    print(f\"Study:   {s.study_name:30s}, Trials: {len(study.trials):4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfd242b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†ä¸­ï¼šåˆ é™¤æŒ‡å®š study\n",
    "# æŒ‡å®šè¦åˆ é™¤çš„åç§°\n",
    "to_delete = [\"melting_point_study\"]   # å¯ä»¥å†™ä¸€ä¸ªæˆ–å¤šä¸ª\n",
    "\n",
    "to_delete = [            ]\n",
    "\n",
    "for s in studies:\n",
    "    if s.study_name in to_delete:\n",
    "        optuna.delete_study(study_name=s.study_name, storage=storage_url)\n",
    "        print(\"å·²åˆ é™¤:\", s.study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72fd5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¸…ç†å studyï¼š ['XGBoost_model', 'LightGBM_model', 'test_XGBoost_model']\n"
     ]
    }
   ],
   "source": [
    "# æ¸…ç†åï¼šå†æ¬¡æ£€æŸ¥\n",
    "studies_after = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "print(\"æ¸…ç†å studyï¼š\", [s.study_name for s in studies_after])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
