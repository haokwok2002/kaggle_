{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系统库\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "# 第三方科学计算 & 可视化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置中文字体，避免乱码\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False          # 解决负号显示成方块的问题\n",
    "\n",
    "# 机器学习 & 优化\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "\n",
    "\n",
    "host = \"10.162.147.95\"\n",
    "user = \"user1\"\n",
    "password = \"123456\"\n",
    "\n",
    "database_name = 'predicting_road_accident_risk'  # 数据库名称\n",
    "competition = database_name  # 竞赛名称\n",
    "kaggle_competition_name = \"playground-series-s5e10\"\n",
    "study_save_name = \"LightGBM_model\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = rf'D:\\数据\\Kaggle_\\{competition}'\n",
    "else:\n",
    "    dir = os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "DIRS = {\n",
    "    \"dir\":              dir,                                       \n",
    "    \"DATA_DIR000\":      os.path.join(dir, \"DATA_DIR000\"),\n",
    "    \"HISTORY\":          os.path.join(dir, \"HISTORY\", f\"{study_save_name}\"),\n",
    "    \"SUBMISSION\":       os.path.join(dir, \"SUBMISSION\", f\"{study_save_name}\"),\n",
    "}\n",
    "\n",
    "# 自动创建目录\n",
    "for key, path in DIRS.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# 打印时一行一个地址\n",
    "print(\"✅ 路径已创建：\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f677d",
   "metadata": {},
   "source": [
    "# 数据提取处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Kaggle 训练集和 Bradley 熔点公开数据集\n",
    "\n",
    "# Kaggle 提供的训练集和测试集\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "# 输出数据集规模，确认加载成功\n",
    "print(\"Train                        shape:\", train_df.shape)\n",
    "print(\"Test                         shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测 DataFrame 每列的数据类型、缺失值情况、唯一值数量，并给出部分示例值。\n",
    "def dataframe_info(df: pd.DataFrame, n_sample: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    检测 DataFrame 每列的数据类型、缺失值情况、唯一值数量，并给出部分示例值。\n",
    "    \n",
    "    参数:\n",
    "        df       : 输入的 pandas DataFrame\n",
    "        n_sample : 每列展示的样本值数量 (默认 3)\n",
    "    \n",
    "    返回:\n",
    "        summary_df : 包含每列信息的 DataFrame\n",
    "    \"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        \"数据类型\": df.dtypes,\n",
    "        \"缺失值数量\": df.isnull().sum(),\n",
    "        \"缺失值比例\": df.isnull().mean(),\n",
    "        \"唯一值数量\": df.nunique()\n",
    "    })\n",
    "    \n",
    "    # 添加示例值（前 n_sample 个唯一值）\n",
    "    summary[\"示例值 (samples)\"] = df.apply(\n",
    "        lambda col: col.dropna().unique()[:n_sample] if col.notnull().any() else []\n",
    "    )\n",
    "    \n",
    "    # 按缺失比例排序\n",
    "    summary = summary.sort_values(\"缺失值比例\", ascending=False)\n",
    "    print(df.columns.values)\n",
    "    display(summary)\n",
    "    # print(summary)\n",
    "\n",
    "\n",
    "# 使用方法\n",
    "dataframe_info(train_df, n_sample = 6)\n",
    "dataframe_info(test_df, n_sample = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88ece8",
   "metadata": {},
   "source": [
    "# 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c871162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印清单\n",
    "def config_to_str(config: dict, indent: int = 0) -> str:\n",
    "    \"\"\"递归生成配置字符串\"\"\"\n",
    "    prefix = \"     \" * indent\n",
    "    lines = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}🔹 {key}:\")\n",
    "            lines.append(config_to_str(value, indent + 1))  # 递归拼接子字典\n",
    "        else:\n",
    "            lines.append(f\"{prefix}- {key:<20}: {value}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f737573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置单\n",
    "config = {\n",
    "    # 固定开关\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"use_feature_gen\"   : True,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 10,\n",
    "\n",
    "\n",
    "\n",
    "    \"study_save_name\"    : study_save_name,\n",
    "\n",
    "\n",
    "    \n",
    "    # 特征选择 XGBoost 参数\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:squarederror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"0*mean\",   \n",
    "\n",
    "    # # 训练设置\n",
    "    # \"xgb_train_model_params\": {\n",
    "    #     'max_depth'   : 6,\n",
    "    #     'eta'         : 0.1,\n",
    "    #     'tree_method' : 'hist',\n",
    "    #     'eval_metric' : 'rmse',\n",
    "    # },\n",
    "    # \"num_boost_round\": 15000,\n",
    "\n",
    "\n",
    "    # 训练设置（LightGBM）\n",
    "    \"lgb_train_model_params\": {\n",
    "        \"objective\": \"regression\",        # 回归任务\n",
    "        \"metric\": \"rmse\",                 # 使用 RMSE 作为评估指标\n",
    "        \"boosting_type\": \"gbdt\",          # 梯度提升树（默认）\n",
    "        \"learning_rate\": 0.05,            # 学习率（原来 XGB 的 eta）\n",
    "        \"num_leaves\": 31,                 # 控制树的复杂度\n",
    "        \"max_depth\": -1,                  # -1 表示不限制深度\n",
    "        \"min_data_in_leaf\": 20,           # 每个叶子最少样本数\n",
    "        \"feature_fraction\": 0.9,          # 每次建树时使用的特征比例\n",
    "        \"bagging_fraction\": 0.8,          # 每次建树时使用的数据比例\n",
    "        \"bagging_freq\": 5,                # 每 5 次迭代进行一次 bagging\n",
    "        \"lambda_l1\": 0.1,                 # L1 正则化\n",
    "        \"lambda_l2\": 0.1,                 # L2 正则化\n",
    "        \"verbosity\": -1                   # 不输出详细日志\n",
    "    },\n",
    "    \"num_boost_round\": 15000,             # 与 XGB 相同，用于控制最大迭代次数\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据拆分 (特征矩阵 与 目标向量)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_features_and_target(train_df: pd.DataFrame, test_df: pd.DataFrame, config: dict):\n",
    "    \"\"\"\n",
    "    数据拆分函数：构造训练集和测试集的特征矩阵与目标向量\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # 2. 构造特征矩阵和目标向量\n",
    "    features_train = train_df.drop(columns=['id', 'accident_risk'])   # 训练集特征 (X)\n",
    "    target_train   = train_df['accident_risk']                            # 训练集目标 (y, 熔点)\n",
    "    features_test  = test_df.drop(columns=['id'])    # 测试集特征 (无 Tm)\n",
    "\n",
    "\n",
    "\n",
    "    # 随机选取部分特征（示例：50 个）\n",
    "    if config[\"ISTEST\"]:\n",
    "\n",
    "        sample_len = 100\n",
    "        features_train = train_df.drop(columns=['id', 'accident_risk']).iloc[:sample_len]  # 训练特征 (前 1000 条)\n",
    "        target_train = train_df.iloc[:sample_len]['accident_risk']               # 训练目标\n",
    "        features_test = test_df.drop(columns=['id'])                # 测试特征 (同样的特征列)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 打印维度信息\n",
    "    print(\"📊 数据拆分完成\")\n",
    "    print(f\"训练集特征 features_train  shape   : {features_train.shape}\")\n",
    "    print(f\"训练集目标   target_train  shape   : {target_train.shape}\")\n",
    "    print(f\"测试集特征  features_test  shape   : {features_test.shape}\")\n",
    "    print(f\"           features_train  类型    : {type(features_train)}\")\n",
    "\n",
    "    return features_train, target_train, features_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9f7ae",
   "metadata": {},
   "source": [
    "### 特征生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32770063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    针对道路事故预测构造新的衍生特征\n",
    "    输入:\n",
    "        df : pd.DataFrame\n",
    "    输出:\n",
    "        df_new : pd.DataFrame，包含新增特征\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    shape_before = df.shape  # 原始维度\n",
    "\n",
    "    # 1. 交互项\n",
    "    df['Speed_x_Curvature'] = df['speed_limit'] * df['curvature']\n",
    "    df['Lanes_x_Speed']     = df['num_lanes'] * df['speed_limit']\n",
    "    df['Accidents_x_Speed'] = df['num_reported_accidents'] * df['speed_limit']\n",
    "\n",
    "    # 类别交互项（需要后续 OneHotEncoder）\n",
    "    df['RoadType_Time']     = df['road_type'] + \"_\" + df['time_of_day']\n",
    "    df['Lighting_Weather']  = df['lighting'] + \"_\" + df['weather']\n",
    "\n",
    "    # 2. 非线性变换\n",
    "    df['Log_Accidents']     = np.log1p(df['num_reported_accidents'])  # log(1+x)\n",
    "\n",
    "    # 分箱（自定义阈值，可调节）\n",
    "    df['Accident_Bins'] = pd.cut(df['num_reported_accidents'],\n",
    "                                bins=[-1, 0, 2, 5, np.inf],\n",
    "                                labels=['none', 'low', 'medium', 'high'])\n",
    "\n",
    "    # 3. 比率特征\n",
    "    df['Accidents_per_Lane']   = df['num_reported_accidents'] / (df['num_lanes'] + 1)\n",
    "    df['Curvature_per_Lane']   = df['curvature'] / (df['num_lanes'] + 1)\n",
    "    df['Speed_per_Lane']       = df['speed_limit'] / (df['num_lanes'] + 1)\n",
    "\n",
    "\n",
    "    shape_after = df.shape  # 新的维度\n",
    "\n",
    "    print(f\"{shape_before[0]} × {shape_before[1]} --> {shape_after[0]} × {shape_after[1]}   新增 {shape_after[1] - shape_before[1]} 列\")\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e546d",
   "metadata": {},
   "source": [
    "### 手动编码 & 布尔转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ae441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def fit_ohe(train_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    在训练集上自动检测 object/category 列，并拟合 OneHotEncoder。\n",
    "    返回 encoder 对象和类别列名。\n",
    "    \"\"\"\n",
    "    # 检测类别列\n",
    "    cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(f\"检测到{len(cat_cols)}类: {cat_cols}\")\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    ohe.fit(train_df[cat_cols])\n",
    "\n",
    "    return ohe, cat_cols\n",
    "\n",
    "\n",
    "def transform_ohe(df: pd.DataFrame, ohe: OneHotEncoder, cat_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    使用已拟合的 OneHotEncoder 对 DataFrame 进行编码。\n",
    "    自动处理 bool -> int，保持 train/test 一致。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    shape_before = df.shape\n",
    "\n",
    "    # 布尔型转 int\n",
    "    for col in df.select_dtypes('bool').columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "    # OHE\n",
    "    ohe_array = ohe.transform(df[cat_cols])\n",
    "    ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
    "    ohe_df = pd.DataFrame(ohe_array, columns=ohe_cols, index=df.index)\n",
    "\n",
    "    # 拼接\n",
    "    df_enc = pd.concat([df.drop(columns=cat_cols), ohe_df], axis=1)\n",
    "\n",
    "    shape_after = df_enc.shape\n",
    "    print(f\"{shape_before[0]} × {shape_before[1]} --> {shape_after[0]} × {shape_after[1]}   新增 {shape_after[1] - shape_before[1]} 列\")\n",
    "\n",
    "    return df_enc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f45be",
   "metadata": {},
   "source": [
    "### 特征生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4582b1",
   "metadata": {},
   "source": [
    "### PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据降维\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "def fit_svd(train_df: pd.DataFrame, n_components: int = 100, random_state: int = 42) -> TruncatedSVD:\n",
    "    \"\"\"\n",
    "    在训练集上拟合 TruncatedSVD，并返回拟合好的模型。\n",
    "    \"\"\"\n",
    "    X_sparse = sparse.csr_matrix(train_df.values)\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "    svd.fit(X_sparse)\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "    print(f\"✅ SVD 已拟合完成 (n_components={n_components})，训练集累计解释方差比: {explained_var:.2%}\")\n",
    "    return svd\n",
    "\n",
    "def transform_svd(df: pd.DataFrame, svd: TruncatedSVD) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    使用拟合好的 SVD 对数据集进行降维。\n",
    "    \"\"\"\n",
    "    shape_before = df.shape\n",
    "    X_sparse = sparse.csr_matrix(df.values)\n",
    "    X_reduced_array = svd.transform(X_sparse)\n",
    "\n",
    "    reduced_df = pd.DataFrame(\n",
    "        X_reduced_array,\n",
    "        index=df.index,\n",
    "        columns=[f\"SVD_{i+1}\" for i in range(X_reduced_array.shape[1])]\n",
    "    )\n",
    "\n",
    "    shape_after = reduced_df.shape\n",
    "    print(f\"{shape_before[0]} × {shape_before[1]} --> {shape_after[0]} × {shape_after[1]}\")\n",
    "    return reduced_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb982f",
   "metadata": {},
   "source": [
    "# 交叉训练验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold + XGBoost 进行训练验证，并保存实验结果\n",
    "# ==============================================================\n",
    "def run_kfold_xgb(features_train, target_train, features_test, config, DIRS, K_FOLDS=10, verbose=0):\n",
    "    \"\"\"\n",
    "    使用 Stratified K-Fold + XGBoost 进行训练验证，并保存实验结果\n",
    "\n",
    "    参数:\n",
    "        features_train, target_train        : 训练集特征和标签\n",
    "        features_test      : 测试集特征\n",
    "        params      : XGBoost 最优参数 (dict)\n",
    "        DIRS        : 保存结果的目录字典\n",
    "        K_FOLDS     : 折数 (默认=5)\n",
    "        verbose     : 是否打印详细信息\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "    config[\"X shape\"] = features_train.shape\n",
    "    config[\"y shape\"] = target_train.shape\n",
    "    config[\"X_test shape\"] = features_test.shape\n",
    "\n",
    "\n",
    "    # ---------- 创建目录 ----------\n",
    "    for _, path in DIRS.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = os.path.join(DIRS['HISTORY'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"——\" * 20)\n",
    "    print(f\"✅ 当前结果将保存到: {time_str}\")\n",
    "\n",
    "\n",
    "    # ---------- 定义交叉验证 ----------\n",
    "    skfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    yeo = PowerTransformer(method=\"yeo-johnson\")                                # 定义 Yeo-Johnson 变换\n",
    "\n",
    "    # ---------- 初始化存储 ----------\n",
    "    oof_val = np.zeros(len(features_train))       # OOF 预测\n",
    "    train_score, val_score = [], []  # 每折 LOSS\n",
    "    test_pred = []                   # 每折 test 预测\n",
    "    fold_records = []                # 保存每折信息\n",
    "    all_importances = []             # 特征重要性\n",
    "    elapsed_list = []                # 耗时记录\n",
    "\n",
    "\n",
    "\n",
    "    # 循环每一折\n",
    "    # ==============================================================\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skfold.split(features_train, pd.qcut(target_train, q=10).cat.codes), 1):\n",
    "\n",
    "        # ----- 打印时间信息 -----\n",
    "        start_now = datetime.now()\n",
    "        start_str = start_now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if elapsed_list:\n",
    "            avg_time = np.mean(elapsed_list)\n",
    "            est_end = start_now + timedelta(seconds=avg_time)\n",
    "\n",
    "            # 每 5 个一组输出耗时\n",
    "            parts = [f\"{t:6.1f}s\" for t in elapsed_list]\n",
    "            grouped = [\" \".join(parts[j:j+5]) for j in range(0, len(parts), 5)]\n",
    "            elapsed_str = \" /// \".join(grouped)\n",
    "\n",
    "            print(\n",
    "                f\"🔄{i:2d}/{K_FOLDS} ST {start_str}\"\n",
    "                f\" ET {est_end.strftime('%H:%M:%S')}\"\n",
    "                f\" avg {avg_time:.1f}s\"\n",
    "                f\" [{elapsed_str}]\",\n",
    "                end=\"\\r\", flush=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"🔄{i:2d}/{K_FOLDS} ST {start_str} ET (暂无历史数据)\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "        # ----- 开始训练 -----\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. 数据集划分\n",
    "        x_train, x_val = features_train.iloc[train_idx], features_train.iloc[val_idx]\n",
    "        y_train, y_val = target_train[train_idx], target_train[val_idx]\n",
    "\n",
    "        # 2. Yeo-Johnson 变换\n",
    "        y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "        y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "\n",
    "        # 3. 特征选择（轻量级 XGBoost）\n",
    "        # 使用\n",
    "        selector_model = xgb.XGBRegressor(**config[\"xgb_selector_model_params\"])\n",
    "        # selector_model = xgb.XGBRegressor(\n",
    "        #     n_estimators   = 500,\n",
    "        #     max_depth      = 6,\n",
    "        #     learning_rate  = 0.05,\n",
    "        #     random_state   = 2025,\n",
    "        #     device         = \"cpu\",\n",
    "        #     objective      = \"reg:absoluteerror\",\n",
    "        #     tree_method    = \"hist\",\n",
    "        #     verbosity      = 0\n",
    "        # )\n",
    "        \n",
    "        \n",
    "\n",
    "        selector_model.fit(x_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(selector_model, prefit=True, threshold=config[\"selector_threshold\"])\n",
    "        selected_features = x_train.columns[selector.get_support()].tolist()\n",
    "        if verbose > 0:\n",
    "            print(f\"✅ 选择的特征数量: {len(selected_features)}\")\n",
    "\n",
    "\n",
    "        # 4. 保留重要特征\n",
    "        x_train_new = x_train[selected_features]\n",
    "        x_val_new   = x_val[selected_features]\n",
    "        x_test_new  = features_test[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 5. 构造 LightGBM 数据集\n",
    "        dtrain = lgb.Dataset(x_train_new, label=y_train)\n",
    "        dval   = lgb.Dataset(x_val_new,   label=y_val)\n",
    "        # dtest  = lgb.Dataset(x_test_new,  label=y_test)\n",
    "\n",
    "        early_stop_rounds = 800 if config[\"lgb_train_model_params\"][\"learning_rate\"] < 0.03 else 300\n",
    "        use_early_stopping = (config[\"lgb_train_model_params\"][\"boosting_type\"] != \"dart\")\n",
    "\n",
    "        callbacks = []\n",
    "        if use_early_stopping:\n",
    "            callbacks.append(lgb.early_stopping(stopping_rounds=300))\n",
    "        callbacks.append(lgb.log_evaluation(period=1000))\n",
    "\n",
    "        # 6. LightGBM 训练\n",
    "        lgb_model = lgb.train(\n",
    "            params=config[\"lgb_train_model_params\"],\n",
    "            train_set=dtrain,\n",
    "            valid_sets=[dtrain, dval],\n",
    "            valid_names=[\"train\", \"valid\"],\n",
    "            num_boost_round=config[\"num_boost_round\"],\n",
    "        callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # # 保存模型\n",
    "        # model_path = os.path.join(history_DIR, f\"lgb_model_fold{i}.txt\")\n",
    "        # lgb_model.save_model(model_path)\n",
    "\n",
    "\n",
    "\n",
    "        # 7. 获取特征重要性\n",
    "        # imp_dict = xgb_model.get_score(importance_type=\"gain\")\n",
    "        # imp_df = pd.DataFrame(imp_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "        # imp_df[\"Fold\"] = i\n",
    "        # all_importances.append(imp_df)\n",
    "\n",
    "\n",
    "        # 8. 预测\n",
    "        y_train_pred = lgb_model.predict(x_train_new, num_iteration=lgb_model.best_iteration)\n",
    "        y_val_pred   = lgb_model.predict(x_val_new,   num_iteration=lgb_model.best_iteration)\n",
    "        y_test_pred  = lgb_model.predict(x_test_new,  num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 9. 逆变换\n",
    "        y_train      = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "        y_val        = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "        y_train_pred = yeo.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
    "        y_val_pred   = yeo.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
    "        y_test_pred  = yeo.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        # 10. 计算 LOSS\n",
    "        train_loss = np.sqrt(np.mean((y_train - y_train_pred) ** 2))\n",
    "        val_loss   = np.sqrt(np.mean((y_val   - y_val_pred) ** 2))\n",
    "\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Fold {i}: Train LOSS={train_loss:.4f}, Val LOSS={val_loss:.4f}，用时 {elapsed:.2f} 秒\")\n",
    "\n",
    "\n",
    "        # ----- 保存结果 -----\n",
    "        train_score.append(train_loss)\n",
    "        val_score.append(val_loss)\n",
    "        oof_val[val_idx] = y_val_pred\n",
    "        test_pred.append(y_test_pred)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        elapsed_list.append(elapsed)\n",
    "\n",
    "        fold_records.append({\n",
    "            \"Fold\": i,\n",
    "            \"Train_LOSS\": train_loss,\n",
    "            \"Val_LOSS\": val_loss,\n",
    "            \"Num_Features\": len(selected_features),\n",
    "            \"Selected_Features\": selected_features,\n",
    "            \"elapsed\": elapsed\n",
    "        })\n",
    "\n",
    "    # 保存整体结果\n",
    "    # ==============================================================\n",
    "    if verbose > 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"📊 Train LOSS 平均值 : {np.mean(train_score):.4f}\")\n",
    "        print(f\"📊 Val   LOSS 平均值 : {np.mean(val_score):.4f}\")\n",
    "        print(f\"📊 Train LOSS 标准差 : {np.std(train_score, ddof=0):.4f}\")\n",
    "        print(f\"📊 Val   LOSS 标准差 : {np.std(val_score, ddof=0):.4f}\")\n",
    "\n",
    "    # 参数\n",
    "    with open(os.path.join(history_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # 每折信息\n",
    "    folds_df = pd.DataFrame(fold_records)\n",
    "    folds_df.to_csv(os.path.join(history_DIR, \"folds_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # # 特征重要性\n",
    "    # if all_importances:\n",
    "    #     valid_imps = [df for df in all_importances if not df.empty]\n",
    "    #     all_imp_df = pd.concat(valid_imps, axis=0) if valid_imps else pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    # else:\n",
    "    #     all_imp_df = pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    # all_imp_df.to_csv(os.path.join(history_DIR, \"feature_importance_all.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # 测试集预测\n",
    "    test_pred_array = np.vstack(test_pred).T\n",
    "    test_pred_df = pd.DataFrame(test_pred_array, columns=[f\"Fold_{j+1}\" for j in range(test_pred_array.shape[1])])\n",
    "    test_pred_df[\"Final_Pred\"] = test_pred_df.mean(axis=1)\n",
    "    test_pred_df.to_csv(os.path.join(history_DIR, \"test_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 总结\n",
    "    with open(os.path.join(history_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train LOSS Mean : {np.mean(train_score):.4f}\\n\")\n",
    "        f.write(f\"Val   LOSS Mean : {np.mean(val_score):.4f}\\n\")\n",
    "        f.write(f\"Train LOSS Std  : {np.std(train_score, ddof=0):.4f}\\n\")\n",
    "        f.write(f\"Val   LOSS Std  : {np.std(val_score, ddof=0):.4f}\\n\")\n",
    "\n",
    "\n",
    "    # 最终提交\n",
    "    final_score = np.mean(val_score)\n",
    "    submission = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"sample_submission.csv\"))\n",
    "    submission[\"accident_risk\"] = test_pred_df[\"Final_Pred\"]\n",
    "\n",
    "    submission_path = os.path.join(history_DIR, f\"sub_{time_str}_{final_score:.8f}.csv\")\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    submission.to_csv(os.path.join(DIRS['SUBMISSION'], f\"sub_{time_str}_{final_score:.8f}.csv\"), index=False)\n",
    "\n",
    "        \n",
    "    config[\"time_str\"] = time_str\n",
    "    config[\"score\"] = final_score\n",
    "\n",
    "\n",
    "    # ---------- 返回结果 ----------\n",
    "    return {\n",
    "        \"oof_val\": oof_val,\n",
    "        \"train_score\": train_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_pred\": test_pred_df,\n",
    "        \"folds_info\": folds_df,\n",
    "        # \"feature_importance\": all_imp_df,\n",
    "        \"submission_path\": submission_path,\n",
    "        \"time\": time_str,\n",
    "        \"final_score\": final_score,\n",
    "        \"config\": config\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f752",
   "metadata": {},
   "source": [
    "# 单次训练推导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行一次\n",
    "\n",
    "# 准备流程---------------------------------------------------------------------------------------------------\n",
    "# 打印当前config\n",
    "print(config_to_str(config))\n",
    "\n",
    "# Kaggle 提供的训练集和测试集\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "# 数据拆分\n",
    "print(\"数据拆分---------------------------\")\n",
    "features_train, target_train, features_test = prepare_features_and_target(train_df, test_df, config)\n",
    "\n",
    "\n",
    "dataframe_info(features_train, n_sample = 6)\n",
    "\n",
    "\n",
    "# 特征生成\n",
    "print(\"特征生成---------------------------\")\n",
    "if config[\"use_feature_gen\"]:\n",
    "    features_train = add_new_features(features_train)\n",
    "    features_test  = add_new_features(features_test)\n",
    "\n",
    "\n",
    "dataframe_info(features_train, n_sample = 6)\n",
    "\n",
    "\n",
    "# 手动编码 & 布尔转换\n",
    "print(\"手动编码 & 布尔转换----------------\")\n",
    "# 在训练集上拟合\n",
    "ohe, cat_cols = fit_ohe(features_train)\n",
    "# 分别编码 train/test，保证一致\n",
    "features_train = transform_ohe(features_train, ohe, cat_cols)\n",
    "features_test  = transform_ohe(features_test,  ohe, cat_cols)\n",
    "\n",
    "\n",
    "# 数据降维\n",
    "print(\"数据降维---------------------------\")\n",
    "if config[\"use_pca\"]:\n",
    "    # 1. 在训练集上拟合\n",
    "    svd = fit_svd(features_train, n_components = config[\"pca_components\"], random_state=42)\n",
    "\n",
    "    # 2. 分别对 train/test transform\n",
    "    features_train_reduced = transform_svd(features_train, svd)\n",
    "    features_test_reduced  = transform_svd(features_test, svd)\n",
    "\n",
    "    shape_before_train = features_train.shape\n",
    "    shape_before_test = features_test.shape\n",
    "\n",
    "    features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "    features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "    shape_after_train = features_train.shape\n",
    "    shape_after_test = features_test.shape\n",
    "\n",
    "    print(f\"Train: {shape_before_train[0]} × {shape_before_train[1]}  -->  {shape_after_train[0]} × {shape_after_train[1]}   新增 {shape_after_train[1] - shape_before_train[1]} 列\")\n",
    "    print(f\"Test : {shape_before_test[0]} × {shape_before_test[1]}  -->  {shape_after_test[0]} × {shape_after_test[1]}   新增 {shape_after_test[1] - shape_before_test[1]} 列\")\n",
    "\n",
    "\n",
    "\n",
    "X, y, X_test = features_train, target_train, features_test\n",
    "print(\"开始训练---------------------------\")\n",
    "\n",
    "# 准备流程---------------------------------------------------------------------------------------------------\n",
    "\n",
    "results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "config = results['config']\n",
    "score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "print('\\n',score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3573b32",
   "metadata": {},
   "source": [
    "# 提交 kaggle 平台测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据 submission_time 定位文件路径 提交 kaggle 平台测试\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def find_submission_file(submission_time, submission_dir):\n",
    "    \"\"\"\n",
    "    在 submission_dir 下查找包含 submission_time 的文件\n",
    "    一旦找到立刻返回完整路径；如果没找到则返回 None\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(submission_dir):\n",
    "        if submission_time in fname:\n",
    "            file_path = os.path.join(submission_dir, fname)\n",
    "            print(f\"✅ 找到目标文件: {fname}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(f\"⚠️ 未找到包含 {submission_time} 的文件\")\n",
    "    return None\n",
    "\n",
    "def submit_and_get_score(file_path, competition_name, message=\"My submission\"):\n",
    "    \"\"\"\n",
    "    封装 Kaggle 提交并等待结果评分\n",
    "    --------------------------------------\n",
    "    file_path        : str  提交文件路径\n",
    "    competition_name : str  Kaggle 比赛名称 (URL 最后一段)\n",
    "    message          : str  提交备注\n",
    "    \"\"\"\n",
    "    # 1. 配置 Kaggle API\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = r\"C:\\Users\\Admin\\.kaggle\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"✅ Kaggle API 已经配置成功！\")\n",
    "\n",
    "    # 2. 提交文件\n",
    "    api.competition_submit(\n",
    "        file_name=file_path,\n",
    "        competition=competition_name,\n",
    "        message=message\n",
    "    )\n",
    "    print(\"✅ 提交完成！请等待评分...\")\n",
    "\n",
    "    # 3. 动态等待\n",
    "    spinner = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
    "    while True:\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        latest = submissions[0]\n",
    "        status_str = str(latest._status).lower()\n",
    "\n",
    "        if \"complete\" in status_str and latest._public_score is not None:\n",
    "            print(\"\\n🎯 最终结果:\")\n",
    "            print(f\"Public 分数 : {latest._public_score}\")\n",
    "            print(f\"Private 分数: {latest._private_score}\")\n",
    "            print(f\"提交 ID     : {latest._ref}\")\n",
    "            print(f\"文件名      : {latest._file_name}\")\n",
    "            print(f\"状态        : {latest._status}\")\n",
    "            print(f\"提交时间    : {latest._date}\")\n",
    "            print(f\"描述/备注   : {latest._description}\")\n",
    "            return latest\n",
    "\n",
    "        if \"pending\" in status_str:\n",
    "            spin_char = next(spinner)\n",
    "            print(f\"当前状态: {status_str} , 等待中 {spin_char}\", end=\"\\r\", flush=True)\n",
    "            time.sleep(0.2)  # 每 0.5 秒检查一次\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n报错\")\n",
    "            print(f\"submissions\")\n",
    "            \n",
    "            break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb1c16",
   "metadata": {},
   "source": [
    "### 不轻易运行，再三考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_time 提交\n",
    "submission_time = \"2025-10-24 01-23-41\"\n",
    "competition_name = kaggle_competition_name\n",
    "message =  f\"该提交文件的参数：\\n{config_to_str(config)} \"\n",
    "\n",
    "target_file = find_submission_file(submission_time, DIRS['SUBMISSION'] )\n",
    "\n",
    "# submit_and_get_score(target_file, competition_name, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9607e",
   "metadata": {},
   "source": [
    "# 参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置单\n",
    "base_config = {\n",
    "    # 固定开关\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 10,\n",
    "\n",
    "\n",
    "\n",
    "    \"study_save_name\"    : study_save_name,\n",
    "\n",
    "    \n",
    "    # 特征选择 XGBoost 参数\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:squarederror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"0*mean\",   \n",
    "\n",
    "    # # 训练设置\n",
    "    # \"xgb_train_model_params\": {\n",
    "    #     'max_depth'   : 6,\n",
    "    #     'eta'         : 0.1,\n",
    "    #     'tree_method' : 'hist',\n",
    "    #     'eval_metric' : 'rmse',\n",
    "    # },\n",
    "    # \"num_boost_round\": 15000,\n",
    "\n",
    "\n",
    "    # 训练设置（LightGBM）\n",
    "    \"lgb_train_model_params\": {\n",
    "        \"objective\": \"regression\",        # 回归任务\n",
    "        \"metric\": \"rmse\",                 # 使用 RMSE 作为评估指标\n",
    "        \"boosting_type\": \"gbdt\",          # 梯度提升树（默认）\n",
    "        \"learning_rate\": 0.05,            # 学习率（原来 XGB 的 eta）\n",
    "        \"num_leaves\": 31,                 # 控制树的复杂度\n",
    "        \"max_depth\": -1,                  # -1 表示不限制深度\n",
    "        \"min_data_in_leaf\": 20,           # 每个叶子最少样本数\n",
    "        \"feature_fraction\": 0.9,          # 每次建树时使用的特征比例\n",
    "        \"bagging_fraction\": 0.8,          # 每次建树时使用的数据比例\n",
    "        \"bagging_freq\": 5,                # 每 5 次迭代进行一次 bagging\n",
    "        \"lambda_l1\": 0.1,                 # L1 正则化\n",
    "        \"lambda_l2\": 0.1,                 # L2 正则化\n",
    "        \"verbosity\": -1                   # 不输出详细日志\n",
    "    },\n",
    "    \"num_boost_round\": 15000,             # 与 XGB 相同，用于控制最大迭代次数\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化任务  加入标识符 host: hao-2   ip: 192.168.40.1\n",
    "\n",
    "import copy\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna 的目标函数 (Objective Function)\n",
    "    每次 trial 会生成一组超参数，用于训练 XGBoost 模型，\n",
    "    并返回交叉验证的平均 RMSE 作为优化目标。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. 定义 超参数 搜索空间\n",
    "    # 拷贝一份 config，避免全局污染\n",
    "    config = copy.deepcopy(base_config)\n",
    "\n",
    "    # 只修改需要优化的参数\n",
    "    config[\"use_feature_gen\"]   = trial.suggest_categorical(\"use_feature_gen\", [True, False])\n",
    "    config[\"use_pca\"]           = trial.suggest_categorical(\"use_pca\", [True, False])\n",
    "    config[\"pca_components\"] = trial.suggest_categorical(\"pca_components\", [5, 10, 15, 20])\n",
    "\n",
    "    # config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_categorical(\"selector_random_state\", [2025])\n",
    "    # config[\"xgb_selector_model_params\"][\"device\"]       = trial.suggest_categorical(\"selector_device\", [\"cpu\", \"cuda\"])\n",
    "\n",
    "    # config[\"selector_threshold\"] = trial.suggest_categorical(\"selector_threshold\", [\"0*mean\", \"0.25*mean\", \"0.5*mean\", \"0.75*mean\", \"mean\"])\n",
    "\n",
    "    # config[\"xgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\"train_max_depth\", 3, 12)\n",
    "    # config[\"xgb_train_model_params\"][\"eta\"] = trial.suggest_float(\"train_eta\", 0.01 , 0.3 , log=True)\n",
    "    config[\"lgb_train_model_params\"][\"learning_rate\"] = trial.suggest_float(\n",
    "        \"learning_rate\", 0.005, 0.1, log=True\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"num_leaves\"] = trial.suggest_int(\n",
    "        \"num_leaves\", 16, 256\n",
    "    )\n",
    "    # config[\"lgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\n",
    "    #     \"max_depth\", -1, 15\n",
    "    # )\n",
    "    config[\"lgb_train_model_params\"][\"min_data_in_leaf\"] = trial.suggest_int(\n",
    "        \"min_data_in_leaf\", 10, 100\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"feature_fraction\"] = trial.suggest_float(\n",
    "        \"feature_fraction\", 0.6, 1.0\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"bagging_fraction\"] = trial.suggest_float(\n",
    "        \"bagging_fraction\", 0.6, 1.0\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"bagging_freq\"] = trial.suggest_int(\n",
    "        \"bagging_freq\", 1, 10\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"lambda_l1\"] = trial.suggest_float(\n",
    "        \"lambda_l1\", 1e-3, 10.0, log=True\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"lambda_l2\"] = trial.suggest_float(\n",
    "        \"lambda_l2\", 1e-3, 10.0, log=True\n",
    "    )\n",
    "    config[\"lgb_train_model_params\"][\"boosting_type\"] = trial.suggest_categorical(\n",
    "        \"boosting_type\", [\"gbdt\", \"dart\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 创建一个黑洞缓冲区\n",
    "    f = io.StringIO()\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 准备流程---------------------------------------------------------------------------------------------------\n",
    "        # 打印当前config\n",
    "        print(config_to_str(config))\n",
    "\n",
    "        # Kaggle 提供的训练集和测试集\n",
    "        train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "        test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "        # 数据拆分\n",
    "        print(\"数据拆分---------------------------\")\n",
    "        features_train, target_train, features_test = prepare_features_and_target(train_df, test_df, config)\n",
    "\n",
    "        # 特征生成\n",
    "        print(\"特征生成---------------------------\")\n",
    "        if config[\"use_feature_gen\"]:\n",
    "            features_train = add_new_features(features_train)\n",
    "            features_test  = add_new_features(features_test)\n",
    "\n",
    "\n",
    "        # 手动编码 & 布尔转换\n",
    "        print(\"手动编码 & 布尔转换----------------\")\n",
    "        # 在训练集上拟合\n",
    "        ohe, cat_cols = fit_ohe(features_train)\n",
    "        # 分别编码 train/test，保证一致\n",
    "        features_train = transform_ohe(features_train, ohe, cat_cols)\n",
    "        features_test  = transform_ohe(features_test,  ohe, cat_cols)\n",
    "\n",
    "\n",
    "        # 数据降维\n",
    "        print(\"数据降维---------------------------\")\n",
    "        if config[\"use_pca\"]:\n",
    "            # 1. 在训练集上拟合\n",
    "            svd = fit_svd(features_train, n_components = config[\"pca_components\"], random_state=42)\n",
    "\n",
    "            # 2. 分别对 train/test transform\n",
    "            features_train_reduced = transform_svd(features_train, svd)\n",
    "            features_test_reduced  = transform_svd(features_test, svd)\n",
    "\n",
    "            shape_before_train = features_train.shape\n",
    "            shape_before_test = features_test.shape\n",
    "\n",
    "            features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "            features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "            shape_after_train = features_train.shape\n",
    "            shape_after_test = features_test.shape\n",
    "\n",
    "            print(f\"Train: {shape_before_train[0]} × {shape_before_train[1]}  -->  {shape_after_train[0]} × {shape_after_train[1]}   新增 {shape_after_train[1] - shape_before_train[1]} 列\")\n",
    "            print(f\"Test : {shape_before_test[0]} × {shape_before_test[1]}  -->  {shape_after_test[0]} × {shape_after_test[1]}   新增 {shape_after_test[1] - shape_before_test[1]} 列\")\n",
    "\n",
    "\n",
    "\n",
    "        X, y, X_test = features_train, target_train, features_test\n",
    "        print(\"开始训练---------------------------\")\n",
    "\n",
    "        # 准备流程---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "    config = results['config']\n",
    "    score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "    HOSTNAME = socket.gethostname()\n",
    "    HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "    trial.set_user_attr(\"host\", HOSTNAME)        # 你自己定义主机 A/B\n",
    "    trial.set_user_attr(\"ip\", HOST_IP)        # 你自己定义角色 A/B\n",
    "\n",
    "    # 4. 返回平均 LOSS\n",
    "    return score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始优化\n",
    "\n",
    "# 1. 定义 SQLite 数据库存储路径\n",
    "\n",
    "storage_url = f\"mysql+pymysql://{user}:{password}@{host}:3306/{database_name}\"\n",
    "\n",
    "STUDY_NAME = F\"test_{study_save_name}\" if base_config[\"ISTEST\"] else study_save_name\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name = STUDY_NAME,\n",
    "    # study_name=\"ghsdjsrtjrswtjhwrt\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# 自动获取当前主机名\\当前主机的 IP 地址\n",
    "HOSTNAME = socket.gethostname()\n",
    "HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "print(\"主机名:\", HOSTNAME,\" 主机 IP:\", HOST_IP)\n",
    "time.sleep(1)\n",
    "\n",
    "# 5. 启动超参数搜索\n",
    "print(\"🔎 开始超参数搜索...\")\n",
    "if base_config[\"ISTEST\"]:\n",
    "    study.optimize(objective, n_trials = 3)\n",
    "else:\n",
    "    study.optimize(objective, n_trials = 200)\n",
    "\n",
    "\n",
    "# 6. 打印最优结果\n",
    "print(\"\\n✅ 训练完成！\")\n",
    "print(f\"📊 已完成试验次数 : {len(study.trials)}\")\n",
    "print(f\"🏆 最优试验编号   : {study.best_trial.number}\")\n",
    "print(f\"📉 最优 LOSS       : {study.best_value}\")\n",
    "print(f\"⚙️ 最优参数组合   : {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c00178",
   "metadata": {},
   "source": [
    "# 管理数据库信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cbfc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据库中的 study 列表:\n",
      "- XGBoost_model\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=0.0616985820   , params={'use_feature_gen': False, 'use_pca': False, 'pca_components': 15, 'selector_threshold': 'mean', 'train_max_depth': 11, 'train_eta': 0.1343165303132418}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=0.0561359815   , params={'use_feature_gen': True, 'use_pca': False, 'pca_components': 5, 'selector_threshold': '0*mean', 'train_max_depth': 6, 'train_eta': 0.022101706430071286}\n",
      "    总 trial 数量: 278\n",
      "====================================================================================================\n",
      "- LightGBM_model\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=0.0562367515   , params={'use_feature_gen': True, 'use_pca': True, 'pca_components': 20, 'learning_rate': 0.05855433126993213, 'num_leaves': 242, 'min_data_in_leaf': 77, 'feature_fraction': 0.8454640426355255, 'bagging_fraction': 0.7469011972885259, 'bagging_freq': 7, 'lambda_l1': 6.693584206135924, 'lambda_l2': 0.2765348113468092, 'boosting_type': 'gbdt'}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=0.0562829993   , params={'use_feature_gen': False, 'use_pca': True, 'pca_components': 10, 'learning_rate': 0.007963988791270001, 'num_leaves': 22, 'min_data_in_leaf': 67, 'feature_fraction': 0.7178025892239908, 'bagging_fraction': 0.7126133392111178, 'bagging_freq': 2, 'lambda_l1': 0.0021554917369354, 'lambda_l2': 0.0027367913297240017, 'boosting_type': 'gbdt'}\n",
      "    总 trial 数量: 10\n",
      "====================================================================================================\n",
      "- test_XGBoost_model\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=0.0756462662   , params={'use_feature_gen': False, 'use_pca': False, 'pca_components': 5, 'selector_threshold': '0*mean', 'train_max_depth': 3, 'train_eta': 0.06842384231485825}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=0.0774058397   , params={'use_feature_gen': True, 'use_pca': False, 'pca_components': 20, 'selector_threshold': '0*mean', 'train_max_depth': 4, 'train_eta': 0.029687276929976067}\n",
      "    总 trial 数量: 9\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 查询数据库详细数据\n",
    "\n",
    "\n",
    "storage_url = f\"mysql+pymysql://{user}:{password}@{host}:3306/{database_name}\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "if not studies:\n",
    "    print(\"❌ 当前数据库里无 study\")\n",
    "else:\n",
    "    print(\"✅ 数据库中的 study 列表:\")\n",
    "    for s in studies:\n",
    "\n",
    "        print(\"-\", s.study_name)\n",
    "\n",
    "        study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "\n",
    "        print(\"         Trials:\")\n",
    "        for trial in study.trials[:2]:  # 仅显示前 2 个 trial\n",
    "            host = trial.user_attrs.get(\"host\") or \"unknown\"\n",
    "            ip = trial.user_attrs.get(\"ip\") or \"unknown\"\n",
    "            value = f\"{trial.value:.10f}\" if trial.value is not None else \"None\"\n",
    "\n",
    "            print(\n",
    "                f\"    Trial {trial.number:4d}: \"\n",
    "                f\"host={host:<16}, ip={ip:<15}, \"\n",
    "                f\"value={value:<15}, params={trial.params}\"\n",
    "            )\n",
    "\n",
    "        print(\"    总 trial 数量:\", len(study.trials))\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac370bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现有 study： ['XGBoost_model', 'LightGBM_model', 'test_XGBoost_model']\n",
      "Study:   XGBoost_model                 , Trials:  278\n",
      "Study:   LightGBM_model                , Trials:   10\n",
      "Study:   test_XGBoost_model            , Trials:    9\n"
     ]
    }
   ],
   "source": [
    "# 清理前：先查看数据库里当前有哪些 study 存在，以及每个 study 里有多少个 trial\n",
    "\n",
    "storage_url = f\"mysql+pymysql://{user}:{password}@{host}:3306/{database_name}\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "print(\"现有 study：\", [s.study_name for s in studies])\n",
    "\n",
    "for s in studies:\n",
    "    study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "    print(f\"Study:   {s.study_name:30s}, Trials: {len(study.trials):4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfd242b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理中：删除指定 study\n",
    "# 指定要删除的名称\n",
    "to_delete = [\"melting_point_study\"]   # 可以写一个或多个\n",
    "\n",
    "to_delete = [            ]\n",
    "\n",
    "for s in studies:\n",
    "    if s.study_name in to_delete:\n",
    "        optuna.delete_study(study_name=s.study_name, storage=storage_url)\n",
    "        print(\"已删除:\", s.study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72fd5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理后 study： ['XGBoost_model', 'LightGBM_model', 'test_XGBoost_model']\n"
     ]
    }
   ],
   "source": [
    "# 清理后：再次检查\n",
    "studies_after = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "print(\"清理后 study：\", [s.study_name for s in studies_after])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
