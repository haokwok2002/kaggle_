{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-30T08:07:14.763074Z",
     "iopub.status.busy": "2025-10-30T08:07:14.762816Z",
     "iopub.status.idle": "2025-10-30T08:07:21.990822Z",
     "shell.execute_reply": "2025-10-30T08:07:21.990102Z",
     "shell.execute_reply.started": "2025-10-30T08:07:14.763052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\conda\\envs\\kaggle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‰ΩøÁî®ËÆæÂ§á: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# üì¶ ÂØºÂÖ•‰æùËµñ\n",
    "# =========================================\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# ‚öôÔ∏è 0Ô∏è‚É£ ÂÖ®Â±ÄÂèÇÊï∞ÈÖçÁΩÆ\n",
    "# =========================================\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models import get_model_weights\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ ‰ΩøÁî®ËÆæÂ§á: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:07:21.992327Z",
     "iopub.status.busy": "2025-10-30T08:07:21.991922Z",
     "iopub.status.idle": "2025-10-30T08:07:21.998124Z",
     "shell.execute_reply": "2025-10-30T08:07:21.997471Z",
     "shell.execute_reply.started": "2025-10-30T08:07:21.992307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ë∑ØÂæÑÂ∑≤ÂàõÂª∫Ôºö\n",
      "\n",
      "dir          : D:\\DATA_hao\\Kaggle_\\csiro-biomass\n",
      "train        : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\train\n",
      "test         : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\test\n",
      "model        : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\DualStream_multihead\n",
      "data         : D:\\DATA_hao\\Kaggle_\\csiro-biomass\n"
     ]
    }
   ],
   "source": [
    "# ÂàùÂßãÂåñ\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = Path('D:/DATA_hao/Kaggle_/csiro-biomass/')\n",
    "    DIRS = {\n",
    "    \"dir\":        dir,                                       \n",
    "    \"train\":     Path(dir, \"train\"),                              \n",
    "    \"test\":     Path(dir, \"test\"),                              \n",
    "    \"model\":     Path(dir,\"DualStream_multihead\"),              \n",
    "    \"data\":     Path(dir),   \n",
    "    }\n",
    "else:\n",
    "    dir = Path('/kaggle/input/csiro-biomass')\n",
    "    DIRS = {\n",
    "    \"dir\":        dir,                                       \n",
    "    \"train\":     Path(dir, \"train\"),                              \n",
    "    \"test\":     Path(dir, \"test\"),                              \n",
    "    \"model\":     Path('/kaggle/input', \"dualstream-multihead-model\"),              \n",
    "    \"data\":     Path(\"/kaggle/working/\"),   \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Ëá™Âä®ÂàõÂª∫ÁõÆÂΩï\n",
    "# for key, path in DIRS.items():\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# ÊâìÂç∞Êó∂‰∏ÄË°å‰∏Ä‰∏™Âú∞ÂùÄ\n",
    "print(\"‚úÖ Ë∑ØÂæÑÂ∑≤ÂàõÂª∫Ôºö\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:07:21.999218Z",
     "iopub.status.busy": "2025-10-30T08:07:21.998874Z",
     "iopub.status.idle": "2025-10-30T08:07:22.506492Z",
     "shell.execute_reply": "2025-10-30T08:07:22.505864Z",
     "shell.execute_reply.started": "2025-10-30T08:07:21.999195Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Â∞èÂáΩÊï∞\n",
    "def show_df_info(df, name: str):\n",
    "    \"\"\"\n",
    "    ÊâìÂç∞Âçï‰∏™ DataFrame ÁöÑÂΩ¢Áä∂‰∏éÂàóÂêç‰ø°ÊÅØ„ÄÇ\n",
    "    ÂèÇÊï∞:\n",
    "        df   : pandas.DataFrame\n",
    "        name : ÊòæÁ§∫ÂêçÁß∞ÔºàÂ≠óÁ¨¶‰∏≤Ôºâ\n",
    "    \"\"\"\n",
    "    print(f\"üìä {name:<16} shape: {str(df.shape):<16}  ÂàóÂêç: {df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "\n",
    "def move_column_first(df, col_name):\n",
    "    \"\"\"\n",
    "    Â∞Ü DataFrame ‰∏≠ÊåáÂÆöÂàóÁßªÂä®Âà∞ÊúÄÂâçÈù¢„ÄÇ\n",
    "    ÂèÇÊï∞:\n",
    "        df (pd.DataFrame): ÂéüÂßãÊï∞ÊçÆÊ°Ü\n",
    "        col_name (str): Ë¶ÅÁßªÂä®Âà∞ÊúÄÂâçÈù¢ÁöÑÂàóÂêç\n",
    "    ËøîÂõû:\n",
    "        pd.DataFrame: Ë∞ÉÊï¥ÂêéÁöÑÊñ∞ DataFrame\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Âàó '{col_name}' ‰∏çÂ≠òÂú®‰∫é DataFrame ‰∏≠„ÄÇ\")\n",
    "\n",
    "    cols = [col_name] + [c for c in df.columns if c != col_name]\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def weighted_r2(y_true_df, y_pred_df):\n",
    "    weights = {\n",
    "        \"Dry_Green_g\": 0.1,\n",
    "        \"Dry_Dead_g\": 0.1,\n",
    "        \"Dry_Clover_g\": 0.1,\n",
    "        \"GDM_g\": 0.2,\n",
    "        \"Dry_Total_g\": 0.5\n",
    "    }\n",
    "\n",
    "    r2_dict = {}\n",
    "    for col in weights.keys():\n",
    "        r2_dict[col] = r2_score(y_true_df[col], y_pred_df[col])\n",
    "\n",
    "    weighted_score = sum(r2_dict[k] * w for k, w in weights.items())\n",
    "    return weighted_score, r2_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Êï∞ÊçÆËØªÂèñ‰∏éÈ¢ÑÂ§ÑÁêÜ\n",
    "\n",
    "# 1Ô∏è‚É£ ËØªÂèñÂéüÂßãÊï∞ÊçÆ\n",
    "df_file_path = Path(DIRS[\"dir\"] / \"train.csv\")\n",
    "df = pd.read_csv(df_file_path)\n",
    "show_df_info(df, \"train.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ ÊèêÂèñÂîØ‰∏Ä IDÔºà‰æãÂ¶Ç \"ID1011485656__Dry_Green_g\" ‚Üí \"ID1011485656\"Ôºâ\n",
    "df[\"ID\"] = df[\"sample_id\"].str.split(\"__\").str[0]\n",
    "\n",
    "# 3Ô∏è‚É£ Â∞Ü ID ÂàóÁßªÂä®Âà∞ÊúÄÂâçÈù¢\n",
    "df = move_column_first(df, \"ID\")\n",
    "show_df_info(df, \"df\")\n",
    "\n",
    "# üß© ÁõÆÊ†áÂÄºÈÄèËßÜÔºàË°åËΩ¨ÂàóÔºâ\n",
    "df_targets = (\n",
    "    df\n",
    "    .pivot_table(\n",
    "        index=\"ID\",\n",
    "        columns=\"target_name\",\n",
    "        values=\"target\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_targets.columns.name = None  # ÂéªÊéâÂ§öÁ∫ßÂàóÂêçÂ±ÇÊ¨°\n",
    "show_df_info(df_targets, \"df_targets\")\n",
    "\n",
    "# üß¨ ÊèêÂèñÂÖÉ‰ø°ÊÅØÔºàÊØè‰∏™ ID ‰ªÖ‰øùÁïô‰∏ÄË°åÔºâ\n",
    "meta_cols = [\n",
    "    \"ID\", \"image_path\", \"Sampling_Date\", \"State\",\n",
    "    \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"\n",
    "]\n",
    "df_meta = df[meta_cols].drop_duplicates(subset=\"ID\")\n",
    "show_df_info(df_meta, \"df_meta\")\n",
    "\n",
    "# üîó ÂêàÂπ∂ÂÖÉ‰ø°ÊÅØ‰∏éÁõÆÊ†áÊï∞ÊçÆ\n",
    "df_train = pd.merge(df_meta, df_targets, on=\"ID\", how=\"left\")\n",
    "\n",
    "show_df_info(df_train, \"df_train\")\n",
    "\n",
    "\n",
    "# df_wide.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† MyDualStreamModelÔºöÂèåÊµÅ + Â§öÂ§¥ÂõûÂΩí + ÂÜÖÈÉ®ËÆ≠ÁªÉÈÄªËæë\n",
    "\n",
    "\n",
    "class WeightedSmoothL1Loss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "        self.weights = list(weights.values())\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        losses = self.loss_fn(pred, target)\n",
    "        weighted = sum(losses[:, i] * w for i, w in enumerate(self.weights))\n",
    "        return weighted.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyDualStreamModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                backbone_name=\"convnext_tiny\", \n",
    "                pretrained=True, \n",
    "                freeze_ratio=0.8,\n",
    "                weights_dict=None):\n",
    "        \"\"\"\n",
    "        ÂèÇÊï∞:\n",
    "        - backbone_name: timm Ê®°ÂûãÂêçÁß∞ (Â¶Ç convnext_tiny, resnet50)\n",
    "        - pretrained: ÊòØÂê¶Âä†ËΩΩ ImageNet ÊùÉÈáç\n",
    "        - freeze_ratio: ÂÜªÁªìÊØî‰æãÔºà0~1Ôºâ\n",
    "        - weights_dict: ÂêÑÁõÆÊ†áÊùÉÈáç (dict), Áî®‰∫é WeightedSmoothL1Loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1Ô∏è‚É£ Backbone\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        in_dim = self.backbone.num_features\n",
    "\n",
    "        # 2Ô∏è‚É£ ÂÜªÁªìÈÉ®ÂàÜÂèÇÊï∞\n",
    "        params = list(self.backbone.parameters())\n",
    "        freeze_until = int(len(params) * freeze_ratio)\n",
    "        for i, p in enumerate(params):\n",
    "            p.requires_grad = i >= freeze_until  # ÂâçÈÉ®ÂàÜÂÜªÁªìÔºåÂêéÈÉ®ÂàÜÂèØÂ≠¶‰π†\n",
    "\n",
    "        # 3Ô∏è‚É£ ÂèåÊµÅËûçÂêà\n",
    "        self.fusion_dim = in_dim * 2\n",
    "\n",
    "        # 4Ô∏è‚É£ ‰∏â‰∏™ËæìÂá∫ Head\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.fusion_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "\n",
    "        self.head_total = make_head()\n",
    "        self.head_gdm   = make_head()\n",
    "        self.head_green = make_head()\n",
    "\n",
    "        # 5Ô∏è‚É£ ÊçüÂ§±ÂáΩÊï∞ÔºàWeighted SmoothL1LossÔºâ\n",
    "        self.loss_fn = WeightedSmoothL1Loss(weights_dict) if weights_dict else nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # üîÅ Forward\n",
    "    # ------------------------------------------------------------\n",
    "    def forward(self, img_left, img_right):\n",
    "        feat_left  = self.backbone(img_left)\n",
    "        feat_right = self.backbone(img_right)\n",
    "        fused = torch.cat([feat_left, feat_right], dim=1)\n",
    "\n",
    "        total = self.head_total(fused)\n",
    "        gdm   = self.head_gdm(fused)\n",
    "        green = self.head_green(fused)\n",
    "        preds = torch.cat([green, gdm, total], dim=1)\n",
    "        return preds  # shape: [batch, 3]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # üßÆ ÊçüÂ§±ËÆ°ÁÆóÔºàÂÜÖÈÉ®Ë∞ÉÁî®Ôºâ\n",
    "    # ------------------------------------------------------------\n",
    "    def compute_loss(self, preds, targets):\n",
    "        return self.loss_fn(preds, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DualStreamDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, target_cols=None, transform=None):\n",
    "        \"\"\"\n",
    "        df: DataFrameÔºåÂåÖÂê´ image_path Âàó\n",
    "        image_dir: ÂõæÂÉèÁõÆÂΩï\n",
    "        target_cols: Â¶ÇÊûúÊòØËÆ≠ÁªÉÈõÜÔºåÊåáÂÆöÁõÆÊ†áÂàó\n",
    "        transform: Albumentations ÂèòÊç¢\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.target_cols = target_cols\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = Path(self.image_dir, str(row[\"image_path\"]))\n",
    "        \n",
    "        # ====== 1Ô∏è‚É£ ÂÆâÂÖ®Âä†ËΩΩ ======\n",
    "        if not img_path.exists():\n",
    "            print(f\"‚ö†Ô∏è ÂõæÁâá‰∏çÂ≠òÂú®: {img_path}\")\n",
    "            image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Êó†Ê≥ïËØªÂèñÂõæÁâá: {img_path} ({e})\")\n",
    "                image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "\n",
    "        # ====== 2Ô∏è‚É£ Á°Æ‰øùËΩ¨Êç¢‰∏∫ NumPy Êï∞ÁªÑ ======\n",
    "        image = np.array(image)  # ËΩ¨Êç¢‰∏∫ NumPy Êï∞ÁªÑ\n",
    "        h, w, _ = image.shape\n",
    "        mid = w // 2\n",
    "        \n",
    "        # ÊãÜÂàÜÊàêÂ∑¶Âè≥‰∏§‰∏™ patch\n",
    "        img_left = image[:, :mid]\n",
    "        img_right = image[:, mid:]\n",
    "\n",
    "        # ====== 4Ô∏è‚É£ Â∫îÁî® Albumentations ÂèòÊç¢ ======\n",
    "        if self.transform:\n",
    "            img_left = self.transform(image=img_left)[\"image\"]\n",
    "            img_right = self.transform(image=img_right)[\"image\"]\n",
    "\n",
    "        # ====== 5Ô∏è‚É£ ËøîÂõûÁªìÊûú ======\n",
    "        if self.target_cols is not None:\n",
    "            targets = torch.tensor(row[self.target_cols].astype(float).values, dtype=torch.float32)\n",
    "            return img_left, img_right, targets\n",
    "        else:\n",
    "            return img_left, img_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Albumentations ÂèòÊç¢\n",
    "def get_train_transforms(size=768):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ColorJitter(p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_valid_transforms(size=768):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "def train_with_groupkfold(\n",
    "    df_train,\n",
    "    save_dir,\n",
    "    model_target_cols,\n",
    "    get_train_transforms,\n",
    "    get_valid_transforms,\n",
    "    weights,\n",
    "    freeze_ratio=0.8,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "):\n",
    "\n",
    "    scaler = GradScaler()  # ‚úÖ ÂàùÂßãÂåñÁº©ÊîæÂô®ÔºàÁî®‰∫éFP16Ê¢ØÂ∫¶Á®≥ÂÆöÔºâ\n",
    "\n",
    "    # ÂÆö‰πâÂàÜÂ±ÇÂô®Ôºà5Êäò‰∫§ÂèâÈ™åËØÅÔºâ\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    # ÂèñÁ¨¨‰∏ÄÊäò\n",
    "    df = df_train.copy()\n",
    "    groups = df[\"Sampling_Date\"]\n",
    "\n",
    "    # üîπ Áî®‰∫é‰øùÂ≠òÊØèÊäòËÆ≠ÁªÉ/È™åËØÅÊçüÂ§±\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=groups)):\n",
    "        print(f\"Fold {fold:3d}: Train={len(train_idx)}, Valid={len(val_idx)}\")\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        valid_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "        train_dataset = DualStreamDataset(\n",
    "            train_df, DIRS[\"dir\"], model_target_cols, transform=get_train_transforms(768)\n",
    "        )\n",
    "        valid_dataset = DualStreamDataset(\n",
    "            valid_df, DIRS[\"dir\"], model_target_cols, transform=get_valid_transforms(768)\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Â¢ûÂä† pin_memory ÊèêÈ´ò‰∏ªÊú∫‚ÜíGPU ‰º†ËæìÈÄüÂ∫¶\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        # ‚úÖ Ê®°Âûã‰ºòÂåñÔºöchannels_last ÂÜÖÂ≠òÂ∏ÉÂ±Ä + AMP ÂÖºÂÆπ\n",
    "        model = MyDualStreamModel(\"convnext_tiny\", pretrained=True, freeze_ratio=freeze_ratio, weights_dict=weights)\n",
    "        model = model.to(device)\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        # üîπ ÊØèÊäòËÆ∞ÂΩïÊçüÂ§±\n",
    "        train_losses_per_epoch = []\n",
    "        val_losses_per_epoch = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # === üî• ÂêØÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ ===\n",
    "            model.train()\n",
    "            running_train_loss = []\n",
    "\n",
    "            # ‚úÖ tqdm ÂÆûÊó∂ÊòæÁ§∫\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1:3d}/{epochs}\"):\n",
    "                img_left, img_right, targets = batch\n",
    "                img_left, img_right, targets = (\n",
    "                    img_left.to(device, non_blocking=True),\n",
    "                    img_right.to(device, non_blocking=True),\n",
    "                    targets.to(device, non_blocking=True),\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)  # ‚úÖ Êõ¥È´òÊïàÊ∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "\n",
    "                # ‚úÖ AMPÊ∑∑ÂêàÁ≤æÂ∫¶‰∏ä‰∏ãÊñá\n",
    "                with autocast():\n",
    "                    preds = model(img_left, img_right)\n",
    "                    loss = model.compute_loss(preds, targets)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                # scaler.unscale_(optimizer)  # ÂèØÈÄâÔºöÂ¶ÇÊûúÊÉ≥Âä†Ê¢ØÂ∫¶Ë£ÅÂâ™ÔºåÂèØÂú®Ê≠§Ëß£Áº©Êîæ\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_train_loss.append(loss.item())\n",
    "\n",
    "            avg_train_loss = float(np.mean(running_train_loss))\n",
    "\n",
    "            # === üßä È™åËØÅÈò∂ÊÆµÔºà‰∏çÁî® AMPÔºâ===\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    img_left, img_right, targets = batch\n",
    "                    img_left, img_right, targets = (\n",
    "                        img_left.to(device, non_blocking=True),\n",
    "                        img_right.to(device, non_blocking=True),\n",
    "                        targets.to(device, non_blocking=True),\n",
    "                    )\n",
    "                    preds = model(img_left, img_right)\n",
    "                    val_loss = model.compute_loss(preds, targets).item()\n",
    "                    val_losses.append(val_loss)\n",
    "\n",
    "            avg_val_loss = float(np.mean(val_losses))\n",
    "\n",
    "            print(f\"Epoch {epoch+1:3d} | Train={avg_train_loss:.4f} | Val={avg_val_loss:.4f}\")\n",
    "\n",
    "            train_losses_per_epoch.append(avg_train_loss)\n",
    "            val_losses_per_epoch.append(avg_val_loss)\n",
    "\n",
    "        # ‚úÖ ‰øùÂ≠òÊØèÊäòÁöÑÊâÄÊúâepochÊçüÂ§±\n",
    "        fold_train_losses.append(train_losses_per_epoch)\n",
    "        fold_val_losses.append(val_losses_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        save_path = save_dir / f\"model_weights_fold_{fold}.pt\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        print(f\"‚úÖ ÂÆåÊï¥Ê®°ÂûãÂ∑≤‰øùÂ≠òÂà∞: {save_path}\")\n",
    "        \n",
    "\n",
    "    # Ëé∑ÂèñÊúÄÂ§ßepochÊï∞ÔºàÊúâÊó∂‰∏çÂêåfoldÂèØËÉΩÈïøÂ∫¶‰∏çÂêåÔºâ\n",
    "    max_epochs = max(len(x) for x in fold_train_losses)\n",
    "    df = pd.DataFrame({\"Epoch\": range(1, max_epochs + 1)})\n",
    "\n",
    "    # ÈÄê‰∏™foldÂ°´ÂÖÖ\n",
    "    for i, (train_list, val_list) in enumerate(zip(fold_train_losses, fold_val_losses), start=1):\n",
    "        # Ëã•Êüê‰∏™foldÈïøÂ∫¶‰∏çË∂≥ÔºåÂàôÁî®NoneÂ°´ÂÖÖ\n",
    "        pad_train = train_list + [None] * (max_epochs - len(train_list))\n",
    "        pad_val = val_list + [None] * (max_epochs - len(val_list))\n",
    "        \n",
    "        df[f\"Train_Loss_Fold{i}\"] = pad_train\n",
    "        df[f\"Val_Loss_Fold{i}\"] = pad_val\n",
    "\n",
    "    # ‰øùÂ≠òÂà∞Excel\n",
    "    output_path = Path(save_dir, \"fold_losses_wide.xlsx\")\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"‚úÖ ÊØèFoldÁöÑTrain/Val LossÂ∑≤‰øùÂ≠ò‰∏∫ÂàóÔºö{output_path}\")\n",
    "\n",
    "    return fold_train_losses, fold_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights           = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"GDM_g\":        0.2,\n",
    "    \"Dry_Total_g\":  0.5\n",
    "}\n",
    "\n",
    "model_target_cols = [\"Dry_Green_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "target_cols       = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"epochs\" : 2,\n",
    "    \"freeze_ratio\" : 0.8,\n",
    "    \"batch_size\" : 32,\n",
    "    \"lr\" : 1e-4,\n",
    "}\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# üöÄ ÂêØÂä®ËÆ≠ÁªÉÔºàGroupKFoldÔºâ\n",
    "# ==========================\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = Path(DIRS['model'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "    fold_train_losses, fold_val_losses = train_with_groupkfold(\n",
    "        df_train             = df_train,\n",
    "        save_dir             = history_DIR,\n",
    "        model_target_cols    = model_target_cols,\n",
    "        get_train_transforms = get_train_transforms,\n",
    "        get_valid_transforms = get_valid_transforms,\n",
    "        weights              = weights,\n",
    "        freeze_ratio         = config[\"freeze_ratio\"],\n",
    "        batch_size           = config[\"batch_size\"],\n",
    "        epochs               = config[\"epochs\"],\n",
    "        lr                   = config[\"lr\"],\n",
    "        device               = device,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßÆ ÂêéÂ§ÑÁêÜÂáΩÊï∞ÔºàÊÅ¢Â§ç 5 ‰∏™ÁõÆÊ†áÔºâ\n",
    "def recover_all_targets(df_pred_3):\n",
    "    df = df_pred_3.copy()\n",
    "    df[\"Dry_Clover_g\"] = np.maximum(0, df[\"GDM_g\"] - df[\"Dry_Green_g\"])\n",
    "    df[\"Dry_Dead_g\"] = np.maximum(0, df[\"Dry_Total_g\"] - df[\"GDM_g\"])\n",
    "    return df[[\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Êï∞ÊçÆËØªÂèñ‰∏éÈ¢ÑÂ§ÑÁêÜ\n",
    "\n",
    "# 1Ô∏è‚É£ ËØªÂèñÂéüÂßãÊï∞ÊçÆ\n",
    "df_file_path = Path(DIRS[\"dir\"] / \"test.csv\")\n",
    "df = pd.read_csv(df_file_path)\n",
    "show_df_info(df, \"test.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ ÊèêÂèñÂîØ‰∏Ä IDÔºà‰æãÂ¶Ç \"ID1011485656__Dry_Green_g\" ‚Üí \"ID1011485656\"Ôºâ\n",
    "df[\"ID\"] = df[\"sample_id\"].str.split(\"__\").str[0]\n",
    "\n",
    "# 3Ô∏è‚É£ Â∞Ü ID ÂàóÁßªÂä®Âà∞ÊúÄÂâçÈù¢\n",
    "df = move_column_first(df, \"ID\")\n",
    "\n",
    "df[\"target\"] = 0\n",
    "\n",
    "show_df_info(df, \"df\")\n",
    "\n",
    "# üß© ÁõÆÊ†áÂÄºÈÄèËßÜÔºàË°åËΩ¨ÂàóÔºâ\n",
    "df_targets = (\n",
    "    df\n",
    "    .pivot_table(\n",
    "        index=\"ID\",\n",
    "        columns=\"target_name\",\n",
    "        values=\"target\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_targets.columns.name = None  # ÂéªÊéâÂ§öÁ∫ßÂàóÂêçÂ±ÇÊ¨°\n",
    "show_df_info(df_targets, \"df_targets\")\n",
    "\n",
    "# üß¨ ÊèêÂèñÂÖÉ‰ø°ÊÅØÔºàÊØè‰∏™ ID ‰ªÖ‰øùÁïô‰∏ÄË°åÔºâ\n",
    "# meta_cols = [\n",
    "#     \"ID\", \"image_path\", \"Sampling_Date\", \"State\",\n",
    "#     \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"\n",
    "# ]\n",
    "\n",
    "meta_cols = [\n",
    "    \"ID\", \"image_path\"\n",
    "]\n",
    "df_meta = df[meta_cols].drop_duplicates(subset=\"ID\")\n",
    "show_df_info(df_meta, \"df_meta\")\n",
    "\n",
    "# üîó ÂêàÂπ∂ÂÖÉ‰ø°ÊÅØ‰∏éÁõÆÊ†áÊï∞ÊçÆ\n",
    "df_test = pd.merge(df_meta, df_targets, on=\"ID\", how=\"left\")\n",
    "\n",
    "show_df_info(df_test, \"df_wide\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def predict_ensemble_df(df_test, transform, model, model_target_cols, model_dir, device, batch_size=32, img_size=768):\n",
    "\n",
    "    model_dir = model_dir\n",
    "    print(f\"Ê®°ÂûãÁõÆÂΩï: {model_dir}\")\n",
    "    assert model_dir.exists(), f\"‚ùå Ê®°ÂûãÁõÆÂΩï‰∏çÂ≠òÂú®: {model_dir}\"\n",
    "\n",
    "    # üîç ÊêúÁ¥¢ÊâÄÊúâ fold Ê®°Âûã\n",
    "    model_paths = sorted(model_dir.glob(\"model_weights_fold*.pt\"))\n",
    "    if not model_paths:\n",
    "        raise FileNotFoundError(f\"‚ùå Êú™ÊâæÂà∞Ê®°ÂûãÊñá‰ª∂: {model_dir}/model_weights_fold*.pt\")\n",
    "\n",
    "    print(f\"üîπ Ê£ÄÊµãÂà∞ {len(model_paths)} ‰∏™Ê®°Âûã:\")\n",
    "    for p in model_paths:\n",
    "        print(\"   -\", p.name)\n",
    "\n",
    "    # 3Ô∏è‚É£ ÊûÑÂª∫ÊµãËØïÊï∞ÊçÆÈõÜ\n",
    "    test_dataset = DualStreamDataset(\n",
    "        df_test,\n",
    "        image_dir=DIRS[\"dir\"],\n",
    "        target_cols=None,\n",
    "        transform=transform   \n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Â≠òÂÇ®ÊØè‰∏™foldÁöÑÈ¢ÑÊµã\n",
    "    fold_preds = []\n",
    "\n",
    "    for fold, model_path in enumerate(model_paths):\n",
    "        # print(f\"üöÄ Âä†ËΩΩÊ®°Âûã {fold+1}/{len(model_paths)}: {model_path.name}\")\n",
    "\n",
    "        # 1Ô∏è‚É£ Âä†ËΩΩÊ®°ÂûãÁªìÊûÑ\n",
    "        model = model\n",
    "\n",
    "        # 2Ô∏è‚É£ Âä†ËΩΩÊùÉÈáç\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        # 3Ô∏è‚É£ Êé®ÁêÜ\n",
    "        preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for img_left, img_right in test_loader:\n",
    "                img_left, img_right = img_left.to(device, non_blocking=True), img_right.to(device, non_blocking=True)\n",
    "                preds = model(img_left, img_right)\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "        fold_pred = np.concatenate(preds_list, axis=0)\n",
    "        fold_preds.append(fold_pred)\n",
    "\n",
    "    # 4Ô∏è‚É£ Â§öÊ®°ÂûãÂπ≥Âùá\n",
    "    preds_mean = np.mean(fold_preds, axis=0)\n",
    "    df_pred3 = pd.DataFrame(preds_mean, columns=model_target_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ÊÅ¢Â§çÂÆåÊï¥ÁöÑ 5 ‰∏™ÁõÆÊ†áÂàó\n",
    "    df_pred5 = recover_all_targets(df_pred3)\n",
    "    show_df_info(df_pred5, \"df_pred5 \")\n",
    "\n",
    "\n",
    "    # ËøΩÂä†Ê†∑Êú¨ ID Âπ∂Ë∞ÉÊï¥ÂàóÈ°∫Â∫è\n",
    "    df_pred5[\"ID\"] = df_test[\"ID\"]\n",
    "    df_pred5 = df_pred5[[\"ID\"] + target_cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ÊâìÂç∞ÁªìÊûúÈ¢ÑËßà\n",
    "    show_df_info(df_pred5, \"final df_pred5\")\n",
    "\n",
    "    return df_pred5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transforms = {\n",
    "    \"base\": A.Compose([\n",
    "        A.Resize(768, 768),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    \"hflip\": A.Compose([\n",
    "        A.Resize(768, 768),\n",
    "        A.HorizontalFlip(p=1),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    \"vflip\": A.Compose([\n",
    "        A.Resize(768, 768),\n",
    "        A.VerticalFlip(p=1),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Âä†ËΩΩÊ®°ÂûãÁªìÊûÑ\n",
    "model = MyDualStreamModel(\"convnext_tiny\", \n",
    "                          pretrained=False, \n",
    "                          freeze_ratio=config[\"freeze_ratio\"], \n",
    "                          weights_dict=weights)\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "model_dir = history_DIR\n",
    "model_dir = history_DIR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tta_preds = []\n",
    "\n",
    "for name, tform in tta_transforms.items():\n",
    "    print(f\"üöÄ Running TTA: {name}\")\n",
    "\n",
    "\n",
    "    transform = tform\n",
    "    df_pred5 = predict_ensemble_df(\n",
    "        df_test=df_test,\n",
    "        transform = transform,\n",
    "        model = model,\n",
    "        model_target_cols=model_target_cols,\n",
    "        model_dir = model_dir,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    print(df_pred5)\n",
    "    tta_preds.append(df_pred5[target_cols].values)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(tta_preds)\n",
    "mean_preds = np.mean(tta_preds, axis=0)\n",
    "\n",
    "print(mean_preds)\n",
    "df_pred_final = df_pred5.copy()\n",
    "df_pred_final[target_cols] = mean_preds\n",
    "print(df_pred_final)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üì§ 5Ô∏è‚É£ ÁîüÊàê Kaggle Êèê‰∫§Êñá‰ª∂ submission.csv\n",
    "# =========================================\n",
    "# ÊåâÊåáÂÆöÈ°∫Â∫èÂ±ïÂºÄ\n",
    "ordered_target_cols = [\n",
    "    \"Dry_Clover_g\",  # 1Ô∏è‚É£\n",
    "    \"Dry_Dead_g\",    # 2Ô∏è‚É£\n",
    "    \"Dry_Green_g\",   # 3Ô∏è‚É£\n",
    "    \"Dry_Total_g\",   # 4Ô∏è‚É£\n",
    "    \"GDM_g\"          # 5Ô∏è‚É£\n",
    "]\n",
    "\n",
    "df_submit = (\n",
    "    df_pred5\n",
    "    .melt(id_vars=\"ID\", value_vars=ordered_target_cols,\n",
    "          var_name=\"target_name\", value_name=\"target\")\n",
    ")\n",
    "\n",
    "# ÁªÑÂêàÊàê Kaggle ÊâÄÈúÄÁöÑ sample_id\n",
    "df_submit[\"sample_id\"] = df_submit[\"ID\"] + \"__\" + df_submit[\"target_name\"]\n",
    "\n",
    "df_submit = move_column_first(df_submit, \"target\")\n",
    "df_submit = move_column_first(df_submit, \"sample_id\")\n",
    "\n",
    "# Âè™‰øùÁïô Kaggle Ë¶ÅÁöÑ‰∏§Âàó\n",
    "df_submit = df_submit[[\"sample_id\", \"target\"]]\n",
    "df_submit\n",
    "# Êåâ sample_id ÊéíÂ∫èÔºàÂèØÈÄâÔºâ\n",
    "# df_submit = df_submit.sort_values(\"sample_id\").reset_index(drop=True)\n",
    "\n",
    "# ‰øùÂ≠òÊñá‰ª∂\n",
    "df_submit.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ Â∑≤ÁîüÊàêÊèê‰∫§Êñá‰ª∂ submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8607422,
     "sourceId": 13552583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
