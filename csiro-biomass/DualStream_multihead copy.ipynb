{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-30T08:07:14.763074Z",
     "iopub.status.busy": "2025-10-30T08:07:14.762816Z",
     "iopub.status.idle": "2025-10-30T08:07:21.990822Z",
     "shell.execute_reply": "2025-10-30T08:07:21.990102Z",
     "shell.execute_reply.started": "2025-10-30T08:07:14.763052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 📦 导入库\n",
    "import os\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import cv2\n",
    "import h5py\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import get_model_weights\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# ⚙️ 全局配置\n",
    "# =========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ 使用设备: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:07:21.992327Z",
     "iopub.status.busy": "2025-10-30T08:07:21.991922Z",
     "iopub.status.idle": "2025-10-30T08:07:21.998124Z",
     "shell.execute_reply": "2025-10-30T08:07:21.997471Z",
     "shell.execute_reply.started": "2025-10-30T08:07:21.992307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 路径：\n",
      "\n",
      "dir          : D:\\DATA_hao\\Kaggle_\\csiro-biomass\n",
      "train        : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\train\n",
      "test         : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\test\n",
      "model        : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\DualStream_multihead\n",
      "data         : D:\\DATA_hao\\Kaggle_\\csiro-biomass\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = Path('D:/DATA_hao/Kaggle_/csiro-biomass/')\n",
    "    DIRS = {\n",
    "    \"dir\":        dir,                                       \n",
    "    \"train\":     Path(dir, \"train\"),                              \n",
    "    \"test\":     Path(dir, \"test\"),                              \n",
    "    \"model\":     Path(dir,\"DualStream_multihead\"),              \n",
    "    \"data\":     Path(dir),   \n",
    "    }\n",
    "else:\n",
    "    dir = Path('/kaggle/input/csiro-biomass')\n",
    "    DIRS = {\n",
    "    \"dir\":        dir,                                       \n",
    "    \"train\":     Path(dir, \"train\"),                              \n",
    "    \"test\":     Path(dir, \"test\"),                              \n",
    "    \"model\":     Path('/kaggle/input', \"dualstream-multihead-model\"),              \n",
    "    \"data\":     Path(\"/kaggle/working/\"),   \n",
    "    }\n",
    "\n",
    "\n",
    "# 打印时一行一个地址\n",
    "print(\"✅ 路径：\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:07:21.999218Z",
     "iopub.status.busy": "2025-10-30T08:07:21.998874Z",
     "iopub.status.idle": "2025-10-30T08:07:22.506492Z",
     "shell.execute_reply": "2025-10-30T08:07:22.505864Z",
     "shell.execute_reply.started": "2025-10-30T08:07:21.999195Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 小函数\n",
    "def show_df_info(df, name: str):\n",
    "    \"\"\"\n",
    "    打印单个 DataFrame 的形状与列名信息。\n",
    "    参数:\n",
    "        df   : pandas.DataFrame\n",
    "        name : 显示名称（字符串）\n",
    "    \"\"\"\n",
    "    print(f\"📊 {name:<16} shape: {str(df.shape):<16}  列名: {df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "\n",
    "def move_column_first(df, col_name):\n",
    "    \"\"\"\n",
    "    将 DataFrame 中指定列移动到最前面。\n",
    "    参数:\n",
    "        df (pd.DataFrame): 原始数据框\n",
    "        col_name (str): 要移动到最前面的列名\n",
    "    返回:\n",
    "        pd.DataFrame: 调整后的新 DataFrame\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"列 '{col_name}' 不存在于 DataFrame 中。\")\n",
    "\n",
    "    cols = [col_name] + [c for c in df.columns if c != col_name]\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "\n",
    "# 🧮 后处理函数（恢复 5 个目标）\n",
    "def recover_all_targets(df_pred_3):\n",
    "    df = df_pred_3.copy()\n",
    "    df[\"Dry_Clover_g\"] = np.maximum(0, df[\"GDM_g\"] - df[\"Dry_Green_g\"])\n",
    "    df[\"Dry_Dead_g\"] = np.maximum(0, df[\"Dry_Total_g\"] - df[\"GDM_g\"])\n",
    "    return df[[\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集、模型、训练 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 MyDualStreamModel：双流 + 多头回归 + 内部训练逻辑\n",
    "class WeightedSmoothL1Loss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "        self.weights = list(weights.values())\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        losses = self.loss_fn(pred, target)\n",
    "        weighted = sum(losses[:, i] * w for i, w in enumerate(self.weights))\n",
    "        return weighted.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyDualStreamModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                backbone_name=\"convnext_tiny\", \n",
    "                pretrained=True, \n",
    "                freeze_ratio=0.8,\n",
    "                weights_dict=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "        - backbone_name: timm 模型名称 (如 convnext_tiny, resnet50)\n",
    "        - pretrained: 是否加载 ImageNet 权重\n",
    "        - freeze_ratio: 冻结比例（0~1）\n",
    "        - weights_dict: 各目标权重 (dict), 用于 WeightedSmoothL1Loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1️⃣ Backbone\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        in_dim = self.backbone.num_features\n",
    "\n",
    "        # 2️⃣ 冻结部分参数\n",
    "        params = list(self.backbone.parameters())\n",
    "        freeze_until = int(len(params) * freeze_ratio)\n",
    "        for i, p in enumerate(params):\n",
    "            p.requires_grad = i >= freeze_until  # 前部分冻结，后部分可学习\n",
    "\n",
    "        # 3️⃣ 双流融合\n",
    "        self.fusion_dim = in_dim * 2\n",
    "\n",
    "        # 4️⃣ 三个输出 Head\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.fusion_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "\n",
    "        self.head_total = make_head()\n",
    "        self.head_gdm   = make_head()\n",
    "        self.head_green = make_head()\n",
    "\n",
    "        # 5️⃣ 损失函数（Weighted SmoothL1Loss）\n",
    "        self.loss_fn = WeightedSmoothL1Loss(weights_dict) if weights_dict else nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🔁 Forward\n",
    "    # ------------------------------------------------------------\n",
    "    def forward(self, img_left, img_right):\n",
    "        feat_left  = self.backbone(img_left)\n",
    "        feat_right = self.backbone(img_right)\n",
    "        fused = torch.cat([feat_left, feat_right], dim=1)\n",
    "\n",
    "        total = self.head_total(fused)\n",
    "        gdm   = self.head_gdm(fused)\n",
    "        green = self.head_green(fused)\n",
    "        preds = torch.cat([green, gdm, total], dim=1)\n",
    "        return preds  # shape: [batch, 3]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🧮 损失计算（内部调用）\n",
    "    # ------------------------------------------------------------\n",
    "    def compute_loss(self, preds, targets):\n",
    "        return self.loss_fn(preds, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载定义\n",
    "class DualStreamDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, target_cols=None, transform=None):\n",
    "        \"\"\"\n",
    "        df: DataFrame，包含 image_path 列\n",
    "        image_dir: 图像目录\n",
    "        target_cols: 如果是训练集，指定目标列\n",
    "        transform: Albumentations 变换\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.target_cols = target_cols\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = Path(self.image_dir, str(row[\"image_path\"]))\n",
    "        \n",
    "        # ====== 1️⃣ 安全加载 ======\n",
    "        if not img_path.exists():\n",
    "            print(f\"⚠️ 图片不存在: {img_path}\")\n",
    "            image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 无法读取图片: {img_path} ({e})\")\n",
    "                image = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "\n",
    "        # ====== 2️⃣ 确保转换为 NumPy 数组 ======\n",
    "        image = np.array(image)  # 转换为 NumPy 数组\n",
    "        h, w, _ = image.shape\n",
    "        mid = w // 2\n",
    "        \n",
    "        # 拆分成左右两个 patch\n",
    "        img_left = image[:, :mid]\n",
    "        img_right = image[:, mid:]\n",
    "\n",
    "        # ====== 4️⃣ 应用 Albumentations 变换 ======\n",
    "        if self.transform:\n",
    "            img_left = self.transform(image=img_left)[\"image\"]\n",
    "            img_right = self.transform(image=img_right)[\"image\"]\n",
    "\n",
    "        # ====== 5️⃣ 返回结果 ======\n",
    "        if self.target_cols is not None:\n",
    "            targets = torch.tensor(row[self.target_cols].astype(float).values, dtype=torch.float32)\n",
    "            return img_left, img_right, targets\n",
    "        else:\n",
    "            return img_left, img_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations 变换   训练集、验证集、测试TTA\n",
    "def get_train_transforms(size=768):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ColorJitter(p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_valid_transforms(size=768):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "tta_transforms = {\n",
    "    \"base\": A.Compose([\n",
    "        A.Resize(768, 768),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    \"hflip\": A.Compose([\n",
    "        A.Resize(768, 768),\n",
    "        A.HorizontalFlip(p=1),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    \"vflip\": A.Compose([\n",
    "        A.Resize(768, 768),\n",
    "        A.VerticalFlip(p=1),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_fold_cv_score(valid_df, all_preds, all_targets):\n",
    "    \"\"\"\n",
    "    计算单个 Fold 的 Weighted R² 分数（与 Kaggle Metric 对齐）\n",
    "\n",
    "    参数:\n",
    "        valid_df      : 当前 fold 的验证 DataFrame（含真实值5列）\n",
    "        all_preds     : 模型预测结果 (list of numpy arrays, shape=[N,3])\n",
    "        all_targets   : 真实目标 (list of numpy arrays, shape=[N,3])\n",
    "\n",
    "    返回:\n",
    "        weighted_r2   : 加权 R² 分数\n",
    "        r2_each       : 各目标单独 R²\n",
    "    \"\"\"\n",
    "    preds_array = np.concatenate(all_preds)\n",
    "    targets_array = np.concatenate(all_targets)\n",
    "\n",
    "    # 构建真实值表\n",
    "    df_val = valid_df.copy()\n",
    "    df_val[[\"Dry_Green_g\", \"GDM_g\", \"Dry_Total_g\"]] = targets_array\n",
    "\n",
    "    # 构建预测表\n",
    "    df_pred = df_val.copy()\n",
    "    df_pred[\"Dry_Green_g\"] = preds_array[:, 0]\n",
    "    df_pred[\"GDM_g\"]       = preds_array[:, 1]\n",
    "    df_pred[\"Dry_Total_g\"] = preds_array[:, 2]\n",
    "\n",
    "    # 根据关系式补齐\n",
    "    df_pred[\"Dry_Clover_g\"] = df_pred[\"GDM_g\"] - df_pred[\"Dry_Green_g\"]\n",
    "    df_pred[\"Dry_Dead_g\"]   = df_pred[\"Dry_Total_g\"] - df_pred[\"GDM_g\"]\n",
    "\n",
    "    # 计算各列R²\n",
    "    target_cols = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    r2_each = {col: r2_score(df_val[col], df_pred[col]) for col in target_cols}\n",
    "\n",
    "    # 加权平均（权重与 Kaggle 一致）\n",
    "    weights = {\n",
    "        \"Dry_Green_g\": 0.1,\n",
    "        \"Dry_Dead_g\": 0.1,\n",
    "        \"Dry_Clover_g\": 0.1,\n",
    "        \"GDM_g\": 0.2,\n",
    "        \"Dry_Total_g\": 0.5,\n",
    "    }\n",
    "    weighted_r2 = sum(r2_each[k] * w for k, w in weights.items())\n",
    "    return weighted_r2, r2_each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 单轮训练\n",
    "def train_one_epoch(model, dataloader, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "\n",
    "    for img_left, img_right, targets in dataloader:\n",
    "        img_left, img_right, targets = (\n",
    "            img_left.to(device, non_blocking=True),\n",
    "            img_right.to(device, non_blocking=True),\n",
    "            targets.to(device, non_blocking=True),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # ✅ 更高效清空梯度\n",
    "        # ✅ AMP混合精度上下文\n",
    "        with autocast():\n",
    "            preds = model(img_left, img_right)\n",
    "            loss = model.compute_loss(preds, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # scaler.unscale_(optimizer)  # 可选：如果想加梯度裁剪，可在此解缩放\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    return float(np.mean(running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 单轮验证 + 本地CV\n",
    "def validate_one_epoch(model, dataloader, valid_df, device):\n",
    "    model.eval()\n",
    "    val_losses, all_preds, all_targets = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_left, img_right, targets in dataloader:\n",
    "            img_left, img_right, targets = (\n",
    "                img_left.to(device, non_blocking=True),\n",
    "                img_right.to(device, non_blocking=True),\n",
    "                targets.to(device, non_blocking=True),\n",
    "            )\n",
    "            preds = model(img_left, img_right)\n",
    "            val_loss = model.compute_loss(preds, targets).item()\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = float(np.mean(val_losses))\n",
    "    weighted_r2, _ = compute_fold_cv_score(valid_df, all_preds, all_targets)\n",
    "    return avg_val_loss, weighted_r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 主函数：KFold 训练\n",
    "def train_with_groupkfold(\n",
    "    df_train,\n",
    "    save_dir,\n",
    "    model_target_cols,\n",
    "    get_train_transforms,\n",
    "    get_valid_transforms,\n",
    "    weights,\n",
    "    freeze_ratio=0.8,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    save_interval=20,\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    df = df_train.copy()\n",
    "    groups = df[\"Sampling_Date\"]\n",
    "\n",
    "    # 用于保存每折 训练损失  验证  本地CV\n",
    "    fold_train_losses, fold_val_losses, fold_cv_scores = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=groups)):\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        valid_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = DualStreamDataset(train_df, DIRS[\"dir\"], model_target_cols, transform=get_train_transforms(768))\n",
    "        valid_dataset = DualStreamDataset(valid_df, DIRS[\"dir\"], model_target_cols, transform=get_valid_transforms(768))\n",
    "\n",
    "        # ✅ 增加 pin_memory 提高主机→GPU 传输速度\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        # ✅ 模型优化：channels_last 内存布局 + AMP 兼容\n",
    "        model = MyDualStreamModel(\"convnext_tiny\", pretrained=True, freeze_ratio=freeze_ratio, weights_dict=weights)\n",
    "        model = model.to(device).to(memory_format=torch.channels_last)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "        \n",
    "        # ✅ 初始化缩放器（用于FP16梯度稳定）\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        # 用于保存当前折 训练损失  验证  本地CV\n",
    "        train_losses, val_losses, cv_scores = [], [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            avg_train_loss = train_one_epoch(model, train_loader, optimizer, device, scaler)\n",
    "            avg_val_loss, weighted_r2 = validate_one_epoch(model, valid_loader, valid_df, device)\n",
    "\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            cv_scores.append(weighted_r2)\n",
    "\n",
    "            # ===  保存  ===\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                save_path = save_dir / f\"model_weights_fold{fold}_epoch{epoch+1}.pt\"\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "            # === 时间计算 ===\n",
    "            elapsed = time.time() - start_time\n",
    "            progress = (epoch + 1) + fold * epochs\n",
    "            all_progress = epochs * n_splits\n",
    "            eta_seconds = elapsed / progress * (all_progress - progress)\n",
    "            eta_time = datetime.now() + timedelta(seconds=eta_seconds)\n",
    "            now_str = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            eta_str = eta_time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            # === 🖨️ 打印信息（带时间 + 预计结束时间） ===\n",
    "            print(\n",
    "                f\"[{now_str}]🧩[{progress/all_progress*100:.2f}%] Fold{fold+1:2d}/{n_splits} \"\n",
    "                f\"Epoch{epoch+1:3d}/{epochs} | \"\n",
    "                f\"Train={avg_train_loss:.4f} | \"\n",
    "                f\"Val={avg_val_loss:.4f} | \"\n",
    "                f\"CV={weighted_r2:.4f} | \"\n",
    "                f\"{elapsed / progress:.2f}s/it | \"\n",
    "                f\"ETA≈{eta_str}\",\n",
    "                end=\"\\r\",\n",
    "                flush=True\n",
    "            )\n",
    "\n",
    "        # 保存完整 fold\n",
    "        torch.save(model.state_dict(), save_dir / f\"model_weights_fold{fold}_final.pt\")\n",
    "        fold_train_losses.append(train_losses)\n",
    "        fold_val_losses.append(val_losses)\n",
    "        fold_cv_scores.append(cv_scores)\n",
    "\n",
    "\n",
    "    # 🔹 保存结果\n",
    "    max_epochs = max(len(x) for x in fold_train_losses)\n",
    "    df_out = pd.DataFrame({\"Epoch\": range(1, max_epochs + 1)})\n",
    "\n",
    "    for i, (train_list, val_list, cv_list) in enumerate(zip(fold_train_losses, fold_val_losses, fold_cv_scores), start=1):\n",
    "        df_out[f\"Train_Loss_Fold{i}\"] = train_list + [None]*(max_epochs-len(train_list))\n",
    "        df_out[f\"Val_Loss_Fold{i}\"]   = val_list   + [None]*(max_epochs-len(val_list))\n",
    "        df_out[f\"CV_Fold{i}\"]         = cv_list    + [None]*(max_epochs-len(cv_list))\n",
    "\n",
    "    out_path = Path(save_dir, \"fold_metrics.xlsx\")\n",
    "    df_out.to_excel(out_path, index=False)\n",
    "    print(f\"✅ 训练日志已保存: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ 模型与训练配置\n",
    "\n",
    "# 1️⃣ 损失权重设置（针对主要目标）\n",
    "weights = {\n",
    "    \"Dry_Green_g\" : 0.1,\n",
    "    \"GDM_g\"       : 0.2,\n",
    "    \"Dry_Total_g\" : 0.5,\n",
    "}\n",
    "\n",
    "# 2️⃣ 模型预测与训练目标列\n",
    "model_target_cols = [\n",
    "    \"Dry_Green_g\",\n",
    "    \"GDM_g\",\n",
    "    \"Dry_Total_g\",\n",
    "]\n",
    "\n",
    "target_cols = [\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Clover_g\",\n",
    "    \"GDM_g\",\n",
    "    \"Dry_Total_g\",\n",
    "]\n",
    "\n",
    "# 3️⃣ 训练超参数配置\n",
    "config = {\n",
    "    \"epochs\"       : 300,\n",
    "    \"freeze_ratio\" : 0.8,\n",
    "    \"batch_size\"   : 32,\n",
    "    \"lr\"           : 1e-4,\n",
    "    \"n_splits\"     : 5,\n",
    "    \"save_interval\":20,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 train.csv        shape: (1785, 9)         列名: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "📊 df               shape: (1785, 10)        列名: ['ID', 'sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "📊 df_targets       shape: (357, 6)          列名: ['ID', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
      "📊 df_meta          shape: (357, 7)          列名: ['ID', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
      "📊 df_train         shape: (357, 12)         列名: ['ID', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n"
     ]
    }
   ],
   "source": [
    "# 📘 数据读取与预处理\n",
    "\n",
    "# 1️⃣ 读取原始数据\n",
    "df_file_path = Path(DIRS[\"dir\"]) / \"train.csv\"\n",
    "df = pd.read_csv(df_file_path)\n",
    "show_df_info(df, \"train.csv\")\n",
    "\n",
    "# 2️⃣ 提取唯一 ID（例如 \"ID1011485656__Dry_Green_g\" → \"ID1011485656\"）\n",
    "df[\"ID\"] = df[\"sample_id\"].str.split(\"__\").str[0]\n",
    "\n",
    "# 3️⃣ 将 ID 列移动到最前面\n",
    "df = move_column_first(df, \"ID\")\n",
    "show_df_info(df, \"df\")\n",
    "\n",
    "# 4️⃣ 目标值透视（行转列）\n",
    "df_targets = (\n",
    "    df\n",
    "    .pivot_table(\n",
    "        index=\"ID\",\n",
    "        columns=\"target_name\",\n",
    "        values=\"target\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_targets.columns.name = None  # 去掉多级列名层次\n",
    "show_df_info(df_targets, \"df_targets\")\n",
    "\n",
    "# 5️⃣ 提取元信息（每个 ID 仅保留一行）\n",
    "meta_cols = [\n",
    "    \"ID\", \"image_path\", \"Sampling_Date\", \"State\",\n",
    "    \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"\n",
    "]\n",
    "df_meta = df[meta_cols].drop_duplicates(subset=\"ID\")\n",
    "show_df_info(df_meta, \"df_meta\")\n",
    "\n",
    "# 6️⃣ 合并元信息与目标数据\n",
    "df_train = pd.merge(df_meta, df_targets, on=\"ID\", how=\"left\")\n",
    "show_df_info(df_train, \"df_train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练部分 本地运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-01 18-29-10\n",
      "✅ 配置文件已保存到: D:\\DATA_hao\\Kaggle_\\csiro-biomass\\DualStream_multihead\\2025-11-01 18-29-10\\config.json\n",
      "[18:31:26] [0.33%]🧩 Fold  1/5 Epoch   5/300 | Train=15.8310 | Val=10.5612 | CV=0.0657 | 27.13s/it | ETA≈05:47:194\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 配置文件已保存到: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 🚀 调用主函数\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m fold_train_losses, fold_val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_groupkfold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhistory_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_target_cols\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_target_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_train_transforms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_train_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_valid_transforms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_valid_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_ratio\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfreeze_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_splits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_interval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ 全部训练完成！结果保存在：\u001b[39m\u001b[38;5;124m\"\u001b[39m, history_DIR)\n",
      "Cell \u001b[1;32mIn[24], line 52\u001b[0m, in \u001b[0;36mtrain_with_groupkfold\u001b[1;34m(df_train, save_dir, model_target_cols, get_train_transforms, get_valid_transforms, weights, freeze_ratio, batch_size, epochs, lr, device, n_splits, save_interval)\u001b[0m\n\u001b[0;32m     48\u001b[0m train_losses, val_losses, cv_scores \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 52\u001b[0m     avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     avg_val_loss, weighted_r2 \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, valid_loader, valid_df, device)\n\u001b[0;32m     55\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "Cell \u001b[1;32mIn[22], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, device, scaler)\u001b[0m\n\u001b[0;32m     16\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(img_left, img_right)\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(preds, targets)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# scaler.unscale_(optimizer)  # 可选：如果想加梯度裁剪，可在此解缩放\u001b[39;00m\n\u001b[0;32m     21\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\kaggle\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\kaggle\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 启动训练 🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    print(time_str)\n",
    "\n",
    "    history_DIR = Path(DIRS['model'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "    # 保存当前配置\n",
    "    config_path = history_DIR / \"config.json\"\n",
    "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"✅ 配置文件已保存到: {config_path}\")\n",
    "\n",
    "    # 🚀 调用主函数\n",
    "    fold_train_losses, fold_val_losses = train_with_groupkfold(\n",
    "        df_train             = df_train,\n",
    "        save_dir             = history_DIR,\n",
    "        model_target_cols    = model_target_cols,\n",
    "        get_train_transforms = get_train_transforms,\n",
    "        get_valid_transforms = get_valid_transforms,\n",
    "        weights              = weights,\n",
    "        freeze_ratio         = config[\"freeze_ratio\"],\n",
    "        batch_size           = config[\"batch_size\"],\n",
    "        epochs               = config[\"epochs\"],\n",
    "        lr                   = config[\"lr\"],\n",
    "        device               = device,\n",
    "        n_splits             = config[\"n_splits\"],\n",
    "        save_interval        = config[\"save_interval\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ 全部训练完成！结果保存在：\", history_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测部分 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📘 数据读取与预处理（测试集）\n",
    "\n",
    "# 1️⃣ 读取原始数据\n",
    "df_file_path = Path(DIRS[\"dir\"]) / \"test.csv\"\n",
    "df = pd.read_csv(df_file_path)\n",
    "show_df_info(df, \"test.csv\")\n",
    "\n",
    "# 2️⃣ 提取唯一 ID（例如 \"ID1011485656__Dry_Green_g\" → \"ID1011485656\"）\n",
    "df[\"ID\"] = df[\"sample_id\"].str.split(\"__\").str[0]\n",
    "\n",
    "# 3️⃣ 将 ID 列移动到最前面\n",
    "df = move_column_first(df, \"ID\")\n",
    "\n",
    "# 4️⃣ 初始化目标列（test 集无目标值）\n",
    "df[\"target\"] = 0\n",
    "show_df_info(df, \"df\")\n",
    "\n",
    "# 5️⃣ 目标列透视（行转列结构保持一致）\n",
    "df_targets = (\n",
    "    df\n",
    "    .pivot_table(\n",
    "        index=\"ID\",\n",
    "        columns=\"target_name\",\n",
    "        values=\"target\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "df_targets.columns.name = None  # 去掉多级列名层次\n",
    "show_df_info(df_targets, \"df_targets\")\n",
    "\n",
    "# 6️⃣ 提取元信息（每个 ID 仅保留一行）\n",
    "meta_cols = [\n",
    "    \"ID\",\n",
    "    \"image_path\",\n",
    "]\n",
    "df_meta = df[meta_cols].drop_duplicates(subset=\"ID\")\n",
    "show_df_info(df_meta, \"df_meta\")\n",
    "\n",
    "# 7️⃣ 合并元信息与目标数据\n",
    "df_test = pd.merge(df_meta, df_targets, on=\"ID\", how=\"left\")\n",
    "show_df_info(df_test, \"df_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 model  transform  model_dir  预测\n",
    "\n",
    "def predict_ensemble_df(df_test, transform, model, model_target_cols, model_dir, device, batch_size=32, img_size=768):\n",
    "\n",
    "    model_dir = model_dir\n",
    "    print(f\"模型目录: {model_dir}\")\n",
    "    assert model_dir.exists(), f\"❌ 模型目录不存在: {model_dir}\"\n",
    "\n",
    "    # 🔍 搜索所有 fold 模型\n",
    "    model_paths = sorted(model_dir.glob(\"model_weights_fold*.pt\"))\n",
    "    if not model_paths:\n",
    "        raise FileNotFoundError(f\"❌ 未找到模型文件: {model_dir}/model_weights_fold*.pt\")\n",
    "\n",
    "    print(f\"🔹 检测到 {len(model_paths)} 个模型:\")\n",
    "    for p in model_paths:\n",
    "        print(\"   -\", p.name)\n",
    "\n",
    "    # 3️⃣ 构建测试数据集\n",
    "    test_dataset = DualStreamDataset(\n",
    "        df_test,\n",
    "        image_dir=DIRS[\"dir\"],\n",
    "        target_cols=None,\n",
    "        transform=transform   \n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # 存储每个fold的预测\n",
    "    fold_preds = []\n",
    "\n",
    "    for fold, model_path in enumerate(model_paths):\n",
    "        print(f\"🚀 加载模型 {fold+1}/{len(model_paths)}: {model_path.name}\")\n",
    "\n",
    "        # 1️⃣ 加载模型结构\n",
    "        model = model\n",
    "\n",
    "        # 2️⃣ 加载权重\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        # 3️⃣ 推理\n",
    "        preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for img_left, img_right in test_loader:\n",
    "                img_left, img_right = img_left.to(device, non_blocking=True), img_right.to(device, non_blocking=True)\n",
    "                preds = model(img_left, img_right)\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "        fold_pred = np.concatenate(preds_list, axis=0)\n",
    "        fold_preds.append(fold_pred)\n",
    "\n",
    "    # 4️⃣ 多模型平均\n",
    "    preds_mean = np.mean(fold_preds, axis=0)\n",
    "    df_pred3 = pd.DataFrame(preds_mean, columns=model_target_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 恢复完整的 5 个目标列\n",
    "    df_pred5 = recover_all_targets(df_pred3)\n",
    "    show_df_info(df_pred5, \"df_pred5 \")\n",
    "\n",
    "\n",
    "    # 追加样本 ID 并调整列顺序\n",
    "    df_pred5[\"ID\"] = df_test[\"ID\"]\n",
    "    df_pred5 = df_pred5[[\"ID\"] + target_cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 打印结果预览\n",
    "    show_df_info(df_pred5, \"final df_pred5\")\n",
    "\n",
    "    return df_pred5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 模型加载与 TTA 推理\n",
    "\n",
    "\n",
    "# 1️⃣ 加载模型结构\n",
    "model = MyDualStreamModel(\"convnext_tiny\", \n",
    "                          pretrained=False, \n",
    "                          freeze_ratio=config[\"freeze_ratio\"], \n",
    "                          weights_dict=weights)\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "# 2️⃣ 设置模型目录（根据运行环境自动切换）\n",
    "if socket.gethostname() == \"hao-2\":\n",
    "    model_dir = Path(DIRS[\"model\"] , \"2025-10-31 23-01-43\")\n",
    "else:\n",
    "    model_dir = DIRS[\"model\"]\n",
    "\n",
    "\n",
    "\n",
    "# 3️⃣ 执行 TTA（Test-Time Augmentation）推理\n",
    "tta_preds = []\n",
    "\n",
    "for name, tform in tta_transforms.items():\n",
    "    print(f\"\\n🚀 Running TTA: {name}\")\n",
    "\n",
    "    transform  = tform\n",
    "    df_pred5   = predict_ensemble_df(\n",
    "        df_test           = df_test,\n",
    "        transform         = transform,\n",
    "        model             = model,\n",
    "        model_target_cols = model_target_cols,\n",
    "        model_dir         = model_dir,\n",
    "        device            = device,\n",
    "    )\n",
    "    \n",
    "    # ✅ 输出阶段性结果\n",
    "    print(f\"\\n📄 当前 TTA 模式 [{name}] 的预测结果预览：\")\n",
    "    print(df_pred5.head())\n",
    "\n",
    "    tta_preds.append(df_pred5[target_cols].values)\n",
    "\n",
    "    print(f\"\\n📦 当前已收集的 TTA 结果数量：{len(tta_preds)}\")\n",
    "    print(f\"📊 当前累计结果形状：{np.array(tta_preds).shape}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "# 4️⃣ 汇总 TTA 结果并计算平均预测\n",
    "print(\"\\n📦 聚合全部 TTA 结果：\")\n",
    "print(f\"共有 {len(tta_preds)} 组预测结果。\")\n",
    "for i, arr in enumerate(tta_preds):\n",
    "    print(f\"  └─ 第 {i+1} 组预测: {arr}\")\n",
    "\n",
    "mean_preds = np.mean(tta_preds, axis=0)\n",
    "\n",
    "print(\"\\n🧮 计算平均值完成：\")\n",
    "print(mean_preds)\n",
    "print(f\"\\n✅ 聚合完成，mean_preds 形状：{mean_preds.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5️⃣ 生成最终预测 DataFrame\n",
    "df_pred_final = df_pred5.copy()\n",
    "df_pred_final[target_cols] = mean_preds\n",
    "\n",
    "print(\"\\n🧾 最终预测 DataFrame 预览：\")\n",
    "print(df_pred_final.head())\n",
    "show_df_info(df_pred_final, \"df_pred_final\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📤 5️⃣ 生成 Kaggle 提交文件 submission.csv\n",
    "\n",
    "\n",
    "\n",
    "df = df_pred_final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 按指定顺序展开\n",
    "ordered_target_cols = [\n",
    "    \"Dry_Clover_g\",  # 1️⃣\n",
    "    \"Dry_Dead_g\",    # 2️⃣\n",
    "    \"Dry_Green_g\",   # 3️⃣\n",
    "    \"Dry_Total_g\",   # 4️⃣\n",
    "    \"GDM_g\"          # 5️⃣\n",
    "]\n",
    "\n",
    "df_submit = (\n",
    "    df\n",
    "    .melt(id_vars=\"ID\", value_vars=ordered_target_cols,\n",
    "          var_name=\"target_name\", value_name=\"target\")\n",
    ")\n",
    "\n",
    "# 组合成 Kaggle 所需的 sample_id\n",
    "df_submit[\"sample_id\"] = df_submit[\"ID\"] + \"__\" + df_submit[\"target_name\"]\n",
    "\n",
    "df_submit = move_column_first(df_submit, \"target\")\n",
    "df_submit = move_column_first(df_submit, \"sample_id\")\n",
    "\n",
    "# 只保留 Kaggle 要的两列\n",
    "df_submit = df_submit[[\"sample_id\", \"target\"]]\n",
    "df_submit\n",
    "# 按 sample_id 排序（可选）\n",
    "# df_submit = df_submit.sort_values(\"sample_id\").reset_index(drop=True)\n",
    "\n",
    "# 保存文件\n",
    "df_submit.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ 已生成提交文件 submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8607422,
     "sourceId": 13552583,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
