{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a782e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalon available: True\n",
      "✅ 路径已创建：\n",
      "\n",
      "dir          : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\n",
      "DATA_DIR000  : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\\DATA_DIR000\n",
      "HISTORY      : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\\HISTORY\n",
      "SUBMISSION   : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\\SUBMISSION\n"
     ]
    }
   ],
   "source": [
    "# 系统库\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 第三方科学计算 & 可视化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置中文字体，避免乱码\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False          # 解决负号显示成方块的问题\n",
    "\n",
    "# 机器学习 & 优化\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "# 化学信息学 (RDKit)\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import (\n",
    "    Descriptors, Crippen, rdMolDescriptors,\n",
    "    MACCSkeys, RDKFingerprint, rdFingerprintGenerator\n",
    ")\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "# 关闭 RDKit 的警告\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Avalon 指纹（可选）\n",
    "try:\n",
    "    from rdkit.Avalon import pyAvalonTools\n",
    "    avalon_available = True\n",
    "except ImportError:\n",
    "    avalon_available = False\n",
    "print(f\"Avalon available: {avalon_available}\")\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe_connected\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = r'D:\\数据\\Kaggle\\Thermophysical Property Melting Point'\n",
    "else:\n",
    "    dir = os.getcwd()\n",
    "\n",
    "\n",
    "DIRS = {\n",
    "    \"dir\":              dir,                                       \n",
    "    \"DATA_DIR000\":      os.path.join(dir, \"DATA_DIR000\"),\n",
    "    \"HISTORY\":          os.path.join(dir, \"HISTORY\"),\n",
    "    \"SUBMISSION\":       os.path.join(dir, \"SUBMISSION\"),\n",
    "}\n",
    "\n",
    "# 自动创建目录\n",
    "for key, path in DIRS.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# 打印时一行一个地址\n",
    "print(\"✅ 路径已创建：\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f677d",
   "metadata": {},
   "source": [
    "# 数据提取处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Kaggle 训练集和 Bradley 熔点公开数据集\n",
    "\n",
    "# Kaggle 提供的训练集和测试集\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "# 外部 Bradley 熔点公开数据集\n",
    "bradley_df = pd.read_excel(os.path.join(DIRS['DATA_DIR000'], \"BradleyMeltingPointDataset.xlsx\"))\n",
    "bradleyplus_df = pd.read_excel(os.path.join(DIRS['DATA_DIR000'], \"BradleyDoublePlusGoodMeltingPointDataset.xlsx\"))\n",
    "\n",
    "# 只保留需要的列\n",
    "train_df = train_df[['SMILES', 'Tm']]\n",
    "test_df  = test_df[['id', 'SMILES']]\n",
    "\n",
    "# 输出数据集规模，确认加载成功\n",
    "print(\"Train                        shape:\", train_df.shape)\n",
    "print(\"Test                         shape:\", test_df.shape)\n",
    "print(\"Bradley dataset              shape:\", bradley_df.shape)\n",
    "print(\"Bradley Plus Good dataset    shape:\", bradleyplus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部 Bradley 熔点数据集处理 & 合并Kaggle 训练集\n",
    "\n",
    "# 1. 摄氏度 → 开尔文: T(K) = T(°C) + 273.15\n",
    "bradley_df['Tm']     = bradley_df['mpC'] + 273.15\n",
    "bradleyplus_df['Tm'] = bradleyplus_df['mpC'] + 273.15\n",
    "\n",
    "# 2. 保留 [SMILES, Tm] 并统一列名\n",
    "bradley_df     = bradley_df[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
    "bradleyplus_df = bradleyplus_df[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
    "\n",
    "# 打印原始信息\n",
    "print(f\"📊 Kaggle 训练集    shape    : {train_df.shape}\")\n",
    "print(f\"📊 Bradley          shape    : {bradley_df.shape}\")\n",
    "print(f\"📊 Bradley Plus     shape    : {bradleyplus_df.shape}\")\n",
    "\n",
    "# 3. 合并 Bradley & Bradley Plus\n",
    "bradley_merge = pd.concat([bradley_df, bradleyplus_df], axis=0).reset_index(drop=True)\n",
    "print(f\"📊 Bradley 合并后   shape    : {bradley_merge.shape}\")\n",
    "\n",
    "# 4. 拼接到 Kaggle 训练集\n",
    "merge_df = pd.concat([train_df, bradley_merge], axis=0).reset_index(drop=True)\n",
    "print(f\"📊 拼接后 merge_df  shape    : {merge_df.shape}\")\n",
    "\n",
    "# 5. 去重处理\n",
    "dup_count = merge_df.duplicated(subset=['SMILES', 'Tm']).sum()\n",
    "print(f\"⚠️ 发现重复数据条数          : {dup_count}\")\n",
    "\n",
    "merge_df = merge_df.drop_duplicates(subset=['SMILES', 'Tm']).reset_index(drop=True)\n",
    "print(f\"✅ 去重后 merge_df  shape    : {merge_df.shape}\")\n",
    "\n",
    "# 6. 最终确认\n",
    "print(\"🎯 数据合并 & 去重完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6200fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有分子描述符 (Descriptors)\n",
    "def extract_all_descriptors(df, SMILES_col):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        df         : DataFrame，包含 SMILES 列\n",
    "        SMILES_col : 字符串，SMILES 列的名称\n",
    "    输出:\n",
    "        DataFrame，原始数据 + 208 个分子描述符\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 获取 RDKit 内置的分子描述符\n",
    "    descriptor_list = Descriptors._descList   # [(name, func), ...]\n",
    "    descriptors = [desc[0] for desc in descriptor_list]\n",
    "    print(f\"📊 一共存在 {len(descriptors)} 个分子描述符特征\")\n",
    "\n",
    "    # 2. 遍历每个分子，计算描述符\n",
    "    results = []\n",
    "    total = len(df)\n",
    "    for idx, smi in enumerate(df[SMILES_col]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        if mol is None:\n",
    "            row = {name: None for name, func in descriptor_list}   # 无效 SMILES\n",
    "        else:\n",
    "            row = {name: func(mol) for name, func in descriptor_list}  # 有效 SMILES\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # 打印进度条（覆盖式打印）\n",
    "        print(f\"🔄 处理进度: {idx+1:5d}/{total:5d}\", end=\"\\r\", flush=True)\n",
    "    print(\"\\n✅ 描述符计算完成\")\n",
    "\n",
    "    # 3. 合并原始数据与新特征\n",
    "    df_desc = pd.DataFrame(results)\n",
    "    return pd.concat([df, df_desc], axis=1)\n",
    "\n",
    "\n",
    "# ============ 应用函数 ============\n",
    "merge_df = extract_all_descriptors(merge_df, \"SMILES\")\n",
    "test_df  = extract_all_descriptors(test_df, \"SMILES\")\n",
    "\n",
    "# 删除无效数据 (有 NaN 的行)\n",
    "merge_df = merge_df.dropna().reset_index(drop=True)\n",
    "test_df  = test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ merge_df shape = {merge_df.shape}\")\n",
    "print(f\"✅ test_df shape  = {test_df.shape}\")\n",
    "\n",
    "\n",
    "# # 保存到 CSV\n",
    "# merge_path = os.path.join(DIRS['DATA_DIR000'], \"merge_descriptors.csv\")\n",
    "# test_path  = os.path.join(DIRS['DATA_DIR000'], \"test_descriptors.csv\")\n",
    "# merge_df.to_csv(merge_path, index=False)\n",
    "# test_df.to_csv(test_path, index=False)\n",
    "\n",
    "# print(f\"✅ merge_df shape = {merge_df.shape}，已保存到 {merge_path}\")\n",
    "# print(f\"✅ test_df shape  = {test_df.shape}，已保存到 {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有分子指纹 (Fingerprints)\n",
    "def extract_all_fingerprint(df, SMILES_col, morgan_radius=2, morgan_nbits=1024):\n",
    "    \"\"\"\n",
    "    输入参数:\n",
    "        df            : DataFrame，包含 SMILES 的表格\n",
    "        SMILES_col    : str，SMILES 所在列的列名\n",
    "        morgan_radius : int，Morgan 指纹半径 (默认=2)\n",
    "        morgan_nbits  : int，Morgan/FCFP/AtomPair 指纹长度 (默认=1024)\n",
    "\n",
    "    返回:\n",
    "        DataFrame，原始数据 + 多种分子指纹特征\n",
    "    \"\"\"\n",
    "\n",
    "    fps_data = []   # 存储所有分子的指纹特征字典\n",
    "\n",
    "    # 1. 定义指纹生成器\n",
    "    morgan_gen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=morgan_radius, fpSize=morgan_nbits,\n",
    "        countSimulation=True, includeChirality=False\n",
    "    )\n",
    "    fcfp = rdFingerprintGenerator.GetMorganFeatureAtomInvGen()\n",
    "    fcfp_gen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=morgan_radius, fpSize=morgan_nbits,\n",
    "        atomInvariantsGenerator=fcfp, countSimulation=True, includeChirality=False\n",
    "    )\n",
    "    atom_gen = rdFingerprintGenerator.GetAtomPairGenerator(\n",
    "        fpSize=morgan_nbits, countSimulation=True, includeChirality=False\n",
    "    )\n",
    "\n",
    "    # 2. 遍历分子，提取指纹\n",
    "    total = len(df)\n",
    "    for idx, smi in enumerate(df[SMILES_col]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            fps_data.append({})\n",
    "            print(f\"⚠ 无效 SMILES: {smi}\")\n",
    "            continue\n",
    "\n",
    "        feature_row = {}\n",
    "\n",
    "        # 2.1 Morgan 指纹 (ECFP)\n",
    "        morgan_fp = morgan_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"Morgan_{i}\"] = morgan_fp[i]\n",
    "\n",
    "        # 2.2 功能类 Morgan (FCFP)\n",
    "        fcfp_fp = fcfp_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"FCFP_{i}\"] = fcfp_fp[i]\n",
    "\n",
    "        # 2.3 MACCS Keys (固定 167 位)\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        for i in range(len(maccs_fp)):\n",
    "            feature_row[f\"MACCS_{i}\"] = int(maccs_fp[i])\n",
    "\n",
    "        # 2.4 AtomPair 指纹\n",
    "        atompair_fp = atom_gen.GetCountFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"AtomPair_{i}\"] = atompair_fp[i]\n",
    "\n",
    "        # 2.5 RDKit 内置指纹\n",
    "        rdkit_fp = RDKFingerprint(mol)\n",
    "        for i in range(len(rdkit_fp)):\n",
    "            feature_row[f\"RDKIT_{i}\"] = int(rdkit_fp[i])\n",
    "\n",
    "        # 2.6 Avalon 指纹 (若可用)\n",
    "        if avalon_available:\n",
    "            avalon_fp = pyAvalonTools.GetAvalonFP(mol, morgan_nbits)\n",
    "            for i in range(len(avalon_fp)):\n",
    "                feature_row[f\"Avalon_{i}\"] = int(avalon_fp[i])\n",
    "\n",
    "        fps_data.append(feature_row)\n",
    "        print(f\"🔄 指纹提取进度: {idx+1:5d}/{total:5d}\", end=\"\\r\", flush=True)\n",
    "    print(\"\\n✅ 分子指纹计算完成\")\n",
    "\n",
    "    # 3. 合并结果并返回\n",
    "    fps_df = pd.DataFrame(fps_data)\n",
    "    return pd.concat([df, fps_df], axis=1)\n",
    "\n",
    "\n",
    "# ============ 应用函数 ============\n",
    "merge_df = extract_all_fingerprint(merge_df, \"SMILES\")\n",
    "test_df  = extract_all_fingerprint(test_df, \"SMILES\")\n",
    "\n",
    "print(f\"✅ merge_df shape = {merge_df.shape}\")\n",
    "print(f\"✅ test_df shape  = {test_df.shape}\")\n",
    "\n",
    "# # 保存结果\n",
    "# merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "# test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "# merge_df.to_csv(merge_fp_path, index=False)\n",
    "# test_df.to_csv(test_fp_path, index=False)\n",
    "\n",
    "# print(f\"✅ merge_df shape = {merge_df.shape}，已保存到 {merge_fp_path}\")\n",
    "# print(f\"✅ test_df shape  = {test_df.shape}，已保存到 {test_fp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88ece8",
   "metadata": {},
   "source": [
    "# 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9df3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "def loaddata(DIRS):\n",
    "    # 定义路径\n",
    "    merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "    test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "    # 读取数据\n",
    "    merge_df = pd.read_csv(merge_fp_path)\n",
    "    test_df  = pd.read_csv(test_fp_path)\n",
    "\n",
    "    # 打印信息\n",
    "    print(f\"✅ merge_df 加载完成，shape = {merge_df.shape}\")\n",
    "    print(f\"✅ test_df  加载完成，shape = {test_df.shape}\")\n",
    "\n",
    "    print(\"特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\")\n",
    "    print(\"合计特征总数 = 6528\")\n",
    "\n",
    "    return  merge_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "merge_df, test_df =  loaddata(DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c871162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印清单\n",
    "def config_to_str(config: dict, indent: int = 0) -> str:\n",
    "    \"\"\"递归生成配置字符串\"\"\"\n",
    "    prefix = \"     \" * indent\n",
    "    lines = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}🔹 {key}:\")\n",
    "            lines.append(config_to_str(value, indent + 1))  # 递归拼接子字典\n",
    "        else:\n",
    "            lines.append(f\"{prefix}- {key:<20}: {value}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f737573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置单\n",
    "config = {\n",
    "    # 固定开关\n",
    "    \"ISTEST\"            : True,\n",
    "\n",
    "    \"remove_dup_smiles\" : True, \n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 100,\n",
    "\n",
    "    # 特征选择 XGBoost 参数\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:absoluteerror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"mean\",   \n",
    "\n",
    "    # 训练设置\n",
    "    \"xgb_train_model_params\": {\n",
    "        'max_depth'   : 6,\n",
    "        'eta'         : 0.1,\n",
    "        'tree_method' : 'hist',\n",
    "        'eval_metric' : 'mae',\n",
    "    },\n",
    "    \"num_boost_round\": 15000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85d2f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 检测到 0 个重复 SMILES\n",
      "✅ 删除完成: 从 (28405, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (100, 110)\n",
      "训练集目标   target_train  shape   : (100,)\n",
      "测试集特征  features_test  shape   : (666, 110)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# 数据拆分 (特征矩阵 与 目标向量)\n",
    "# ============================================\n",
    "# 特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
    "# 合计特征总数 = 6528\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_features_and_target(merge_df: pd.DataFrame, test_df: pd.DataFrame, config: dict):\n",
    "    \"\"\"\n",
    "    数据拆分函数：构造训练集和测试集的特征矩阵与目标向量\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1. 检查并处理重复 SMILES\n",
    "    if config[\"remove_dup_smiles\"]:\n",
    "\n",
    "        dup_smiles = set(merge_df['SMILES']) & set(test_df['SMILES'])\n",
    "        print(f\"⚠️ 检测到 {len(dup_smiles)} 个重复 SMILES\")\n",
    "\n",
    "        before_shape = merge_df.shape\n",
    "        # 删除训练集中出现在测试集的 SMILES，避免数据泄漏\n",
    "        merge_df = merge_df[~merge_df['SMILES'].isin(test_df['SMILES'])].reset_index(drop=True)\n",
    "        after_shape = merge_df.shape\n",
    "\n",
    "        print(f\"✅ 删除完成: 从 {before_shape} → {after_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 构造特征矩阵和目标向量\n",
    "    features_train = merge_df.drop(columns=['SMILES', 'Tm'])   # 训练集特征 (X)\n",
    "    target_train   = merge_df['Tm']                            # 训练集目标 (y, 熔点)\n",
    "    features_test  = test_df.drop(columns=['SMILES', 'id'])    # 测试集特征 (无 Tm)\n",
    "\n",
    "\n",
    "    # 随机选取部分特征（示例：50 个）\n",
    "    if config[\"ISTEST\"]:\n",
    "        np.random.seed(42)\n",
    "        selected_features = np.random.choice(\n",
    "            merge_df.drop(columns=['SMILES', 'Tm']).columns,\n",
    "            size=110,\n",
    "            replace=False\n",
    "        )\n",
    "        sample_len = 100\n",
    "        features_train = merge_df.iloc[:sample_len][selected_features]   # 训练特征 (前 1000 条)\n",
    "        target_train = merge_df.iloc[:sample_len]['Tm']               # 训练目标\n",
    "        features_test = test_df[selected_features]          # 测试特征 (同样的特征列)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 打印维度信息\n",
    "    print(\"📊 数据拆分完成\")\n",
    "    print(f\"训练集特征 features_train  shape   : {features_train.shape}\")\n",
    "    print(f\"训练集目标   target_train  shape   : {target_train.shape}\")\n",
    "    print(f\"测试集特征  features_test  shape   : {features_test.shape}\")\n",
    "    print(f\"           features_train  类型    : {type(features_train)}\")\n",
    "\n",
    "    return features_train, target_train, features_test\n",
    "\n",
    "features_train, target_train, features_test = prepare_features_and_target(merge_df, test_df, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2724c74",
   "metadata": {},
   "source": [
    "### 非零占比分布的直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2af6143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_nonzero_ratio_hist(features_train: pd.DataFrame, features_test: pd.DataFrame, bins_size: int = 10):\n",
    "    \"\"\"\n",
    "    绘制并比较训练集和测试集每列非零占比分布的直方图\n",
    "\n",
    "    参数:\n",
    "        features_train : pd.DataFrame\n",
    "            训练集特征矩阵\n",
    "        features_test  : pd.DataFrame\n",
    "            测试集特征矩阵\n",
    "        bins_size : int, 默认=10\n",
    "            分箱数量 (0%~100% 区间划分)\n",
    "\n",
    "    返回:\n",
    "        train_counts, test_counts : np.ndarray\n",
    "            两个数据集在各区间内的列数\n",
    "    \"\"\"\n",
    "    # 逐列非零占比（百分比形式）\n",
    "    train_ratio = features_train.apply(lambda col: (col != 0).mean() * 100)\n",
    "    test_ratio  = features_test.apply(lambda col: (col != 0).mean() * 100)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # 绘制直方图 (density=True 表示频率形式)\n",
    "    counts1, bins1, _ = plt.hist(train_ratio, bins=bins_size, alpha=0.6, \n",
    "                                 label=\"features_train\", density=True)\n",
    "    counts2, bins2, _ = plt.hist(test_ratio, bins=bins_size, alpha=0.6, \n",
    "                                 label=\"features_test\", density=True)\n",
    "\n",
    "    # 分别计算数量（而不是频率）\n",
    "    train_counts, _ = np.histogram(train_ratio, bins=bins1)\n",
    "    test_counts, _ = np.histogram(test_ratio, bins=bins2)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"features_train 各区间数量：\")\n",
    "    for i in range(len(bins1)-1):\n",
    "        print(f\"{bins1[i]:.0f}% - {bins1[i+1]:.0f}% : {train_counts[i]} 列\")\n",
    "\n",
    "    print(\"\\nfeatures_test 各区间数量：\")\n",
    "    for i in range(len(bins2)-1):\n",
    "        print(f\"{bins2[i]:.0f}% - {bins2[i+1]:.0f}% : {test_counts[i]} 列\")\n",
    "\n",
    "    # 在柱子上标注数量\n",
    "    for c, b in zip(train_counts, bins1[:-1]):\n",
    "        if c > 0:\n",
    "            plt.text(b + (bins1[1]-bins1[0])/2, 0.01, str(c), \n",
    "                     ha=\"center\", va=\"bottom\", fontsize=8, color=\"black\", rotation=90)\n",
    "\n",
    "    for c, b in zip(test_counts, bins2[:-1]):\n",
    "        if c > 0:\n",
    "            plt.text(b + (bins2[1]-bins2[0])/2, 0.03, str(c), \n",
    "                     ha=\"center\", va=\"bottom\", fontsize=8, color=\"blue\", rotation=90)\n",
    "\n",
    "    plt.xlabel(\"非零占比 (%)\")\n",
    "    plt.ylabel(\"频率 (Frequency, 0~1)\")\n",
    "    plt.title(\"各列非零占比分布直方图\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return train_counts, test_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c396b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_counts, test_counts = plot_nonzero_ratio_hist(features_train, features_test, bins_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f45be",
   "metadata": {},
   "source": [
    "### 特征生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "905d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_chemical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    基于分子描述符构造新的衍生特征\n",
    "    输入:\n",
    "        df : pd.DataFrame，必须包含以下列：\n",
    "            ['NumHDonors', 'NumHAcceptors', 'MolLogP', 'TPSA',\n",
    "            'NumRotatableBonds', 'MolWt', 'NumAromaticRings', 'BertzCT']\n",
    "    输出:\n",
    "        df_new : pd.DataFrame，包含新增特征\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['HBond_Product']        = df['NumHDonors'] * df['NumHAcceptors']\n",
    "    df['HBond_Sum']            = df['NumHDonors'] + df['NumHAcceptors']\n",
    "    df['LogP_div_TPSA']        = df['MolLogP'] / (df['TPSA'] + 1)\n",
    "    df['LogP_x_TPSA']          = df['MolLogP'] * df['TPSA']\n",
    "    df['Flexibility_Score']    = df['NumRotatableBonds'] / (df['MolWt'] + 1)\n",
    "    df['MolWt_x_AromaticRings']= df['MolWt'] * df['NumAromaticRings']\n",
    "    df['Complexity_per_MW']    = df['BertzCT'] / (df['MolWt'] + 1)\n",
    "    df['Rigidity_Score']       = df['NumAromaticRings'] / (df['NumRotatableBonds'] + 1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "280348cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 110), (100, 110))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config[\"use_feature_gen\"]:\n",
    "    features_train = add_chemical_features(features_train)\n",
    "    features_test  = add_chemical_features(features_test)\n",
    "\n",
    "features_train.shape, features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4582b1",
   "metadata": {},
   "source": [
    "### PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b473df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "def apply_truncated_svd(df: pd.DataFrame, n_components: int = 100, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    使用 TruncatedSVD 对 DataFrame 进行降维\n",
    "    输入:\n",
    "        df           : pd.DataFrame，特征矩阵（需去掉 ID / label 等非特征列）\n",
    "        n_components : int，降维后的目标维度\n",
    "        random_state : int，随机种子\n",
    "    输出:\n",
    "        reduced_df   : pd.DataFrame，降维后的结果，保持原行索引\n",
    "    \"\"\"\n",
    "    # 转换为稀疏矩阵\n",
    "    X_sparse = sparse.csr_matrix(df.values)\n",
    "\n",
    "    # 初始化 SVD\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    # 训练并降维\n",
    "    X_reduced_array = svd.fit_transform(X_sparse)\n",
    "\n",
    "    # 包装为 DataFrame\n",
    "    reduced_df = pd.DataFrame(\n",
    "        X_reduced_array,\n",
    "        index=df.index,\n",
    "        columns=[f\"SVD_{i+1}\" for i in range(X_reduced_array.shape[1])]\n",
    "    )\n",
    "    # 方差解释率\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "\n",
    "    # 打印信息\n",
    "    print( \"原始维度         : \", df.shape)\n",
    "    print( \"降维后           : \", reduced_df.shape) \n",
    "    print(f\"累计解释方差比   :  {explained_var:.2%}\")\n",
    "\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "717978bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始维度         :  (100, 110)\n",
      "降维后           :  (100, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 110)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100, 210), (100, 210))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对数据降维\n",
    "if config[\"use_pca\"]:\n",
    "    features_train_reduced = apply_truncated_svd(features_train, n_components = 100)\n",
    "    features_test_reduced = apply_truncated_svd(features_test, n_components = 100)\n",
    "\n",
    "    features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "    features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "features_train.shape, features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f752",
   "metadata": {},
   "source": [
    "# 单次训练推导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b691bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold + XGBoost 进行训练验证，并保存实验结果\n",
    "# ==============================================================\n",
    "def run_kfold_xgb(features_train, target_train, features_test, config, DIRS, K_FOLDS=10, verbose=0):\n",
    "    \"\"\"\n",
    "    使用 Stratified K-Fold + XGBoost 进行训练验证，并保存实验结果\n",
    "\n",
    "    参数:\n",
    "        features_train, target_train        : 训练集特征和标签\n",
    "        features_test      : 测试集特征\n",
    "        params      : XGBoost 最优参数 (dict)\n",
    "        DIRS        : 保存结果的目录字典\n",
    "        K_FOLDS     : 折数 (默认=5)\n",
    "        verbose     : 是否打印详细信息\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "    config[\"X shape\"] = features_train.shape\n",
    "    config[\"y shape\"] = target_train.shape\n",
    "    config[\"X_test shape\"] = features_test.shape\n",
    "\n",
    "\n",
    "    # ---------- 创建目录 ----------\n",
    "    for _, path in DIRS.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = os.path.join(DIRS['HISTORY'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"——\" * 20)\n",
    "    print(f\"✅ 当前结果将保存到: {time_str}\")\n",
    "\n",
    "\n",
    "    # ---------- 定义交叉验证 ----------\n",
    "    skfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    yeo = PowerTransformer(method=\"yeo-johnson\")                                # 定义 Yeo-Johnson 变换\n",
    "\n",
    "    # ---------- 初始化存储 ----------\n",
    "    oof_val = np.zeros(len(features_train))       # OOF 预测\n",
    "    train_score, val_score = [], []  # 每折 MAE\n",
    "    test_pred = []                   # 每折 test 预测\n",
    "    fold_records = []                # 保存每折信息\n",
    "    all_importances = []             # 特征重要性\n",
    "    elapsed_list = []                # 耗时记录\n",
    "\n",
    "\n",
    "\n",
    "    # 循环每一折\n",
    "    # ==============================================================\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skfold.split(features_train, pd.qcut(target_train, q=10).cat.codes), 1):\n",
    "\n",
    "        # ----- 打印时间信息 -----\n",
    "        start_now = datetime.now()\n",
    "        start_str = start_now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if elapsed_list:\n",
    "            avg_time = np.mean(elapsed_list)\n",
    "            est_end = start_now + timedelta(seconds=avg_time)\n",
    "\n",
    "            # 每 5 个一组输出耗时\n",
    "            parts = [f\"{t:6.1f}s\" for t in elapsed_list]\n",
    "            grouped = [\" \".join(parts[j:j+5]) for j in range(0, len(parts), 5)]\n",
    "            elapsed_str = \" /// \".join(grouped)\n",
    "\n",
    "            print(\n",
    "                f\"🔄{i:2d}/{K_FOLDS} ST {start_str}\"\n",
    "                f\" ET {est_end.strftime('%H:%M:%S')}\"\n",
    "                f\" avg {avg_time:.1f}s\"\n",
    "                f\" [{elapsed_str}]\",\n",
    "                end=\"\\r\", flush=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"🔄{i:2d}/{K_FOLDS} ST {start_str} ET (暂无历史数据)\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "        # ----- 开始训练 -----\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. 数据集划分\n",
    "        x_train, x_val = features_train.iloc[train_idx], features_train.iloc[val_idx]\n",
    "        y_train, y_val = target_train[train_idx], target_train[val_idx]\n",
    "\n",
    "        # 2. Yeo-Johnson 变换\n",
    "        y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "        y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "\n",
    "        # 3. 特征选择（轻量级 XGBoost）\n",
    "        # 使用\n",
    "        selector_model = xgb.XGBRegressor(**config[\"xgb_selector_model_params\"])\n",
    "        # selector_model = xgb.XGBRegressor(\n",
    "        #     n_estimators   = 500,\n",
    "        #     max_depth      = 6,\n",
    "        #     learning_rate  = 0.05,\n",
    "        #     random_state   = 2025,\n",
    "        #     device         = \"cpu\",\n",
    "        #     objective      = \"reg:absoluteerror\",\n",
    "        #     tree_method    = \"hist\",\n",
    "        #     verbosity      = 0\n",
    "        # )\n",
    "        \n",
    "        \n",
    "\n",
    "        selector_model.fit(x_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(selector_model, prefit=True, threshold=config[\"selector_threshold\"])\n",
    "        selected_features = x_train.columns[selector.get_support()].tolist()\n",
    "        if verbose > 0:\n",
    "            print(f\"✅ 选择的特征数量: {len(selected_features)}\")\n",
    "\n",
    "\n",
    "        # 4. 保留重要特征\n",
    "        x_train_new = x_train[selected_features]\n",
    "        x_val_new   = x_val[selected_features]\n",
    "        x_test_new  = features_test[selected_features]\n",
    "\n",
    "        # 5. 转换为 DMatrix\n",
    "        dtrain = xgb.DMatrix(x_train_new, y_train, feature_names=selected_features)\n",
    "        dval   = xgb.DMatrix(x_val_new,   y_val,   feature_names=selected_features)\n",
    "        dtest  = xgb.DMatrix(x_test_new,             feature_names=selected_features)\n",
    "\n",
    "\n",
    "        # 6. XGBoost 训练\n",
    "        xgb_model = xgb.train(\n",
    "            params                 = config[\"xgb_train_model_params\"],\n",
    "            dtrain                 = dtrain,\n",
    "            num_boost_round        = config[\"num_boost_round\"],\n",
    "            evals                  = [(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds  = 300,\n",
    "            verbose_eval           = (1000 if verbose > 0 else False)\n",
    "        )\n",
    "\n",
    "\n",
    "        # 保存模型\n",
    "        model_path = os.path.join(history_DIR, f\"xgb_model_fold{i}.json\")\n",
    "        xgb_model.save_model(model_path)\n",
    "\n",
    "        # 7. 获取特征重要性\n",
    "        imp_dict = xgb_model.get_score(importance_type=\"gain\")\n",
    "        imp_df = pd.DataFrame(imp_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "        imp_df[\"Fold\"] = i\n",
    "        all_importances.append(imp_df)\n",
    "\n",
    "\n",
    "        # 8. 预测\n",
    "        y_train_pred = xgb_model.predict(dtrain)\n",
    "        y_val_pred   = xgb_model.predict(dval)\n",
    "        y_test_pred  = xgb_model.predict(dtest)\n",
    "\n",
    "        # 9. 逆变换\n",
    "        y_train      = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "        y_val        = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "        y_train_pred = yeo.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
    "        y_val_pred   = yeo.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
    "        y_test_pred  = yeo.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        # 10. 计算 MAE\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        val_mae   = mean_absolute_error(y_val,   y_val_pred)\n",
    "        if verbose > 0:\n",
    "            print(f\"Fold {i}: Train MAE={train_mae:.4f}, Val MAE={val_mae:.4f}，用时 {elapsed:.2f} 秒\")\n",
    "\n",
    "\n",
    "        # ----- 保存结果 -----\n",
    "        train_score.append(train_mae)\n",
    "        val_score.append(val_mae)\n",
    "        oof_val[val_idx] = y_val_pred\n",
    "        test_pred.append(y_test_pred)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        elapsed_list.append(elapsed)\n",
    "\n",
    "        fold_records.append({\n",
    "            \"Fold\": i,\n",
    "            \"Train_MAE\": train_mae,\n",
    "            \"Val_MAE\": val_mae,\n",
    "            \"Num_Features\": len(selected_features),\n",
    "            \"Selected_Features\": selected_features,\n",
    "            \"elapsed\": elapsed\n",
    "        })\n",
    "\n",
    "    # 保存整体结果\n",
    "    # ==============================================================\n",
    "    if verbose > 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"📊 Train MAE 平均值 : {np.mean(train_score):.4f}\")\n",
    "        print(f\"📊 Val   MAE 平均值 : {np.mean(val_score):.4f}\")\n",
    "        print(f\"📊 Train MAE 标准差 : {np.std(train_score, ddof=0):.4f}\")\n",
    "        print(f\"📊 Val   MAE 标准差 : {np.std(val_score, ddof=0):.4f}\")\n",
    "\n",
    "    # 参数\n",
    "    with open(os.path.join(history_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # 每折信息\n",
    "    folds_df = pd.DataFrame(fold_records)\n",
    "    folds_df.to_csv(os.path.join(history_DIR, \"folds_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # 特征重要性\n",
    "    if all_importances:\n",
    "        valid_imps = [df for df in all_importances if not df.empty]\n",
    "        all_imp_df = pd.concat(valid_imps, axis=0) if valid_imps else pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    else:\n",
    "        all_imp_df = pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    all_imp_df.to_csv(os.path.join(history_DIR, \"feature_importance_all.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # 测试集预测\n",
    "    test_pred_array = np.vstack(test_pred).T\n",
    "    test_pred_df = pd.DataFrame(test_pred_array, columns=[f\"Fold_{j+1}\" for j in range(test_pred_array.shape[1])])\n",
    "    test_pred_df[\"Final_Pred\"] = test_pred_df.mean(axis=1)\n",
    "    test_pred_df.to_csv(os.path.join(history_DIR, \"test_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 总结\n",
    "    with open(os.path.join(history_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train MAE Mean : {np.mean(train_score):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Mean : {np.mean(val_score):.4f}\\n\")\n",
    "        f.write(f\"Train MAE Std  : {np.std(train_score, ddof=0):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Std  : {np.std(val_score, ddof=0):.4f}\\n\")\n",
    "\n",
    "\n",
    "    # 最终提交\n",
    "    final_score = np.mean(val_score)\n",
    "    submission = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"sample_submission.csv\"))\n",
    "    submission[\"Tm\"] = test_pred_df[\"Final_Pred\"]\n",
    "\n",
    "    submission_path = os.path.join(history_DIR, f\"sub_{time_str}_{final_score:.8f}.csv\")\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    submission.to_csv(os.path.join(DIRS['SUBMISSION'], f\"sub_{time_str}_{final_score:.8f}.csv\"), index=False)\n",
    "\n",
    "        \n",
    "    config[\"time_str\"] = time_str\n",
    "    config[\"score\"] = final_score\n",
    "\n",
    "\n",
    "    # ---------- 返回结果 ----------\n",
    "    return {\n",
    "        \"oof_val\": oof_val,\n",
    "        \"train_score\": train_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_pred\": test_pred_df,\n",
    "        \"folds_info\": folds_df,\n",
    "        \"feature_importance\": all_imp_df,\n",
    "        \"submission_path\": submission_path,\n",
    "        \"time\": time_str,\n",
    "        \"final_score\": final_score,\n",
    "        \"config\": config\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "292d721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 210) (666, 210)\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 00-12-29\n",
      "🔄 1/10 ST 00:12:29 ET (暂无历史数据)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning:\n",
      "\n",
      "The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄10/10 ST 00:12:39 ET 00:12:40 avg 1.1s [   1.1s    1.1s    1.2s    1.1s    1.1s ///    1.1s    1.1s    1.2s    1.1s]\n",
      " 53.25167077636718\n"
     ]
    }
   ],
   "source": [
    "# 执行一次\n",
    "\n",
    "X = features_train\n",
    "y = target_train\n",
    "X_test = features_test\n",
    "print(X.shape, X_test.shape)\n",
    "\n",
    "\n",
    "results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "config = results['config']\n",
    "\n",
    "print('\\n',results['final_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a9113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : True\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : False\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 6\n",
      "     - eta                 : 0.1\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "- X shape             : (100, 210)\n",
      "- y shape             : (100,)\n",
      "- X_test shape        : (666, 210)\n",
      "- time_str            : 2025-10-22 00-07-23\n",
      "- score               : 53.25167077636718\n"
     ]
    }
   ],
   "source": [
    "# 打印当前config\n",
    "print(config_to_str(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3573b32",
   "metadata": {},
   "source": [
    "# 提交 kaggle 平台测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "118f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据 submission_time 定位文件路径 提交 kaggle 平台测试\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def find_submission_file(submission_time, submission_dir):\n",
    "    \"\"\"\n",
    "    在 submission_dir 下查找包含 submission_time 的文件\n",
    "    一旦找到立刻返回完整路径；如果没找到则返回 None\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(submission_dir):\n",
    "        if submission_time in fname:\n",
    "            file_path = os.path.join(submission_dir, fname)\n",
    "            print(f\"✅ 找到目标文件: {fname}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(f\"⚠️ 未找到包含 {submission_time} 的文件\")\n",
    "    return None\n",
    "\n",
    "def submit_and_get_score(file_path, competition_name, message=\"My submission\"):\n",
    "    \"\"\"\n",
    "    封装 Kaggle 提交并等待结果评分\n",
    "    --------------------------------------\n",
    "    file_path        : str  提交文件路径\n",
    "    competition_name : str  Kaggle 比赛名称 (URL 最后一段)\n",
    "    message          : str  提交备注\n",
    "    \"\"\"\n",
    "    # 1. 配置 Kaggle API\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = r\"C:\\Users\\Admin\\.kaggle\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"✅ Kaggle API 已经配置成功！\")\n",
    "\n",
    "    # 2. 提交文件\n",
    "    api.competition_submit(\n",
    "        file_name=file_path,\n",
    "        competition=competition_name,\n",
    "        message=message\n",
    "    )\n",
    "    print(\"✅ 提交完成！请等待评分...\")\n",
    "\n",
    "    # 3. 动态等待\n",
    "    spinner = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
    "    while True:\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        latest = submissions[0]\n",
    "        status_str = str(latest._status).lower()\n",
    "\n",
    "        if \"complete\" in status_str and latest._public_score is not None:\n",
    "            print(\"\\n🎯 最终结果:\")\n",
    "            print(f\"Public 分数 : {latest._public_score}\")\n",
    "            print(f\"Private 分数: {latest._private_score}\")\n",
    "            print(f\"提交 ID     : {latest._ref}\")\n",
    "            print(f\"文件名      : {latest._file_name}\")\n",
    "            print(f\"状态        : {latest._status}\")\n",
    "            print(f\"提交时间    : {latest._date}\")\n",
    "            print(f\"描述/备注   : {latest._description}\")\n",
    "            return latest\n",
    "\n",
    "        spin_char = next(spinner)\n",
    "        print(f\"当前状态: {status_str} , 等待中 {spin_char}\", end=\"\\r\", flush=True)\n",
    "        time.sleep(0.2)  # 每 0.5 秒检查一次\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb1c16",
   "metadata": {},
   "source": [
    "### 不轻易运行，再三考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fdb8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该提交文件的参数：\n",
      "- ISTEST              : True\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : False\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 6\n",
      "     - eta                 : 0.1\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "- X shape             : (100, 210)\n",
      "- y shape             : (100,)\n",
      "- X_test shape        : (666, 210)\n",
      "- time_str            : 2025-10-22 00-07-23\n",
      "- score               : 53.25167077636718 \n",
      "⚠️ 未找到包含 2025-10-21 23-51-09 的文件\n"
     ]
    }
   ],
   "source": [
    "# submission_time 提交\n",
    "submission_time = \"2025-10-21 23-51-09\"\n",
    "competition_name = \"melting-point\"\n",
    "message =  f\"该提交文件的参数：\\n{config_to_str(config)} \"\n",
    "print(message)\n",
    "\n",
    "target_file = find_submission_file(submission_time, DIRS['SUBMISSION'] )\n",
    "\n",
    "# submit_and_get_score(target_file, competition_name, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9607e",
   "metadata": {},
   "source": [
    "# 参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "740d9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置单\n",
    "base_config  = {\n",
    "    # 固定开关\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"remove_dup_smiles\" : True, \n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 100,\n",
    "\n",
    "    # 特征选择 XGBoost 参数\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:absoluteerror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"mean\",   \n",
    "\n",
    "    # 训练设置\n",
    "    \"xgb_train_model_params\": {\n",
    "        'max_depth'   : 6,\n",
    "        'eta'         : 0.1,\n",
    "        'tree_method' : 'hist',\n",
    "        'eval_metric' : 'mae',\n",
    "    },\n",
    "    \"num_boost_round\": 15000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a3a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化任务  加入标识符 host: hao-2   ip: 192.168.40.1\n",
    "\n",
    "import copy\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna 的目标函数 (Objective Function)\n",
    "    每次 trial 会生成一组超参数，用于训练 XGBoost 模型，\n",
    "    并返回交叉验证的平均 RMSE 作为优化目标。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. 定义 超参数 搜索空间\n",
    "    # 拷贝一份 config，避免全局污染\n",
    "    config = copy.deepcopy(base_config)\n",
    "\n",
    "    # 只修改需要优化的参数\n",
    "    config[\"remove_dup_smiles\"] = trial.suggest_categorical(\"remove_dup_smiles\", [True, False])\n",
    "    config[\"use_feature_gen\"]   = trial.suggest_categorical(\"use_feature_gen\", [True, False])\n",
    "    config[\"use_pca\"]           = trial.suggest_categorical(\"use_pca\", [True, False])\n",
    "\n",
    "    # config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_int(\"selector_random_state\", 1, 9999)\n",
    "    config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_categorical(\"selector_random_state\", [42, 2025])\n",
    "    config[\"xgb_selector_model_params\"][\"device\"]       = trial.suggest_categorical(\"selector_device\", [\"cpu\", \"cuda\"])\n",
    "    # config[\"xgb_selector_model_params\"][\"tree_method\"]  = trial.suggest_categorical(\"selector_tree_method\", [\"hist\", \"approx\"])\n",
    "\n",
    "    config[\"selector_threshold\"] = trial.suggest_categorical(\"selector_threshold\", [\"mean\", \"0.75*mean\", \"0.5*mean\", \"1.25*mean\"])\n",
    "\n",
    "    config[\"xgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\"train_max_depth\", 3, 12)\n",
    "    config[\"xgb_train_model_params\"][\"eta\"] = trial.suggest_float(\"train_eta\", 0.01 , 0.3 , log=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 主流程---------------------------------------------------------------------------------------------------\n",
    "    # 创建一个黑洞缓冲区\n",
    "    f = io.StringIO()\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        None\n",
    "\n",
    "    # 打印当前config\n",
    "    print(config_to_str(config))\n",
    "    \n",
    "\n",
    "    # 加载数据\n",
    "    merge_df, test_df =  loaddata(DIRS)\n",
    "\n",
    "    # 数据拆分\n",
    "    print(\"数据拆分---------------------------\")\n",
    "    features_train, target_train, features_test = prepare_features_and_target(merge_df, test_df, config)\n",
    "\n",
    "    # 特征生成\n",
    "    if config[\"use_feature_gen\"]:\n",
    "        print(\"特征生成---------------------------\")\n",
    "        features_train = add_chemical_features(features_train)\n",
    "        features_test  = add_chemical_features(features_test)\n",
    "        print(features_train.shape, features_test.shape)\n",
    "\n",
    "    # 数据降维\n",
    "    if config[\"use_pca\"]:\n",
    "        print(\"数据降维---------------------------\")\n",
    "        features_train_reduced = apply_truncated_svd(features_train, n_components = 100)\n",
    "        features_test_reduced = apply_truncated_svd(features_test, n_components = 100)\n",
    "\n",
    "        features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "        features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "        print(features_train.shape, features_test.shape)\n",
    "\n",
    "    X, y, X_test = features_train, target_train, features_test\n",
    "    print(\"开始训练---------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "    config = results['config']\n",
    "    score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "    HOSTNAME = socket.gethostname()\n",
    "    HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "    trial.set_user_attr(\"host\", HOSTNAME)        # 你自己定义主机 A/B\n",
    "    trial.set_user_attr(\"ip\", HOST_IP)        # 你自己定义角色 A/B\n",
    "\n",
    "    # 4. 返回平均 MAE\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff8aec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"test\" if base_config[\"ISTEST\"] else \"optuna_task1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66f848ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 01:01:07,374] Using an existing study with name 'optuna_task1' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主机名: hao-2  主机 IP: 192.168.40.1\n",
      "🔎 开始超参数搜索...\n",
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 6\n",
      "     - eta                 : 0.01368700824230773\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28405, 6536) (666, 6536)\n",
      "数据降维---------------------------\n",
      "原始维度         :  (28405, 6536)\n",
      "降维后           :  (28405, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 6536)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "(28405, 6636) (666, 6636)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 01-01-26\n",
      "🔄10/10 ST 02:35:14 ET 02:45:39 avg 625.3s [ 603.4s  613.6s  632.2s  626.2s  629.3s ///  629.7s  635.6s  634.7s  623.2s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 02:45:55,299] Trial 8 finished with value: 17.849929328445675 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.01368700824230773}. Best is trial 8 with value: 17.849929328445675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : False\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cuda\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : 0.75*mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 4\n",
      "     - eta                 : 0.05053010910637573\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28808, 6528)\n",
      "训练集目标   target_train  shape   : (28808,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28808, 6536) (666, 6536)\n",
      "数据降维---------------------------\n",
      "原始维度         :  (28808, 6536)\n",
      "降维后           :  (28808, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 6536)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "(28808, 6636) (666, 6636)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 02-46-14\n",
      "🔄10/10 ST 04:09:05 ET 04:18:18 avg 552.4s [ 564.9s  558.0s  557.8s  550.5s  554.2s ///  536.8s  552.2s  546.1s  551.2s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 04:18:13,600] Trial 9 finished with value: 17.995321574148264 and parameters: {'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.05053010910637573}. Best is trial 8 with value: 17.849929328445675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : False\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : 0.5*mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 4\n",
      "     - eta                 : 0.026729074261343022\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "数据降维---------------------------\n",
      "原始维度         :  (28405, 6528)\n",
      "降维后           :  (28405, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 6528)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "(28405, 6628) (666, 6628)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 04-18-32\n",
      "🔄10/10 ST 05:48:30 ET 05:58:29 avg 599.7s [ 614.2s  592.1s  605.9s  597.5s  609.2s ///  603.8s  590.5s  596.8s  587.1s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 05:58:28,065] Trial 10 finished with value: 18.924810628669103 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.026729074261343022}. Best is trial 8 with value: 17.849929328445675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : False\n",
      "- use_feature_gen     : False\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : 0.75*mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 12\n",
      "     - eta                 : 0.0955937800637472\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28808, 6528)\n",
      "训练集目标   target_train  shape   : (28808,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 05-58-38\n",
      "🔄10/10 ST 06:16:26 ET 06:18:25 avg 118.7s [ 197.3s  106.8s  111.3s  109.8s  109.1s ///  107.1s  109.4s  108.4s  109.3s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 06:18:13,573] Trial 11 finished with value: 18.474602442284255 and parameters: {'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 12, 'train_eta': 0.0955937800637472}. Best is trial 8 with value: 17.849929328445675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : False\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 5\n",
      "     - eta                 : 0.01844230194954457\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 06-18-24\n",
      "🔄10/10 ST 07:36:47 ET 07:45:29 avg 522.5s [ 516.5s  518.2s  528.5s  532.6s  518.8s ///  524.0s  524.7s  520.0s  519.0s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 07:45:30,510] Trial 12 finished with value: 18.843924579801143 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 5, 'train_eta': 0.01844230194954457}. Best is trial 8 with value: 17.849929328445675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 11\n",
      "     - eta                 : 0.013329388592334063\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28405, 6536) (666, 6536)\n",
      "数据降维---------------------------\n",
      "原始维度         :  (28405, 6536)\n",
      "降维后           :  (28405, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 6536)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "(28405, 6636) (666, 6636)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 07-45-50\n",
      "🔄10/10 ST 09:00:21 ET 09:08:38 avg 496.7s [ 502.8s  474.8s  494.3s  481.8s  488.7s ///  547.0s  493.1s  486.9s  501.1s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 09:13:17,184] Trial 13 finished with value: 18.582485674397393 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 11, 'train_eta': 0.013329388592334063}. Best is trial 8 with value: 17.849929328445675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : False\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 8\n",
      "     - eta                 : 0.013678962412610542\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28808, 6528)\n",
      "训练集目标   target_train  shape   : (28808,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28808, 6536) (666, 6536)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 09-13-28\n",
      "🔄10/10 ST 10:55:31 ET 11:06:51 avg 680.3s [ 647.0s  675.9s  692.2s  673.8s  694.7s ///  679.5s  695.6s  670.1s  694.1s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 11:06:58,744] Trial 14 finished with value: 17.556320884921817 and parameters: {'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.013678962412610542}. Best is trial 14 with value: 17.556320884921817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : False\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : 1.25*mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 9\n",
      "     - eta                 : 0.045732428074001644\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28808, 6528)\n",
      "训练集目标   target_train  shape   : (28808,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28808, 6536) (666, 6536)\n",
      "数据降维---------------------------\n",
      "原始维度         :  (28808, 6536)\n",
      "降维后           :  (28808, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 6536)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "(28808, 6636) (666, 6636)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 11-07-18\n",
      "🔄10/10 ST 11:38:23 ET 11:41:50 avg 207.2s [ 171.9s  438.6s  170.7s  174.8s  181.8s ///  173.0s  204.4s  176.3s  173.2s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 11:41:15,531] Trial 19 finished with value: 18.085312780044305 and parameters: {'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 9, 'train_eta': 0.045732428074001644}. Best is trial 14 with value: 17.556320884921817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : False\n",
      "- use_feature_gen     : False\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : 0.75*mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 4\n",
      "     - eta                 : 0.06743144217230011\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28808, 6528)\n",
      "训练集目标   target_train  shape   : (28808,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 11-41-26\n",
      "🔄10/10 ST 13:06:08 ET 13:15:33 avg 564.7s [ 567.6s  550.5s  581.2s  576.2s  553.3s ///  564.7s  563.5s  563.8s  561.5s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 13:15:15,085] Trial 21 finished with value: 17.975167301336665 and parameters: {'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.06743144217230011}. Best is trial 14 with value: 17.556320884921817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cuda\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 7\n",
      "     - eta                 : 0.01061463707842147\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28405, 6536) (666, 6536)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 13-15-26\n",
      "🔄10/10 ST 14:46:05 ET 14:56:09 avg 604.3s [ 572.1s  581.1s  591.3s  602.1s  614.7s ///  617.0s  613.3s  628.2s  618.5s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 14:56:17,858] Trial 23 finished with value: 18.02550230756041 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01061463707842147}. Best is trial 14 with value: 17.556320884921817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : True\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 42\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 7\n",
      "     - eta                 : 0.02880197004042992\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28405, 6536) (666, 6536)\n",
      "数据降维---------------------------\n",
      "原始维度         :  (28405, 6536)\n",
      "降维后           :  (28405, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "原始维度         :  (666, 6536)\n",
      "降维后           :  (666, 100)\n",
      "累计解释方差比   :  100.00%\n",
      "(28405, 6636) (666, 6636)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 14-56-38\n",
      "🔄10/10 ST 16:10:08 ET 16:18:18 avg 490.0s [ 415.9s  438.5s  447.1s  441.9s  444.2s ///  457.5s  638.9s  651.8s  474.4s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 16:18:13,719] Trial 25 finished with value: 17.762302445492622 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.02880197004042992}. Best is trial 14 with value: 17.556320884921817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : True\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 9\n",
      "     - eta                 : 0.027734152778904956\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "⚠️ 检测到 276 个重复 SMILES\n",
      "✅ 删除完成: 从 (28808, 6530) → (28405, 6530)\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28405, 6528)\n",
      "训练集目标   target_train  shape   : (28405,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28405, 6536) (666, 6536)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 16-18-31\n",
      "🔄10/10 ST 17:20:18 ET 17:27:10 avg 411.9s [ 430.9s  392.4s  463.2s  412.9s  398.1s ///  401.2s  403.4s  406.6s  398.1s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 17:27:23,095] Trial 27 finished with value: 17.725474441857436 and parameters: {'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 9, 'train_eta': 0.027734152778904956}. Best is trial 14 with value: 17.556320884921817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ISTEST              : False\n",
      "- remove_dup_smiles   : False\n",
      "- use_feature_gen     : True\n",
      "- use_pca             : False\n",
      "- pca_components      : 100\n",
      "🔹 xgb_selector_model_params:\n",
      "     - n_estimators        : 500\n",
      "     - max_depth           : 6\n",
      "     - learning_rate       : 0.05\n",
      "     - random_state        : 2025\n",
      "     - device              : cpu\n",
      "     - objective           : reg:absoluteerror\n",
      "     - tree_method         : hist\n",
      "     - verbosity           : 0\n",
      "- selector_threshold  : mean\n",
      "🔹 xgb_train_model_params:\n",
      "     - max_depth           : 10\n",
      "     - eta                 : 0.020728885589285505\n",
      "     - tree_method         : hist\n",
      "     - eval_metric         : mae\n",
      "- num_boost_round     : 15000\n",
      "✅ merge_df 加载完成，shape = (28808, 6530)\n",
      "✅ test_df  加载完成，shape = (666, 6530)\n",
      "特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "合计特征总数 = 6528\n",
      "数据拆分---------------------------\n",
      "📊 数据拆分完成\n",
      "训练集特征 features_train  shape   : (28808, 6528)\n",
      "训练集目标   target_train  shape   : (28808,)\n",
      "测试集特征  features_test  shape   : (666, 6528)\n",
      "           features_train  类型    : <class 'pandas.core.frame.DataFrame'>\n",
      "特征生成---------------------------\n",
      "(28808, 6536) (666, 6536)\n",
      "开始训练---------------------------\n",
      "————————————————————————————————————————\n",
      "✅ 当前结果将保存到: 2025-10-22 17-27-34\n",
      "🔄 4/10 ST 17:51:21 ET 17:59:17 avg 475.7s [ 414.1s  434.7s  578.4s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-22 17:58:59,082] Trial 29 failed with parameters: {'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 10, 'train_eta': 0.020728885589285505} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24996\\4259608936.py\", line 84, in objective\n",
      "    results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24996\\1853940361.py\", line 130, in run_kfold_xgb\n",
      "    xgb_model = xgb.train(\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\training.py\", line 182, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\callback.py\", line 258, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py\", line 2206, in eval_set\n",
      "    self._assign_dmatrix_features(d[0])\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py\", line 3037, in _assign_dmatrix_features\n",
      "    if self.feature_types is None:\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py\", line 2035, in feature_types\n",
      "    return self._get_feature_info(\"feature_type\")\n",
      "  File \"d:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py\", line 1997, in _get_feature_info\n",
      "    _LIB.XGBoosterGetStrFeatureInfo(\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-22 17:58:59,083] Trial 29 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 6. 打印最优结果\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ 训练完成！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[74], line 84\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     77\u001b[0m X, y, X_test \u001b[38;5;241m=\u001b[39m features_train, target_train, features_test\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练---------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_kfold_xgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIRS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_FOLDS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m config \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     86\u001b[0m score \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[43], line 130\u001b[0m, in \u001b[0;36mrun_kfold_xgb\u001b[1;34m(features_train, target_train, features_test, config, DIRS, K_FOLDS, verbose)\u001b[0m\n\u001b[0;32m    126\u001b[0m dtest  \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(x_test_new,             feature_names\u001b[38;5;241m=\u001b[39mselected_features)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 6. XGBoost 训练\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgb_train_model_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_boost_round\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m    141\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(history_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_model_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\callback.py:258\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m name\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset name should not contain `-`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 258\u001b[0m score: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py:2206\u001b[0m, in \u001b[0;36mBooster.eval_set\u001b[1;34m(self, evals, iteration, feval, output_margin)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2205\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected string, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(d[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_dmatrix_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2208\u001b[0m dmats \u001b[38;5;241m=\u001b[39m c_array(ctypes\u001b[38;5;241m.\u001b[39mc_void_p, [d[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[0;32m   2209\u001b[0m evnames \u001b[38;5;241m=\u001b[39m c_array(ctypes\u001b[38;5;241m.\u001b[39mc_char_p, [c_str(d[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py:3037\u001b[0m, in \u001b[0;36mBooster._assign_dmatrix_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3036\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m-> 3037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3038\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types \u001b[38;5;241m=\u001b[39m ft\n\u001b[0;32m   3040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(fn)\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py:2035\u001b[0m, in \u001b[0;36mBooster.feature_types\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2029\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_types\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[FeatureTypes]:\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Feature types for this booster.  Can be directly set by input data or by\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;124;03m    assignment.  See :py:class:`DMatrix` for details.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \n\u001b[0;32m   2034\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_feature_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Software\\conda\\envs\\py39_tf\\lib\\site-packages\\xgboost\\core.py:1997\u001b[0m, in \u001b[0;36mBooster._get_feature_info\u001b[1;34m(self, field)\u001b[0m\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1996\u001b[0m _check_call(\n\u001b[1;32m-> 1997\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterGetStrFeatureInfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1998\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43msarr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2003\u001b[0m )\n\u001b[0;32m   2004\u001b[0m feature_info \u001b[38;5;241m=\u001b[39m from_cstr_to_pystr(sarr, length)\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_info \u001b[38;5;28;01mif\u001b[39;00m feature_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始优化\n",
    "\n",
    "# 1. 定义 SQLite 数据库存储路径\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name = STUDY_NAME,\n",
    "    # study_name=\"ghsdjsrtjrswtjhwrt\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# 自动获取当前主机名\\当前主机的 IP 地址\n",
    "HOSTNAME = socket.gethostname()\n",
    "HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "print(\"主机名:\", HOSTNAME,\" 主机 IP:\", HOST_IP)\n",
    "time.sleep(1)\n",
    "\n",
    "# 5. 启动超参数搜索\n",
    "print(\"🔎 开始超参数搜索...\")\n",
    "if base_config[\"ISTEST\"]:\n",
    "    study.optimize(objective, n_trials = 3)\n",
    "else:\n",
    "    study.optimize(objective, n_trials = 100)\n",
    "\n",
    "\n",
    "# 6. 打印最优结果\n",
    "print(\"\\n✅ 训练完成！\")\n",
    "print(f\"📊 已完成试验次数 : {len(study.trials)}\")\n",
    "print(f\"🏆 最优试验编号   : {study.best_trial.number}\")\n",
    "print(f\"📉 最优 MAE       : {study.best_value}\")\n",
    "print(f\"⚙️ 最优参数组合   : {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c00178",
   "metadata": {},
   "source": [
    "# 管理数据库信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebff3434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据库中的 study 列表:\n",
      "- optuna_task1\n",
      "         Trials:\n",
      "    Trial    0: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 6, 'train_eta': 0.14963770710824598}\n",
      "    Trial    1: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 7, 'train_eta': 0.018021464633564754}\n",
      "    Trial    2: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.021659776125338565}\n",
      "    Trial    3: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 4, 'train_eta': 0.23918065062107188}\n",
      "    Trial    4: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 6, 'train_eta': 0.10392736979117173}\n",
      "    Trial    5: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.24138275928502126}\n",
      "    Trial    6: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.06282544238599601}\n",
      "    Trial    7: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 10, 'train_eta': 0.017725574116549923}\n",
      "    Trial    8: host=hao-2           , ip=192.168.40.1   , value=17.8499   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.01368700824230773}\n",
      "    Trial    9: host=hao-2           , ip=192.168.40.1   , value=17.9953   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.05053010910637573}\n",
      "    Trial   10: host=hao-2           , ip=192.168.40.1   , value=18.9248   , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.026729074261343022}\n",
      "    Trial   11: host=hao-2           , ip=192.168.40.1   , value=18.4746   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 12, 'train_eta': 0.0955937800637472}\n",
      "    Trial   12: host=hao-2           , ip=192.168.40.1   , value=18.8439   , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 5, 'train_eta': 0.01844230194954457}\n",
      "    Trial   13: host=hao-2           , ip=192.168.40.1   , value=18.5825   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 11, 'train_eta': 0.013329388592334063}\n",
      "    Trial   14: host=hao-2           , ip=192.168.40.1   , value=17.5563   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.013678962412610542}\n",
      "    Trial   15: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 6, 'train_eta': 0.03616840082637632}\n",
      "    Trial   16: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 9, 'train_eta': 0.03020978585855579}\n",
      "    Trial   17: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 9, 'train_eta': 0.06269871406279931}\n",
      "    Trial   18: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.0384   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.09158730675112933}\n",
      "    Trial   19: host=hao-2           , ip=192.168.40.1   , value=18.0853   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 9, 'train_eta': 0.045732428074001644}\n",
      "    Trial   20: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.2934   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 5, 'train_eta': 0.01810149024614987}\n",
      "    Trial   21: host=hao-2           , ip=192.168.40.1   , value=17.9752   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.06743144217230011}\n",
      "    Trial   22: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.5059   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 8, 'train_eta': 0.2193719583768128}\n",
      "    Trial   23: host=hao-2           , ip=192.168.40.1   , value=18.0255   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01061463707842147}\n",
      "    Trial   24: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=17.8361   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01024884269234489}\n",
      "    Trial   25: host=hao-2           , ip=192.168.40.1   , value=17.7623   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.02880197004042992}\n",
      "    Trial   26: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=17.9105   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.028795735946134868}\n",
      "    Trial   27: host=hao-2           , ip=192.168.40.1   , value=17.7255   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 9, 'train_eta': 0.027734152778904956}\n",
      "    Trial   28: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=17.6955   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 9, 'train_eta': 0.031257959966538636}\n",
      "    Trial   29: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 10, 'train_eta': 0.020728885589285505}\n",
      "    Trial   30: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 9, 'train_eta': 0.29698899022302994}\n",
      "    总 trial 数量: 31\n",
      "====================================================================================================\n",
      "- test\n",
      "         Trials:\n",
      "    Trial    0: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.04410160779890175}\n",
      "    Trial    1: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=49.1581   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.04105647603066798}\n",
      "    Trial    2: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 4, 'train_eta': 0.04449261200509654}\n",
      "    总 trial 数量: 3\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 查询数据库详细数据\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "if not studies:\n",
    "    print(\"❌ 当前数据库里无 study\")\n",
    "else:\n",
    "    print(\"✅ 数据库中的 study 列表:\")\n",
    "    for s in studies:\n",
    "\n",
    "        print(\"-\", s.study_name)\n",
    "\n",
    "        study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "\n",
    "        print(\"         Trials:\")\n",
    "        for trial in study.trials:\n",
    "            host = trial.user_attrs.get(\"host\") or \"unknown\"\n",
    "            ip = trial.user_attrs.get(\"ip\") or \"unknown\"\n",
    "            value = f\"{trial.value:.4f}\" if trial.value is not None else \"None\"\n",
    "\n",
    "            print(\n",
    "                f\"    Trial {trial.number:4d}: \"\n",
    "                f\"host={host:<16}, ip={ip:<15}, \"\n",
    "                f\"value={value:<10}, params={trial.params}\"\n",
    "            )\n",
    "\n",
    "        print(\"    总 trial 数量:\", len(study.trials))\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6dfcc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现有 study： ['test']\n",
      "Study:   test                          , Trials:   12\n"
     ]
    }
   ],
   "source": [
    "# 清理前：先查看数据库里当前有哪些 study 存在，以及每个 study 里有多少个 trial\n",
    "\n",
    "storage = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"现有 study：\", [s.study_name for s in studies])\n",
    "\n",
    "for s in studies:\n",
    "    study = optuna.load_study(study_name=s.study_name, storage=storage)\n",
    "    print(f\"Study:   {s.study_name:30s}, Trials: {len(study.trials):4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc73b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理中：删除指定 study\n",
    "# 指定要删除的名称\n",
    "to_delete = [\"melting_point_study\"]   # 可以写一个或多个\n",
    "\n",
    "to_delete = [            ]\n",
    "\n",
    "for s in studies:\n",
    "    if s.study_name in to_delete:\n",
    "        optuna.delete_study(study_name=s.study_name, storage=storage)\n",
    "        print(\"已删除:\", s.study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce984fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理后 study： []\n"
     ]
    }
   ],
   "source": [
    "# 清理后：再次检查\n",
    "studies_after = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"清理后 study：\", [s.study_name for s in studies_after])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
