{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install optuna pymysql sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c8e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª å½“å‰çŠ¶æ€ï¼šå®éªŒè¿è¡Œ\n",
      "ğŸ“‚ Study Name: test\n"
     ]
    }
   ],
   "source": [
    "ISTEST = True\n",
    "\n",
    "\n",
    "# ISTEST = False\n",
    "\n",
    "STUDY_NAME = \"test\" if ISTEST else \"optuna_task1\"\n",
    "\n",
    "if ISTEST:\n",
    "    status_msg = \"ğŸ§ª å½“å‰çŠ¶æ€ï¼šå®éªŒè¿è¡Œ\"\n",
    "else:\n",
    "    status_msg = \"ğŸš€ å½“å‰çŠ¶æ€ï¼šæ­£å¼è¿è¡Œ\"\n",
    "\n",
    "print(status_msg)\n",
    "print(f\"ğŸ“‚ Study Name: {STUDY_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb05eb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalon available: True\n",
      "âœ… è·¯å¾„å·²åˆ›å»ºï¼š\n",
      "\n",
      "dir          : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\n",
      "DATA_DIR000  : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\\DATA_DIR000\n",
      "HISTORY      : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\\HISTORY\n",
      "SUBMISSION   : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\\SUBMISSION\n"
     ]
    }
   ],
   "source": [
    "# ç³»ç»Ÿåº“\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ç¬¬ä¸‰æ–¹ç§‘å­¦è®¡ç®— & å¯è§†åŒ–\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…ä¹±ç \n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # é»‘ä½“\n",
    "plt.rcParams['axes.unicode_minus'] = False          # è§£å†³è´Ÿå·æ˜¾ç¤ºæˆæ–¹å—çš„é—®é¢˜\n",
    "\n",
    "# æœºå™¨å­¦ä¹  & ä¼˜åŒ–\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "# åŒ–å­¦ä¿¡æ¯å­¦ (RDKit)\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import (\n",
    "    Descriptors, Crippen, rdMolDescriptors,\n",
    "    MACCSkeys, RDKFingerprint, rdFingerprintGenerator\n",
    ")\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "# å…³é—­ RDKit çš„è­¦å‘Š\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Avalon æŒ‡çº¹ï¼ˆå¯é€‰ï¼‰\n",
    "try:\n",
    "    from rdkit.Avalon import pyAvalonTools\n",
    "    avalon_available = True\n",
    "except ImportError:\n",
    "    avalon_available = False\n",
    "print(f\"Avalon available: {avalon_available}\")\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe_connected\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = r'D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point'\n",
    "else:\n",
    "    dir = os.getcwd()\n",
    "\n",
    "\n",
    "DIRS = {\n",
    "    \"dir\":              dir,                                       \n",
    "    \"DATA_DIR000\":      os.path.join(dir, \"DATA_DIR000\"),\n",
    "    \"HISTORY\":          os.path.join(dir, \"HISTORY\"),\n",
    "    \"SUBMISSION\":       os.path.join(dir, \"SUBMISSION\"),\n",
    "}\n",
    "\n",
    "# è‡ªåŠ¨åˆ›å»ºç›®å½•\n",
    "for key, path in DIRS.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# æ‰“å°æ—¶ä¸€è¡Œä¸€ä¸ªåœ°å€\n",
    "print(\"âœ… è·¯å¾„å·²åˆ›å»ºï¼š\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc226c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… merge_df åŠ è½½å®Œæˆï¼Œshape = (28808, 6530)\n",
      "âœ… test_df  åŠ è½½å®Œæˆï¼Œshape = (666, 6530)\n",
      "ç‰¹å¾å­—æ®µ: SMILES, Tm | æè¿°ç¬¦: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
      "åˆè®¡ç‰¹å¾æ€»æ•° = 6528\n"
     ]
    }
   ],
   "source": [
    "# è¯»å–å·²å¤„ç†å¥½çš„æœ€ç»ˆç‰¹å¾æ•°æ®\n",
    "\n",
    "# ç‰¹å¾å­—æ®µ: SMILES, Tm | æè¿°ç¬¦: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
    "# åˆè®¡ç‰¹å¾æ€»æ•° = 6528\n",
    "\n",
    "# å®šä¹‰è·¯å¾„\n",
    "merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "merge_df = pd.read_csv(merge_fp_path)\n",
    "test_df  = pd.read_csv(test_fp_path)\n",
    "\n",
    "# æ‰“å°ä¿¡æ¯\n",
    "print(f\"âœ… merge_df åŠ è½½å®Œæˆï¼Œshape = {merge_df.shape}\")\n",
    "print(f\"âœ… test_df  åŠ è½½å®Œæˆï¼Œshape = {test_df.shape}\")\n",
    "\n",
    "print(\"ç‰¹å¾å­—æ®µ: SMILES, Tm | æè¿°ç¬¦: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\")\n",
    "print(\"åˆè®¡ç‰¹å¾æ€»æ•° = 6528\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268ee02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ•°æ®æ‹†åˆ†å®Œæˆ\n",
      "è®­ç»ƒé›†ç‰¹å¾ features_train  shape   : (500, 20)\n",
      "è®­ç»ƒé›†ç›®æ ‡   target_train  shape   : (500,)\n",
      "æµ‹è¯•é›†ç‰¹å¾  features_test  shape   : (666, 20)\n",
      "           features_train  ç±»å‹    : <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®æ‹†åˆ† (ç‰¹å¾çŸ©é˜µ ä¸ ç›®æ ‡å‘é‡)\n",
    "# ============================================\n",
    "# ç‰¹å¾å­—æ®µ: SMILES, Tm | æè¿°ç¬¦: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
    "# åˆè®¡ç‰¹å¾æ€»æ•° = 6528\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # æ„å»ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†\n",
    "# # 1. æ‰¾åˆ°é‡å¤çš„ SMILES\n",
    "# dup_smiles = set(merge_df['SMILES']) & set(test_df['SMILES'])\n",
    "# print(f\"âš ï¸ æ£€æµ‹åˆ° {len(dup_smiles)} ä¸ªé‡å¤ SMILES\")\n",
    "\n",
    "# # 2. åˆ é™¤ merge_df é‡Œ SMILES åœ¨ test_df é‡Œçš„è¡Œ\n",
    "# before_shape = merge_df.shape\n",
    "# merge_df = merge_df[~merge_df['SMILES'].isin(test_df['SMILES'])].reset_index(drop=True)\n",
    "# after_shape = merge_df.shape\n",
    "\n",
    "# print(f\"âœ… åˆ é™¤å®Œæˆ: ä» {before_shape} â†’ {after_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "features_train = merge_df.drop(labels=['SMILES', 'Tm'], axis=1)      # ç‰¹å¾çŸ©é˜µ Xï¼šå»æ‰ SMILES å’Œç›®æ ‡å€¼ Tm\n",
    "target_train = merge_df['Tm']                                      # ç›®æ ‡å‘é‡ yï¼šåªä¿ç•™ Tm (ç†”ç‚¹ï¼Œå•ä½ K)\n",
    "features_test = test_df.drop(labels=['SMILES', 'id'], axis=1)  # æµ‹è¯•é›†ç‰¹å¾ï¼šå»æ‰ SMILES å’Œ id (å› ä¸º test æ²¡æœ‰ Tm)\n",
    "\n",
    "\n",
    "\n",
    "# éšæœºé€‰å–éƒ¨åˆ†ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼š50 ä¸ªï¼‰\n",
    "if ISTEST:\n",
    "    np.random.seed(42)\n",
    "    selected_features = np.random.choice(\n",
    "        merge_df.drop(columns=['SMILES', 'Tm']).columns,\n",
    "        size=20,\n",
    "        replace=False\n",
    "    )\n",
    "    sample_len = 500\n",
    "    features_train = merge_df.iloc[:sample_len][selected_features]   # è®­ç»ƒç‰¹å¾ (å‰ 1000 æ¡)\n",
    "    target_train = merge_df.iloc[:sample_len]['Tm']               # è®­ç»ƒç›®æ ‡\n",
    "    features_test = test_df[selected_features]          # æµ‹è¯•ç‰¹å¾ (åŒæ ·çš„ç‰¹å¾åˆ—)\n",
    "\n",
    "\n",
    "\n",
    "# 3. æ‰“å°ç»´åº¦ä¿¡æ¯\n",
    "print(\"ğŸ“Š æ•°æ®æ‹†åˆ†å®Œæˆ\")\n",
    "print(f\"è®­ç»ƒé›†ç‰¹å¾ features_train  shape   : {features_train.shape}\")\n",
    "print(f\"è®­ç»ƒé›†ç›®æ ‡   target_train  shape   : {target_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†ç‰¹å¾  features_test  shape   : {features_test.shape}\")\n",
    "print(f\"           features_train  ç±»å‹    : {type(features_train)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4877be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold + XGBoost è¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œå¹¶ä¿å­˜å®éªŒç»“æœ\n",
    "# ==============================================================\n",
    "def run_kfold_xgb(features_train, target_train, features_test, params, DIRS, K_FOLDS=5, verbose=0):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Stratified K-Fold + XGBoost è¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œå¹¶ä¿å­˜å®éªŒç»“æœ\n",
    "\n",
    "    å‚æ•°:\n",
    "        features_train, target_train        : è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "        features_test      : æµ‹è¯•é›†ç‰¹å¾\n",
    "        params      : XGBoost æœ€ä¼˜å‚æ•° (dict)\n",
    "        DIRS        : ä¿å­˜ç»“æœçš„ç›®å½•å­—å…¸\n",
    "        K_FOLDS     : æŠ˜æ•° (é»˜è®¤=5)\n",
    "        verbose     : æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    # ---------- åˆ›å»ºç›®å½• ----------\n",
    "    for _, path in DIRS.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = os.path.join(DIRS['HISTORY'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"â€”â€”\" * 20)\n",
    "    print(f\"âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: {time_str}\")\n",
    "\n",
    "\n",
    "    # ---------- å®šä¹‰äº¤å‰éªŒè¯ ----------\n",
    "    skfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    yeo = PowerTransformer(method=\"yeo-johnson\")                                # å®šä¹‰ Yeo-Johnson å˜æ¢\n",
    "\n",
    "    # ---------- åˆå§‹åŒ–å­˜å‚¨ ----------\n",
    "    oof_val = np.zeros(len(features_train))       # OOF é¢„æµ‹\n",
    "    train_score, val_score = [], []  # æ¯æŠ˜ MAE\n",
    "    test_pred = []                   # æ¯æŠ˜ test é¢„æµ‹\n",
    "    fold_records = []                # ä¿å­˜æ¯æŠ˜ä¿¡æ¯\n",
    "    all_importances = []             # ç‰¹å¾é‡è¦æ€§\n",
    "    elapsed_list = []                # è€—æ—¶è®°å½•\n",
    "\n",
    "\n",
    "\n",
    "    # å¾ªç¯æ¯ä¸€æŠ˜\n",
    "    # ==============================================================\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skfold.split(features_train, pd.qcut(target_train, q=10).cat.codes), 1):\n",
    "\n",
    "        # ----- æ‰“å°æ—¶é—´ä¿¡æ¯ -----\n",
    "        start_now = datetime.now()\n",
    "        start_str = start_now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if elapsed_list:\n",
    "            avg_time = np.mean(elapsed_list)\n",
    "            est_end = start_now + timedelta(seconds=avg_time)\n",
    "\n",
    "            # æ¯ 5 ä¸ªä¸€ç»„è¾“å‡ºè€—æ—¶\n",
    "            parts = [f\"{t:6.1f}s\" for t in elapsed_list]\n",
    "            grouped = [\" \".join(parts[j:j+5]) for j in range(0, len(parts), 5)]\n",
    "            elapsed_str = \" /// \".join(grouped)\n",
    "\n",
    "            print(\n",
    "                f\"ğŸ”„{i:2d}/{K_FOLDS} ST {start_str}\"\n",
    "                f\" ET {est_end.strftime('%H:%M:%S')}\"\n",
    "                f\" avg {avg_time:.1f}s\"\n",
    "                f\" [{elapsed_str}]\",\n",
    "                end=\"\\r\", flush=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"ğŸ”„{i:2d}/{K_FOLDS} ST {start_str} ET (æš‚æ— å†å²æ•°æ®)\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "        # ----- å¼€å§‹è®­ç»ƒ -----\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. æ•°æ®é›†åˆ’åˆ†\n",
    "        x_train, x_val = features_train.iloc[train_idx], features_train.iloc[val_idx]\n",
    "        y_train, y_val = target_train[train_idx], target_train[val_idx]\n",
    "\n",
    "        # 2. Yeo-Johnson å˜æ¢\n",
    "        y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "        y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "\n",
    "        # 3. ç‰¹å¾é€‰æ‹©ï¼ˆè½»é‡çº§ XGBoostï¼‰\n",
    "        selector_model = xgb.XGBRegressor(\n",
    "            n_estimators=500, \n",
    "            max_depth=6, \n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "            device=\"cuda\", \n",
    "            objective=\"reg:absoluteerror\",\n",
    "            tree_method=\"hist\", \n",
    "            verbosity=0\n",
    "        )\n",
    "        selector_model.fit(x_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(selector_model, prefit=True, threshold=\"mean\")\n",
    "        selected_features = x_train.columns[selector.get_support()].tolist()\n",
    "        if verbose > 0:\n",
    "            print(f\"âœ… é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}\")\n",
    "\n",
    "\n",
    "        # 4. ä¿ç•™é‡è¦ç‰¹å¾\n",
    "        x_train_new = x_train[selected_features]\n",
    "        x_val_new   = x_val[selected_features]\n",
    "        x_test_new  = features_test[selected_features]\n",
    "\n",
    "        # 5. è½¬æ¢ä¸º DMatrix\n",
    "        dtrain = xgb.DMatrix(x_train_new, y_train, feature_names=selected_features)\n",
    "        dval   = xgb.DMatrix(x_val_new,   y_val,   feature_names=selected_features)\n",
    "        dtest  = xgb.DMatrix(x_test_new,             feature_names=selected_features)\n",
    "\n",
    "\n",
    "        # 6. XGBoost è®­ç»ƒ\n",
    "        xgb_model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=params[\"num_boost_round\"],\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=300,\n",
    "            verbose_eval=(1000 if verbose > 0 else False)\n",
    "        )\n",
    "\n",
    "\n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        model_path = os.path.join(history_DIR, f\"xgb_model_fold{i}.json\")\n",
    "        xgb_model.save_model(model_path)\n",
    "\n",
    "        # 7. è·å–ç‰¹å¾é‡è¦æ€§\n",
    "        imp_dict = xgb_model.get_score(importance_type=\"gain\")\n",
    "        imp_df = pd.DataFrame(imp_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "        imp_df[\"Fold\"] = i\n",
    "        all_importances.append(imp_df)\n",
    "\n",
    "\n",
    "        # 8. é¢„æµ‹\n",
    "        y_train_pred = xgb_model.predict(dtrain)\n",
    "        y_val_pred   = xgb_model.predict(dval)\n",
    "        y_test_pred  = xgb_model.predict(dtest)\n",
    "\n",
    "        # 9. é€†å˜æ¢\n",
    "        y_train      = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "        y_val        = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "        y_train_pred = yeo.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
    "        y_val_pred   = yeo.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
    "        y_test_pred  = yeo.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        # 10. è®¡ç®— MAE\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        val_mae   = mean_absolute_error(y_val,   y_val_pred)\n",
    "        if verbose > 0:\n",
    "            print(f\"Fold {i}: Train MAE={train_mae:.4f}, Val MAE={val_mae:.4f}ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’\")\n",
    "\n",
    "\n",
    "\n",
    "        # ----- ä¿å­˜ç»“æœ -----\n",
    "        train_score.append(train_mae)\n",
    "        val_score.append(val_mae)\n",
    "        oof_val[val_idx] = y_val_pred\n",
    "        test_pred.append(y_test_pred)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        elapsed_list.append(elapsed)\n",
    "\n",
    "        fold_records.append({\n",
    "            \"Fold\": i,\n",
    "            \"Train_MAE\": train_mae,\n",
    "            \"Val_MAE\": val_mae,\n",
    "            \"Num_Features\": len(selected_features),\n",
    "            \"Selected_Features\": selected_features,\n",
    "            \"elapsed\": elapsed\n",
    "        })\n",
    "\n",
    "    # ä¿å­˜æ•´ä½“ç»“æœ\n",
    "    # ==============================================================\n",
    "    if verbose > 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"ğŸ“Š Train MAE å¹³å‡å€¼ : {np.mean(train_score):.4f}\")\n",
    "        print(f\"ğŸ“Š Val   MAE å¹³å‡å€¼ : {np.mean(val_score):.4f}\")\n",
    "        print(f\"ğŸ“Š Train MAE æ ‡å‡†å·® : {np.std(train_score, ddof=0):.4f}\")\n",
    "        print(f\"ğŸ“Š Val   MAE æ ‡å‡†å·® : {np.std(val_score, ddof=0):.4f}\")\n",
    "\n",
    "    # å‚æ•°\n",
    "    with open(os.path.join(history_DIR, \"params.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(params, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # æ¯æŠ˜ä¿¡æ¯\n",
    "    folds_df = pd.DataFrame(fold_records)\n",
    "    folds_df.to_csv(os.path.join(history_DIR, \"folds_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # ç‰¹å¾é‡è¦æ€§\n",
    "    if all_importances:\n",
    "        valid_imps = [df for df in all_importances if not df.empty]\n",
    "        all_imp_df = pd.concat(valid_imps, axis=0) if valid_imps else pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    else:\n",
    "        all_imp_df = pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    all_imp_df.to_csv(os.path.join(history_DIR, \"feature_importance_all.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    test_pred_array = np.vstack(test_pred).T\n",
    "    test_pred_df = pd.DataFrame(test_pred_array, columns=[f\"Fold_{j+1}\" for j in range(test_pred_array.shape[1])])\n",
    "    test_pred_df[\"Final_Pred\"] = test_pred_df.mean(axis=1)\n",
    "    test_pred_df.to_csv(os.path.join(history_DIR, \"test_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # æ€»ç»“\n",
    "    with open(os.path.join(history_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train MAE Mean : {np.mean(train_score):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Mean : {np.mean(val_score):.4f}\\n\")\n",
    "        f.write(f\"Train MAE Std  : {np.std(train_score, ddof=0):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Std  : {np.std(val_score, ddof=0):.4f}\\n\")\n",
    "\n",
    "\n",
    "    # æœ€ç»ˆæäº¤\n",
    "    final_score = np.mean(val_score)\n",
    "    submission = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"sample_submission.csv\"))\n",
    "    submission[\"Tm\"] = test_pred_df[\"Final_Pred\"]\n",
    "\n",
    "    submission_path = os.path.join(history_DIR, f\"sub_{time_str}_{final_score:.8f}.csv\")\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    submission.to_csv(os.path.join(DIRS['SUBMISSION'], f\"sub_{time_str}_{final_score:.8f}.csv\"), index=False)\n",
    "\n",
    "\n",
    "    # ---------- è¿”å›ç»“æœ ----------\n",
    "    return {\n",
    "        \"oof_val\": oof_val,\n",
    "        \"train_score\": train_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_pred\": test_pred_df,\n",
    "        \"folds_info\": folds_df,\n",
    "        \"feature_importance\": all_imp_df,\n",
    "        \"submission_path\": submission_path,\n",
    "        \"time\": time_str,\n",
    "        \"final_score\": final_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b04208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¼˜åŒ–ä»»åŠ¡  åŠ å…¥æ ‡è¯†ç¬¦ host: hao-2   ip: 192.168.40.1\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna çš„ç›®æ ‡å‡½æ•° (Objective Function)\n",
    "    æ¯æ¬¡ trial ä¼šç”Ÿæˆä¸€ç»„è¶…å‚æ•°ï¼Œç”¨äºè®­ç»ƒ XGBoost æ¨¡å‹ï¼Œ\n",
    "    å¹¶è¿”å›äº¤å‰éªŒè¯çš„å¹³å‡ RMSE ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. å®šä¹‰ XGBoost è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "    xgb_params = {\n",
    "        \"verbosity\"        : 0,                                   # è®­ç»ƒæ—¶æ—¥å¿—è¾“å‡ºçº§åˆ« (0=é™é»˜)\n",
    "        \"objective\"        : \"reg:absoluteerror\",              # å›å½’ä»»åŠ¡ç›®æ ‡å‡½æ•°\n",
    "        \"tree_method\"      : \"gpu_hist\",                          # ä½¿ç”¨ GPU åŠ é€Ÿçš„ç›´æ–¹å›¾ç®—æ³•\n",
    "        \"predictor\"        : \"gpu_predictor\",                     # GPU é¢„æµ‹\n",
    "        \"device\"           : \"cuda\",                              # æŒ‡å®šè®¾å¤‡ (CUDA GPU)\n",
    "        \"eval_metric\"      : \"mae\",                               # è¯„ä¼°æŒ‡æ ‡ï¼šå¹³å‡ç»å¯¹è¯¯å·®\n",
    "        \"booster\"          : \"gbtree\",                            # åŸºå­¦ä¹ å™¨ï¼šæ ‘æ¨¡å‹\n",
    "        \"num_boost_round\"     : 20000,                               # å¦‚æœç”¨ sklearn API æ‰ä¿ç•™ï¼›xgb.train ç”¨ num_boost_round\n",
    "\n",
    "        # -------- éœ€è¦è°ƒä¼˜çš„è¶…å‚æ•° --------\n",
    "        \"max_depth\"        : trial.suggest_int  (\"max_depth\"       , 3    , 7),\n",
    "        \"learning_rate\"    : trial.suggest_float(\"learning_rate\"   , 0.01 , 0.3 , log=True),\n",
    "        \"min_child_weight\" : trial.suggest_int  (\"min_child_weight\", 1    , 10),\n",
    "        \"subsample\"        : trial.suggest_float(\"subsample\"       , 0.5  , 1.0),\n",
    "        \"colsample_bytree\" : trial.suggest_float(\"colsample_bytree\", 0.5  , 1.0),\n",
    "        \"gamma\"            : trial.suggest_float(\"gamma\"           , 0.0  , 1.0),\n",
    "        \"reg_lambda\"       : trial.suggest_float(\"reg_lambda\"      , 0.1  , 5.0 , log=True),\n",
    "        \"reg_alpha\"        : trial.suggest_float(\"reg_alpha\"       , 0.1  , 1.0 , log=True),\n",
    "    }\n",
    "\n",
    "\n",
    "    results = run_kfold_xgb(features_train, target_train, features_test, xgb_params, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "\n",
    "    score = results['final_score']\n",
    "    \n",
    "\n",
    "    \n",
    "    HOSTNAME = socket.gethostname()\n",
    "    HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "    trial.set_user_attr(\"host\", HOSTNAME)        # ä½ è‡ªå·±å®šä¹‰ä¸»æœº A/B\n",
    "    trial.set_user_attr(\"ip\", HOST_IP)        # ä½ è‡ªå·±å®šä¹‰è§’è‰² A/B\n",
    "\n",
    "    \n",
    "    # 4. è¿”å›å¹³å‡ MAE\n",
    "    return score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9422187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 15:55:03,256] Using an existing study with name 'test' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸»æœºå: hao-2  ä¸»æœº IP: 192.168.40.1\n",
      "ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢...\n",
      "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
      "âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: 2025-10-21 15-55-04\n",
      "ğŸ”„10/10 ST 15:55:24 ET 15:55:26 avg 2.2s [   2.0s    2.7s    1.5s    3.4s    1.6s ///    2.0s    2.5s    1.5s    2.8s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 15:55:26,125] Trial 6 finished with value: 63.064621230468745 and parameters: {'max_depth': 3, 'learning_rate': 0.019651386606002744, 'min_child_weight': 6, 'subsample': 0.5334783686346105, 'colsample_bytree': 0.5894583143905426, 'gamma': 0.49088975643532917, 'reg_lambda': 0.15242062420054192, 'reg_alpha': 0.21577721406330097}. Best is trial 6 with value: 63.064621230468745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
      "âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: 2025-10-21 15-55-26\n",
      "ğŸ”„10/10 ST 15:55:41 ET 15:55:43 avg 1.7s [   1.5s    2.5s    1.4s    1.6s    1.5s ///    1.5s    1.5s    2.2s    1.7s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 15:55:44,048] Trial 7 finished with value: 63.42987898193358 and parameters: {'max_depth': 3, 'learning_rate': 0.16701877404534557, 'min_child_weight': 8, 'subsample': 0.6649407630812719, 'colsample_bytree': 0.8350044077322408, 'gamma': 0.5935872801065222, 'reg_lambda': 0.15156043255073376, 'reg_alpha': 0.30274397572605766}. Best is trial 6 with value: 63.064621230468745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
      "âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: 2025-10-21 15-55-44\n",
      "ğŸ”„10/10 ST 15:56:04 ET 15:56:06 avg 2.3s [   1.9s    3.3s    1.5s    3.4s    1.5s ///    2.0s    1.7s    1.4s    3.9s]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 15:56:06,540] Trial 8 finished with value: 63.14212542968748 and parameters: {'max_depth': 3, 'learning_rate': 0.013736348754728076, 'min_child_weight': 7, 'subsample': 0.5852735359947805, 'colsample_bytree': 0.68036314460227, 'gamma': 0.7599864574319253, 'reg_lambda': 0.2653823032465253, 'reg_alpha': 0.19019002717897487}. Best is trial 6 with value: 63.064621230468745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… è®­ç»ƒå®Œæˆï¼\n",
      "ğŸ“Š å·²å®Œæˆè¯•éªŒæ¬¡æ•° : 9\n",
      "ğŸ† æœ€ä¼˜è¯•éªŒç¼–å·   : 6\n",
      "ğŸ“‰ æœ€ä¼˜ MAE       : 63.064621230468745\n",
      "âš™ï¸ æœ€ä¼˜å‚æ•°ç»„åˆ   : {'max_depth': 3, 'learning_rate': 0.019651386606002744, 'min_child_weight': 6, 'subsample': 0.5334783686346105, 'colsample_bytree': 0.5894583143905426, 'gamma': 0.49088975643532917, 'reg_lambda': 0.15242062420054192, 'reg_alpha': 0.21577721406330097}\n"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹ä¼˜åŒ–\n",
    "\n",
    "# 1. å®šä¹‰ SQLite æ•°æ®åº“å­˜å‚¨è·¯å¾„\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name = STUDY_NAME,\n",
    "    # study_name=\"ghsdjsrtjrswtjhwrt\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# è‡ªåŠ¨è·å–å½“å‰ä¸»æœºå\\å½“å‰ä¸»æœºçš„ IP åœ°å€\n",
    "HOSTNAME = socket.gethostname()\n",
    "HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "print(\"ä¸»æœºå:\", HOSTNAME,\" ä¸»æœº IP:\", HOST_IP)\n",
    "time.sleep(1)\n",
    "\n",
    "# 5. å¯åŠ¨è¶…å‚æ•°æœç´¢\n",
    "print(\"ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢...\")\n",
    "if ISTEST:\n",
    "    study.optimize(objective, n_trials = 3)\n",
    "else:\n",
    "    study.optimize(objective, n_trials = 100)\n",
    "\n",
    "\n",
    "# 6. æ‰“å°æœ€ä¼˜ç»“æœ\n",
    "print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š å·²å®Œæˆè¯•éªŒæ¬¡æ•° : {len(study.trials)}\")\n",
    "print(f\"ğŸ† æœ€ä¼˜è¯•éªŒç¼–å·   : {study.best_trial.number}\")\n",
    "print(f\"ğŸ“‰ æœ€ä¼˜ MAE       : {study.best_value}\")\n",
    "print(f\"âš™ï¸ æœ€ä¼˜å‚æ•°ç»„åˆ   : {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb4503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6becea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "åœæ­¢è¿è¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074d107",
   "metadata": {},
   "source": [
    "# ç®¡ç†æ•°æ®åº“ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "258b33c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®åº“ä¸­çš„ study åˆ—è¡¨:\n",
      "- test\n",
      "         Trials:\n",
      "    Trial    0: host=hao-2           , ip=192.168.40.1   , value=63.3390   , params={'max_depth': 7, 'learning_rate': 0.014881350294625223, 'min_child_weight': 6, 'subsample': 0.7434737625708363, 'colsample_bytree': 0.9897332098694307, 'gamma': 0.2774819216237613, 'reg_lambda': 0.5956939885569088, 'reg_alpha': 0.6311524797185967}\n",
      "    Trial    1: host=hao-2           , ip=192.168.40.1   , value=63.6148   , params={'max_depth': 3, 'learning_rate': 0.2713247898317377, 'min_child_weight': 7, 'subsample': 0.9849126281161902, 'colsample_bytree': 0.8309378401166326, 'gamma': 0.7814392281295133, 'reg_lambda': 3.2585889489649014, 'reg_alpha': 0.235425190902393}\n",
      "    Trial    2: host=hao-2           , ip=192.168.40.1   , value=63.3587   , params={'max_depth': 5, 'learning_rate': 0.05739020873816809, 'min_child_weight': 10, 'subsample': 0.6583362500908256, 'colsample_bytree': 0.5101341233098649, 'gamma': 0.6972120855106135, 'reg_lambda': 3.618819426954319, 'reg_alpha': 0.1364431449462794}\n",
      "    Trial    3: host=hao-2           , ip=192.168.40.1   , value=63.1698   , params={'max_depth': 4, 'learning_rate': 0.010684385348500094, 'min_child_weight': 7, 'subsample': 0.677502588848207, 'colsample_bytree': 0.8564329920489518, 'gamma': 0.9007377834132619, 'reg_lambda': 0.29012528778881785, 'reg_alpha': 0.2532545196762364}\n",
      "    Trial    4: host=hao-2           , ip=192.168.40.1   , value=63.2542   , params={'max_depth': 3, 'learning_rate': 0.02415841699596378, 'min_child_weight': 10, 'subsample': 0.8770903924941493, 'colsample_bytree': 0.8827606894431587, 'gamma': 0.730689040965059, 'reg_lambda': 0.11850428241010616, 'reg_alpha': 0.9828137567158434}\n",
      "    Trial    5: host=hao-2           , ip=192.168.40.1   , value=63.6910   , params={'max_depth': 7, 'learning_rate': 0.11417427789523216, 'min_child_weight': 6, 'subsample': 0.9257953313915841, 'colsample_bytree': 0.8881933816423004, 'gamma': 0.12052847243890474, 'reg_lambda': 0.7658902130962354, 'reg_alpha': 0.24079518857358084}\n",
      "    Trial    6: host=hao-2           , ip=192.168.40.1   , value=63.0646   , params={'max_depth': 3, 'learning_rate': 0.019651386606002744, 'min_child_weight': 6, 'subsample': 0.5334783686346105, 'colsample_bytree': 0.5894583143905426, 'gamma': 0.49088975643532917, 'reg_lambda': 0.15242062420054192, 'reg_alpha': 0.21577721406330097}\n",
      "    Trial    7: host=hao-2           , ip=192.168.40.1   , value=63.4299   , params={'max_depth': 3, 'learning_rate': 0.16701877404534557, 'min_child_weight': 8, 'subsample': 0.6649407630812719, 'colsample_bytree': 0.8350044077322408, 'gamma': 0.5935872801065222, 'reg_lambda': 0.15156043255073376, 'reg_alpha': 0.30274397572605766}\n",
      "    Trial    8: host=hao-2           , ip=192.168.40.1   , value=63.1421   , params={'max_depth': 3, 'learning_rate': 0.013736348754728076, 'min_child_weight': 7, 'subsample': 0.5852735359947805, 'colsample_bytree': 0.68036314460227, 'gamma': 0.7599864574319253, 'reg_lambda': 0.2653823032465253, 'reg_alpha': 0.19019002717897487}\n",
      "    æ€» trial æ•°é‡: 9\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢æ•°æ®åº“è¯¦ç»†æ•°æ®\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "if not studies:\n",
    "    print(\"âŒ å½“å‰æ•°æ®åº“é‡Œæ—  study\")\n",
    "else:\n",
    "    print(\"âœ… æ•°æ®åº“ä¸­çš„ study åˆ—è¡¨:\")\n",
    "    for s in studies:\n",
    "\n",
    "        print(\"-\", s.study_name)\n",
    "\n",
    "        study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "\n",
    "        print(\"         Trials:\")\n",
    "        for trial in study.trials:\n",
    "            host = trial.user_attrs.get(\"host\") or \"unknown\"\n",
    "            ip = trial.user_attrs.get(\"ip\") or \"unknown\"\n",
    "            value = f\"{trial.value:.4f}\" if trial.value is not None else \"None\"\n",
    "\n",
    "            print(\n",
    "                f\"    Trial {trial.number:4d}: \"\n",
    "                f\"host={host:<16}, ip={ip:<15}, \"\n",
    "                f\"value={value:<10}, params={trial.params}\"\n",
    "            )\n",
    "\n",
    "        print(\"    æ€» trial æ•°é‡:\", len(study.trials))\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cff1ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç°æœ‰ studyï¼š ['test']\n",
      "Study:   test                          , Trials:    9\n"
     ]
    }
   ],
   "source": [
    "# æ¸…ç†å‰ï¼šå…ˆæŸ¥çœ‹æ•°æ®åº“é‡Œå½“å‰æœ‰å“ªäº› study å­˜åœ¨ï¼Œä»¥åŠæ¯ä¸ª study é‡Œæœ‰å¤šå°‘ä¸ª trial\n",
    "\n",
    "storage = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"ç°æœ‰ studyï¼š\", [s.study_name for s in studies])\n",
    "\n",
    "for s in studies:\n",
    "    study = optuna.load_study(study_name=s.study_name, storage=storage)\n",
    "    print(f\"Study:   {s.study_name:30s}, Trials: {len(study.trials):4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c32f73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†ä¸­ï¼šåˆ é™¤æŒ‡å®š study\n",
    "# æŒ‡å®šè¦åˆ é™¤çš„åç§°\n",
    "to_delete = [\"melting_point_study\"]   # å¯ä»¥å†™ä¸€ä¸ªæˆ–å¤šä¸ª\n",
    "to_delete = []   # å¯ä»¥å†™ä¸€ä¸ªæˆ–å¤šä¸ª\n",
    "\n",
    "for s in studies:\n",
    "    if s.study_name in to_delete:\n",
    "        optuna.delete_study(study_name=s.study_name, storage=storage)\n",
    "        print(\"å·²åˆ é™¤:\", s.study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a5e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¸…ç†å studyï¼š ['test']\n"
     ]
    }
   ],
   "source": [
    "# æ¸…ç†åï¼šå†æ¬¡æ£€æŸ¥\n",
    "studies_after = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"æ¸…ç†å studyï¼š\", [s.study_name for s in studies_after])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84390793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec45b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "åœæ­¢è¿è¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e22ee",
   "metadata": {},
   "source": [
    "# å•æ¬¡è®­ç»ƒæ¨å¯¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49b0c888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰æœ€ä¼˜ MAE : 63.064621230468745 \n",
      "\n",
      "verbosity           : 0                   \n",
      "objective           : reg:absoluteerror   \n",
      "tree_method         : gpu_hist            \n",
      "predictor           : gpu_predictor       \n",
      "device              : cuda                \n",
      "eval_metric         : mae                 \n",
      "booster             : gbtree              \n",
      "num_boost_round     : 20000               \n",
      "max_depth           : 3                   \n",
      "learning_rate       : 0.019651386606002744\n",
      "min_child_weight    : 6                   \n",
      "subsample           : 0.5334783686346105  \n",
      "colsample_bytree    : 0.5894583143905426  \n",
      "gamma               : 0.49088975643532917 \n",
      "reg_lambda          : 0.15242062420054192 \n",
      "reg_alpha           : 0.21577721406330097 \n"
     ]
    }
   ],
   "source": [
    "# ä» Optuna study è·å–æœ€ä¼˜å‚æ•°\n",
    "\n",
    "storage = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "# åªåŠ è½½æŒ‡å®š study\n",
    "study = optuna.load_study(study_name=STUDY_NAME, storage=storage)\n",
    "\n",
    "print(f\"å½“å‰æœ€ä¼˜ MAE : {study.best_value} \\n\")\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# æ„é€ æœ€ç»ˆè®­ç»ƒç”¨å‚æ•°ï¼ˆå¯¹é½æ ¼å¼ï¼‰\n",
    "params = {\n",
    "    \"verbosity\"        : 0,                                   # æ—¥å¿—é™é»˜\n",
    "    \"objective\"        : \"reg:absoluteerror\",                 # å›å½’ä»»åŠ¡ç›®æ ‡å‡½æ•° (MAE)\n",
    "    \"tree_method\"      : \"gpu_hist\",                          # GPU åŠ é€Ÿç›´æ–¹å›¾ç®—æ³•\n",
    "    \"predictor\"        : \"gpu_predictor\",                     # GPU é¢„æµ‹\n",
    "    \"device\"           : \"cuda\",                              # CUDA GPU\n",
    "    \"eval_metric\"      : \"mae\",                               # è¯„ä¼°æŒ‡æ ‡\n",
    "    \"booster\"          : \"gbtree\",                            # æ ‘æ¨¡å‹\n",
    "    \"num_boost_round\"  : 20_000,                              # æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "\n",
    "    # -------- è°ƒä¼˜åçš„è¶…å‚æ•° --------\n",
    "    \"max_depth\"        : best_params.get(\"max_depth\"       , 6   ),\n",
    "    \"learning_rate\"    : best_params.get(\"learning_rate\"   , 0.10),\n",
    "    \"min_child_weight\" : best_params.get(\"min_child_weight\", 6   ),\n",
    "    \"subsample\"        : best_params.get(\"subsample\"       , 0.60),\n",
    "    \"colsample_bytree\" : best_params.get(\"colsample_bytree\", 0.60),\n",
    "    \"gamma\"            : best_params.get(\"gamma\"           , 0.40),\n",
    "    \"reg_lambda\"       : best_params.get(\"reg_lambda\"      , 1.60), \n",
    "    \"reg_alpha\"        : best_params.get(\"reg_alpha\"       , 0.40),\n",
    "}\n",
    "\n",
    "\n",
    "for k, v in params.items():\n",
    "    print(f\"{k:20s}: {str(v):20s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2048049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params.get(\"lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a16d3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
      "âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: 2025-10-21 15-58-09\n",
      "ğŸ”„10/10 ST 15:58:29 ET 15:58:31 avg 2.2s [   2.1s    2.6s    1.5s    3.5s    1.6s ///    2.1s    2.4s    1.5s    2.8s]\n",
      " 63.064621230468745\n"
     ]
    }
   ],
   "source": [
    "# å•ä¸€æ‰§è¡Œä¸€æ¬¡\n",
    "\n",
    "results = run_kfold_xgb(features_train, target_train, features_test, params, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "\n",
    "score = results['final_score']\n",
    "\n",
    "print('\\n',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da34bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "åœæ­¢è¿è¡Œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bb468",
   "metadata": {},
   "source": [
    "# æäº¤ kaggle å¹³å°æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af747198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ® submission_time å®šä½æ–‡ä»¶è·¯å¾„ æäº¤ kaggle å¹³å°æµ‹è¯•\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def find_submission_file(submission_time, submission_dir):\n",
    "    \"\"\"\n",
    "    åœ¨ submission_dir ä¸‹æŸ¥æ‰¾åŒ…å« submission_time çš„æ–‡ä»¶\n",
    "    ä¸€æ—¦æ‰¾åˆ°ç«‹åˆ»è¿”å›å®Œæ•´è·¯å¾„ï¼›å¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å› None\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(submission_dir):\n",
    "        if submission_time in fname:\n",
    "            file_path = os.path.join(submission_dir, fname)\n",
    "            print(f\"âœ… æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {fname}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(f\"âš ï¸ æœªæ‰¾åˆ°åŒ…å« {submission_time} çš„æ–‡ä»¶\")\n",
    "    return None\n",
    "\n",
    "def submit_and_get_score(file_path, competition_name, message=\"My submission\"):\n",
    "    \"\"\"\n",
    "    å°è£… Kaggle æäº¤å¹¶ç­‰å¾…ç»“æœè¯„åˆ†\n",
    "    --------------------------------------\n",
    "    file_path        : str  æäº¤æ–‡ä»¶è·¯å¾„\n",
    "    competition_name : str  Kaggle æ¯”èµ›åç§° (URL æœ€åä¸€æ®µ)\n",
    "    message          : str  æäº¤å¤‡æ³¨\n",
    "    \"\"\"\n",
    "    # 1. é…ç½® Kaggle API\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = r\"C:\\Users\\Admin\\.kaggle\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"âœ… Kaggle API å·²ç»é…ç½®æˆåŠŸï¼\")\n",
    "\n",
    "    # 2. æäº¤æ–‡ä»¶\n",
    "    api.competition_submit(\n",
    "        file_name=file_path,\n",
    "        competition=competition_name,\n",
    "        message=message\n",
    "    )\n",
    "    print(\"âœ… æäº¤å®Œæˆï¼è¯·ç­‰å¾…è¯„åˆ†...\")\n",
    "\n",
    "    # 3. åŠ¨æ€ç­‰å¾…\n",
    "    spinner = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
    "    while True:\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        latest = submissions[0]\n",
    "        status_str = str(latest._status).lower()\n",
    "\n",
    "        if \"complete\" in status_str and latest._public_score is not None:\n",
    "            print(\"\\nğŸ¯ æœ€ç»ˆç»“æœ:\")\n",
    "            print(f\"Public åˆ†æ•° : {latest._public_score}\")\n",
    "            print(f\"Private åˆ†æ•°: {latest._private_score}\")\n",
    "            print(f\"æäº¤ ID     : {latest._ref}\")\n",
    "            print(f\"æ–‡ä»¶å      : {latest._file_name}\")\n",
    "            print(f\"çŠ¶æ€        : {latest._status}\")\n",
    "            print(f\"æäº¤æ—¶é—´    : {latest._date}\")\n",
    "            print(f\"æè¿°/å¤‡æ³¨   : {latest._description}\")\n",
    "            return latest\n",
    "\n",
    "        spin_char = next(spinner)\n",
    "        print(f\"å½“å‰çŠ¶æ€: {status_str} , ç­‰å¾…ä¸­ {spin_char}\", end=\"\\r\", flush=True)\n",
    "        time.sleep(0.2)  # æ¯ 0.5 ç§’æ£€æŸ¥ä¸€æ¬¡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03847a15",
   "metadata": {},
   "source": [
    "### ä¸è½»æ˜“è¿è¡Œï¼Œå†ä¸‰è€ƒè™‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a8f4665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: sub_2025-10-21 15-58-09_63.06462123.csv\n"
     ]
    }
   ],
   "source": [
    "# submission_time æäº¤\n",
    "submission_time = \"2025-10-21 15-58-09\"   \n",
    "competition_name = \"melting-point\"\n",
    "message =  \"æœ¬åœ°æäº¤æµ‹è¯•\"\n",
    "\n",
    "target_file = find_submission_file(submission_time, DIRS['SUBMISSION'] )\n",
    "\n",
    "# submit_and_get_score(target_file, competition_name, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "åœæ­¢è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9b69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd282f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
