{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a782e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalon available: True\n",
      "✅ 路径已创建：\n",
      "\n",
      "dir          : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\n",
      "DATA_DIR000  : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\\DATA_DIR000\n",
      "HISTORY      : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\\HISTORY\n",
      "SUBMISSION   : D:\\数据\\Kaggle\\Thermophysical Property Melting Point\\SUBMISSION\n"
     ]
    }
   ],
   "source": [
    "# 系统库\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 第三方科学计算 & 可视化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置中文字体，避免乱码\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False          # 解决负号显示成方块的问题\n",
    "\n",
    "# 机器学习 & 优化\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "# 化学信息学 (RDKit)\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import (\n",
    "    Descriptors, Crippen, rdMolDescriptors,\n",
    "    MACCSkeys, RDKFingerprint, rdFingerprintGenerator\n",
    ")\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "# 关闭 RDKit 的警告\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Avalon 指纹（可选）\n",
    "try:\n",
    "    from rdkit.Avalon import pyAvalonTools\n",
    "    avalon_available = True\n",
    "except ImportError:\n",
    "    avalon_available = False\n",
    "print(f\"Avalon available: {avalon_available}\")\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe_connected\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = r'D:\\数据\\Kaggle\\Thermophysical Property Melting Point'\n",
    "else:\n",
    "    dir = os.getcwd()\n",
    "\n",
    "\n",
    "DIRS = {\n",
    "    \"dir\":              dir,                                       \n",
    "    \"DATA_DIR000\":      os.path.join(dir, \"DATA_DIR000\"),\n",
    "    \"HISTORY\":          os.path.join(dir, \"HISTORY\"),\n",
    "    \"SUBMISSION\":       os.path.join(dir, \"SUBMISSION\"),\n",
    "}\n",
    "\n",
    "# 自动创建目录\n",
    "for key, path in DIRS.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# 打印时一行一个地址\n",
    "print(\"✅ 路径已创建：\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f677d",
   "metadata": {},
   "source": [
    "# 数据提取处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Kaggle 训练集和 Bradley 熔点公开数据集\n",
    "\n",
    "# Kaggle 提供的训练集和测试集\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "# 外部 Bradley 熔点公开数据集\n",
    "bradley_df = pd.read_excel(os.path.join(DIRS['DATA_DIR000'], \"BradleyMeltingPointDataset.xlsx\"))\n",
    "bradleyplus_df = pd.read_excel(os.path.join(DIRS['DATA_DIR000'], \"BradleyDoublePlusGoodMeltingPointDataset.xlsx\"))\n",
    "\n",
    "# 只保留需要的列\n",
    "train_df = train_df[['SMILES', 'Tm']]\n",
    "test_df  = test_df[['id', 'SMILES']]\n",
    "\n",
    "# 输出数据集规模，确认加载成功\n",
    "print(\"Train                        shape:\", train_df.shape)\n",
    "print(\"Test                         shape:\", test_df.shape)\n",
    "print(\"Bradley dataset              shape:\", bradley_df.shape)\n",
    "print(\"Bradley Plus Good dataset    shape:\", bradleyplus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部 Bradley 熔点数据集处理 & 合并Kaggle 训练集\n",
    "\n",
    "# 1. 摄氏度 → 开尔文: T(K) = T(°C) + 273.15\n",
    "bradley_df['Tm']     = bradley_df['mpC'] + 273.15\n",
    "bradleyplus_df['Tm'] = bradleyplus_df['mpC'] + 273.15\n",
    "\n",
    "# 2. 保留 [SMILES, Tm] 并统一列名\n",
    "bradley_df     = bradley_df[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
    "bradleyplus_df = bradleyplus_df[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
    "\n",
    "# 打印原始信息\n",
    "print(f\"📊 Kaggle 训练集    shape    : {train_df.shape}\")\n",
    "print(f\"📊 Bradley          shape    : {bradley_df.shape}\")\n",
    "print(f\"📊 Bradley Plus     shape    : {bradleyplus_df.shape}\")\n",
    "\n",
    "# 3. 合并 Bradley & Bradley Plus\n",
    "bradley_merge = pd.concat([bradley_df, bradleyplus_df], axis=0).reset_index(drop=True)\n",
    "print(f\"📊 Bradley 合并后   shape    : {bradley_merge.shape}\")\n",
    "\n",
    "# 4. 拼接到 Kaggle 训练集\n",
    "merge_df = pd.concat([train_df, bradley_merge], axis=0).reset_index(drop=True)\n",
    "print(f\"📊 拼接后 merge_df  shape    : {merge_df.shape}\")\n",
    "\n",
    "# 5. 去重处理\n",
    "dup_count = merge_df.duplicated(subset=['SMILES', 'Tm']).sum()\n",
    "print(f\"⚠️ 发现重复数据条数          : {dup_count}\")\n",
    "\n",
    "merge_df = merge_df.drop_duplicates(subset=['SMILES', 'Tm']).reset_index(drop=True)\n",
    "print(f\"✅ 去重后 merge_df  shape    : {merge_df.shape}\")\n",
    "\n",
    "# 6. 最终确认\n",
    "print(\"🎯 数据合并 & 去重完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6200fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有分子描述符 (Descriptors)\n",
    "def extract_all_descriptors(df, SMILES_col):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        df         : DataFrame，包含 SMILES 列\n",
    "        SMILES_col : 字符串，SMILES 列的名称\n",
    "    输出:\n",
    "        DataFrame，原始数据 + 208 个分子描述符\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 获取 RDKit 内置的分子描述符\n",
    "    descriptor_list = Descriptors._descList   # [(name, func), ...]\n",
    "    descriptors = [desc[0] for desc in descriptor_list]\n",
    "    print(f\"📊 一共存在 {len(descriptors)} 个分子描述符特征\")\n",
    "\n",
    "    # 2. 遍历每个分子，计算描述符\n",
    "    results = []\n",
    "    total = len(df)\n",
    "    for idx, smi in enumerate(df[SMILES_col]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        if mol is None:\n",
    "            row = {name: None for name, func in descriptor_list}   # 无效 SMILES\n",
    "        else:\n",
    "            row = {name: func(mol) for name, func in descriptor_list}  # 有效 SMILES\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # 打印进度条（覆盖式打印）\n",
    "        print(f\"🔄 处理进度: {idx+1:5d}/{total:5d}\", end=\"\\r\", flush=True)\n",
    "    print(\"\\n✅ 描述符计算完成\")\n",
    "\n",
    "    # 3. 合并原始数据与新特征\n",
    "    df_desc = pd.DataFrame(results)\n",
    "    return pd.concat([df, df_desc], axis=1)\n",
    "\n",
    "\n",
    "# ============ 应用函数 ============\n",
    "merge_df = extract_all_descriptors(merge_df, \"SMILES\")\n",
    "test_df  = extract_all_descriptors(test_df, \"SMILES\")\n",
    "\n",
    "# 删除无效数据 (有 NaN 的行)\n",
    "merge_df = merge_df.dropna().reset_index(drop=True)\n",
    "test_df  = test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ merge_df shape = {merge_df.shape}\")\n",
    "print(f\"✅ test_df shape  = {test_df.shape}\")\n",
    "\n",
    "\n",
    "# # 保存到 CSV\n",
    "# merge_path = os.path.join(DIRS['DATA_DIR000'], \"merge_descriptors.csv\")\n",
    "# test_path  = os.path.join(DIRS['DATA_DIR000'], \"test_descriptors.csv\")\n",
    "# merge_df.to_csv(merge_path, index=False)\n",
    "# test_df.to_csv(test_path, index=False)\n",
    "\n",
    "# print(f\"✅ merge_df shape = {merge_df.shape}，已保存到 {merge_path}\")\n",
    "# print(f\"✅ test_df shape  = {test_df.shape}，已保存到 {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有分子指纹 (Fingerprints)\n",
    "def extract_all_fingerprint(df, SMILES_col, morgan_radius=2, morgan_nbits=1024):\n",
    "    \"\"\"\n",
    "    输入参数:\n",
    "        df            : DataFrame，包含 SMILES 的表格\n",
    "        SMILES_col    : str，SMILES 所在列的列名\n",
    "        morgan_radius : int，Morgan 指纹半径 (默认=2)\n",
    "        morgan_nbits  : int，Morgan/FCFP/AtomPair 指纹长度 (默认=1024)\n",
    "\n",
    "    返回:\n",
    "        DataFrame，原始数据 + 多种分子指纹特征\n",
    "    \"\"\"\n",
    "\n",
    "    fps_data = []   # 存储所有分子的指纹特征字典\n",
    "\n",
    "    # 1. 定义指纹生成器\n",
    "    morgan_gen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=morgan_radius, fpSize=morgan_nbits,\n",
    "        countSimulation=True, includeChirality=False\n",
    "    )\n",
    "    fcfp = rdFingerprintGenerator.GetMorganFeatureAtomInvGen()\n",
    "    fcfp_gen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=morgan_radius, fpSize=morgan_nbits,\n",
    "        atomInvariantsGenerator=fcfp, countSimulation=True, includeChirality=False\n",
    "    )\n",
    "    atom_gen = rdFingerprintGenerator.GetAtomPairGenerator(\n",
    "        fpSize=morgan_nbits, countSimulation=True, includeChirality=False\n",
    "    )\n",
    "\n",
    "    # 2. 遍历分子，提取指纹\n",
    "    total = len(df)\n",
    "    for idx, smi in enumerate(df[SMILES_col]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            fps_data.append({})\n",
    "            print(f\"⚠ 无效 SMILES: {smi}\")\n",
    "            continue\n",
    "\n",
    "        feature_row = {}\n",
    "\n",
    "        # 2.1 Morgan 指纹 (ECFP)\n",
    "        morgan_fp = morgan_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"Morgan_{i}\"] = morgan_fp[i]\n",
    "\n",
    "        # 2.2 功能类 Morgan (FCFP)\n",
    "        fcfp_fp = fcfp_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"FCFP_{i}\"] = fcfp_fp[i]\n",
    "\n",
    "        # 2.3 MACCS Keys (固定 167 位)\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        for i in range(len(maccs_fp)):\n",
    "            feature_row[f\"MACCS_{i}\"] = int(maccs_fp[i])\n",
    "\n",
    "        # 2.4 AtomPair 指纹\n",
    "        atompair_fp = atom_gen.GetCountFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"AtomPair_{i}\"] = atompair_fp[i]\n",
    "\n",
    "        # 2.5 RDKit 内置指纹\n",
    "        rdkit_fp = RDKFingerprint(mol)\n",
    "        for i in range(len(rdkit_fp)):\n",
    "            feature_row[f\"RDKIT_{i}\"] = int(rdkit_fp[i])\n",
    "\n",
    "        # 2.6 Avalon 指纹 (若可用)\n",
    "        if avalon_available:\n",
    "            avalon_fp = pyAvalonTools.GetAvalonFP(mol, morgan_nbits)\n",
    "            for i in range(len(avalon_fp)):\n",
    "                feature_row[f\"Avalon_{i}\"] = int(avalon_fp[i])\n",
    "\n",
    "        fps_data.append(feature_row)\n",
    "        print(f\"🔄 指纹提取进度: {idx+1:5d}/{total:5d}\", end=\"\\r\", flush=True)\n",
    "    print(\"\\n✅ 分子指纹计算完成\")\n",
    "\n",
    "    # 3. 合并结果并返回\n",
    "    fps_df = pd.DataFrame(fps_data)\n",
    "    return pd.concat([df, fps_df], axis=1)\n",
    "\n",
    "\n",
    "# ============ 应用函数 ============\n",
    "merge_df = extract_all_fingerprint(merge_df, \"SMILES\")\n",
    "test_df  = extract_all_fingerprint(test_df, \"SMILES\")\n",
    "\n",
    "print(f\"✅ merge_df shape = {merge_df.shape}\")\n",
    "print(f\"✅ test_df shape  = {test_df.shape}\")\n",
    "\n",
    "# # 保存结果\n",
    "# merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "# test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "# merge_df.to_csv(merge_fp_path, index=False)\n",
    "# test_df.to_csv(test_fp_path, index=False)\n",
    "\n",
    "# print(f\"✅ merge_df shape = {merge_df.shape}，已保存到 {merge_fp_path}\")\n",
    "# print(f\"✅ test_df shape  = {test_df.shape}，已保存到 {test_fp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88ece8",
   "metadata": {},
   "source": [
    "# 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def loaddata(DIRS):\n",
    "    # 定义路径\n",
    "    merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "    test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "    # 读取数据\n",
    "    merge_df = pd.read_csv(merge_fp_path)\n",
    "    test_df  = pd.read_csv(test_fp_path)\n",
    "\n",
    "    # 打印信息\n",
    "    print(f\"✅ merge_df 加载完成，shape = {merge_df.shape}\")\n",
    "    print(f\"✅ test_df  加载完成，shape = {test_df.shape}\")\n",
    "\n",
    "    print(\"特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\")\n",
    "    print(\"合计特征总数 = 6528\")\n",
    "\n",
    "    return  merge_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "merge_df, test_df =  loaddata(DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c871162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印清单\n",
    "def config_to_str(config: dict, indent: int = 0) -> str:\n",
    "    \"\"\"递归生成配置字符串\"\"\"\n",
    "    prefix = \"     \" * indent\n",
    "    lines = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}🔹 {key}:\")\n",
    "            lines.append(config_to_str(value, indent + 1))  # 递归拼接子字典\n",
    "        else:\n",
    "            lines.append(f\"{prefix}- {key:<20}: {value}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f737573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置单\n",
    "config = {\n",
    "    # 固定开关\n",
    "    \"ISTEST\"            : True,\n",
    "\n",
    "    \"remove_dup_smiles\" : True, \n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 100,\n",
    "\n",
    "    # 特征选择 XGBoost 参数\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:absoluteerror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"mean\",   \n",
    "\n",
    "    # 训练设置\n",
    "    \"xgb_train_model_params\": {\n",
    "        'max_depth'   : 6,\n",
    "        'eta'         : 0.1,\n",
    "        'tree_method' : 'hist',\n",
    "        'eval_metric' : 'mae',\n",
    "    },\n",
    "    \"num_boost_round\": 15000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据拆分 (特征矩阵 与 目标向量)\n",
    "# ============================================\n",
    "# 特征字段: SMILES, Tm | 描述符: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
    "# 合计特征总数 = 6528\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_features_and_target(merge_df: pd.DataFrame, test_df: pd.DataFrame, config: dict):\n",
    "    \"\"\"\n",
    "    数据拆分函数：构造训练集和测试集的特征矩阵与目标向量\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1. 检查并处理重复 SMILES\n",
    "    if config[\"remove_dup_smiles\"]:\n",
    "\n",
    "        dup_smiles = set(merge_df['SMILES']) & set(test_df['SMILES'])\n",
    "        print(f\"⚠️ 检测到 {len(dup_smiles)} 个重复 SMILES\")\n",
    "\n",
    "        before_shape = merge_df.shape\n",
    "        # 删除训练集中出现在测试集的 SMILES，避免数据泄漏\n",
    "        merge_df = merge_df[~merge_df['SMILES'].isin(test_df['SMILES'])].reset_index(drop=True)\n",
    "        after_shape = merge_df.shape\n",
    "\n",
    "        print(f\"✅ 删除完成: 从 {before_shape} → {after_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 构造特征矩阵和目标向量\n",
    "    features_train = merge_df.drop(columns=['SMILES', 'Tm'])   # 训练集特征 (X)\n",
    "    target_train   = merge_df['Tm']                            # 训练集目标 (y, 熔点)\n",
    "    features_test  = test_df.drop(columns=['SMILES', 'id'])    # 测试集特征 (无 Tm)\n",
    "\n",
    "\n",
    "    # 随机选取部分特征（示例：50 个）\n",
    "    if config[\"ISTEST\"]:\n",
    "        np.random.seed(42)\n",
    "        selected_features = np.random.choice(\n",
    "            merge_df.drop(columns=['SMILES', 'Tm']).columns,\n",
    "            size=110,\n",
    "            replace=False\n",
    "        )\n",
    "        sample_len = 100\n",
    "        features_train = merge_df.iloc[:sample_len][selected_features]   # 训练特征 (前 1000 条)\n",
    "        target_train = merge_df.iloc[:sample_len]['Tm']               # 训练目标\n",
    "        features_test = test_df[selected_features]          # 测试特征 (同样的特征列)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 打印维度信息\n",
    "    print(\"📊 数据拆分完成\")\n",
    "    print(f\"训练集特征 features_train  shape   : {features_train.shape}\")\n",
    "    print(f\"训练集目标   target_train  shape   : {target_train.shape}\")\n",
    "    print(f\"测试集特征  features_test  shape   : {features_test.shape}\")\n",
    "    print(f\"           features_train  类型    : {type(features_train)}\")\n",
    "\n",
    "    return features_train, target_train, features_test\n",
    "\n",
    "features_train, target_train, features_test = prepare_features_and_target(merge_df, test_df, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2724c74",
   "metadata": {},
   "source": [
    "### 非零占比分布的直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_nonzero_ratio_hist(features_train: pd.DataFrame, features_test: pd.DataFrame, bins_size: int = 10):\n",
    "    \"\"\"\n",
    "    绘制并比较训练集和测试集每列非零占比分布的直方图\n",
    "\n",
    "    参数:\n",
    "        features_train : pd.DataFrame\n",
    "            训练集特征矩阵\n",
    "        features_test  : pd.DataFrame\n",
    "            测试集特征矩阵\n",
    "        bins_size : int, 默认=10\n",
    "            分箱数量 (0%~100% 区间划分)\n",
    "\n",
    "    返回:\n",
    "        train_counts, test_counts : np.ndarray\n",
    "            两个数据集在各区间内的列数\n",
    "    \"\"\"\n",
    "    # 逐列非零占比（百分比形式）\n",
    "    train_ratio = features_train.apply(lambda col: (col != 0).mean() * 100)\n",
    "    test_ratio  = features_test.apply(lambda col: (col != 0).mean() * 100)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # 绘制直方图 (density=True 表示频率形式)\n",
    "    counts1, bins1, _ = plt.hist(train_ratio, bins=bins_size, alpha=0.6, \n",
    "                                 label=\"features_train\", density=True)\n",
    "    counts2, bins2, _ = plt.hist(test_ratio, bins=bins_size, alpha=0.6, \n",
    "                                 label=\"features_test\", density=True)\n",
    "\n",
    "    # 分别计算数量（而不是频率）\n",
    "    train_counts, _ = np.histogram(train_ratio, bins=bins1)\n",
    "    test_counts, _ = np.histogram(test_ratio, bins=bins2)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"features_train 各区间数量：\")\n",
    "    for i in range(len(bins1)-1):\n",
    "        print(f\"{bins1[i]:.0f}% - {bins1[i+1]:.0f}% : {train_counts[i]} 列\")\n",
    "\n",
    "    print(\"\\nfeatures_test 各区间数量：\")\n",
    "    for i in range(len(bins2)-1):\n",
    "        print(f\"{bins2[i]:.0f}% - {bins2[i+1]:.0f}% : {test_counts[i]} 列\")\n",
    "\n",
    "    # 在柱子上标注数量\n",
    "    for c, b in zip(train_counts, bins1[:-1]):\n",
    "        if c > 0:\n",
    "            plt.text(b + (bins1[1]-bins1[0])/2, 0.01, str(c), \n",
    "                     ha=\"center\", va=\"bottom\", fontsize=8, color=\"black\", rotation=90)\n",
    "\n",
    "    for c, b in zip(test_counts, bins2[:-1]):\n",
    "        if c > 0:\n",
    "            plt.text(b + (bins2[1]-bins2[0])/2, 0.03, str(c), \n",
    "                     ha=\"center\", va=\"bottom\", fontsize=8, color=\"blue\", rotation=90)\n",
    "\n",
    "    plt.xlabel(\"非零占比 (%)\")\n",
    "    plt.ylabel(\"频率 (Frequency, 0~1)\")\n",
    "    plt.title(\"各列非零占比分布直方图\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return train_counts, test_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_counts, test_counts = plot_nonzero_ratio_hist(features_train, features_test, bins_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f45be",
   "metadata": {},
   "source": [
    "### 特征生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_chemical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    基于分子描述符构造新的衍生特征\n",
    "    输入:\n",
    "        df : pd.DataFrame，必须包含以下列：\n",
    "            ['NumHDonors', 'NumHAcceptors', 'MolLogP', 'TPSA',\n",
    "            'NumRotatableBonds', 'MolWt', 'NumAromaticRings', 'BertzCT']\n",
    "    输出:\n",
    "        df_new : pd.DataFrame，包含新增特征\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['HBond_Product']        = df['NumHDonors'] * df['NumHAcceptors']\n",
    "    df['HBond_Sum']            = df['NumHDonors'] + df['NumHAcceptors']\n",
    "    df['LogP_div_TPSA']        = df['MolLogP'] / (df['TPSA'] + 1)\n",
    "    df['LogP_x_TPSA']          = df['MolLogP'] * df['TPSA']\n",
    "    df['Flexibility_Score']    = df['NumRotatableBonds'] / (df['MolWt'] + 1)\n",
    "    df['MolWt_x_AromaticRings']= df['MolWt'] * df['NumAromaticRings']\n",
    "    df['Complexity_per_MW']    = df['BertzCT'] / (df['MolWt'] + 1)\n",
    "    df['Rigidity_Score']       = df['NumAromaticRings'] / (df['NumRotatableBonds'] + 1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280348cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"use_feature_gen\"]:\n",
    "    features_train = add_chemical_features(features_train)\n",
    "    features_test  = add_chemical_features(features_test)\n",
    "\n",
    "features_train.shape, features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4582b1",
   "metadata": {},
   "source": [
    "### PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "def apply_truncated_svd(df: pd.DataFrame, n_components: int = 100, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    使用 TruncatedSVD 对 DataFrame 进行降维\n",
    "    输入:\n",
    "        df           : pd.DataFrame，特征矩阵（需去掉 ID / label 等非特征列）\n",
    "        n_components : int，降维后的目标维度\n",
    "        random_state : int，随机种子\n",
    "    输出:\n",
    "        reduced_df   : pd.DataFrame，降维后的结果，保持原行索引\n",
    "    \"\"\"\n",
    "    # 转换为稀疏矩阵\n",
    "    X_sparse = sparse.csr_matrix(df.values)\n",
    "\n",
    "    # 初始化 SVD\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    # 训练并降维\n",
    "    X_reduced_array = svd.fit_transform(X_sparse)\n",
    "\n",
    "    # 包装为 DataFrame\n",
    "    reduced_df = pd.DataFrame(\n",
    "        X_reduced_array,\n",
    "        index=df.index,\n",
    "        columns=[f\"SVD_{i+1}\" for i in range(X_reduced_array.shape[1])]\n",
    "    )\n",
    "    # 方差解释率\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "\n",
    "    # 打印信息\n",
    "    print( \"原始维度         : \", df.shape)\n",
    "    print( \"降维后           : \", reduced_df.shape) \n",
    "    print(f\"累计解释方差比   :  {explained_var:.2%}\")\n",
    "\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717978bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据降维\n",
    "if config[\"use_pca\"]:\n",
    "    features_train_reduced = apply_truncated_svd(features_train, n_components = 100)\n",
    "    features_test_reduced = apply_truncated_svd(features_test, n_components = 100)\n",
    "\n",
    "    features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "    features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "features_train.shape, features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f752",
   "metadata": {},
   "source": [
    "# 单次训练推导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold + XGBoost 进行训练验证，并保存实验结果\n",
    "# ==============================================================\n",
    "def run_kfold_xgb(features_train, target_train, features_test, config, DIRS, K_FOLDS=10, verbose=0):\n",
    "    \"\"\"\n",
    "    使用 Stratified K-Fold + XGBoost 进行训练验证，并保存实验结果\n",
    "\n",
    "    参数:\n",
    "        features_train, target_train        : 训练集特征和标签\n",
    "        features_test      : 测试集特征\n",
    "        params      : XGBoost 最优参数 (dict)\n",
    "        DIRS        : 保存结果的目录字典\n",
    "        K_FOLDS     : 折数 (默认=5)\n",
    "        verbose     : 是否打印详细信息\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "    config[\"X shape\"] = features_train.shape\n",
    "    config[\"y shape\"] = target_train.shape\n",
    "    config[\"X_test shape\"] = features_test.shape\n",
    "\n",
    "\n",
    "    # ---------- 创建目录 ----------\n",
    "    for _, path in DIRS.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = os.path.join(DIRS['HISTORY'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"——\" * 20)\n",
    "    print(f\"✅ 当前结果将保存到: {time_str}\")\n",
    "\n",
    "\n",
    "    # ---------- 定义交叉验证 ----------\n",
    "    skfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    yeo = PowerTransformer(method=\"yeo-johnson\")                                # 定义 Yeo-Johnson 变换\n",
    "\n",
    "    # ---------- 初始化存储 ----------\n",
    "    oof_val = np.zeros(len(features_train))       # OOF 预测\n",
    "    train_score, val_score = [], []  # 每折 MAE\n",
    "    test_pred = []                   # 每折 test 预测\n",
    "    fold_records = []                # 保存每折信息\n",
    "    all_importances = []             # 特征重要性\n",
    "    elapsed_list = []                # 耗时记录\n",
    "\n",
    "\n",
    "\n",
    "    # 循环每一折\n",
    "    # ==============================================================\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skfold.split(features_train, pd.qcut(target_train, q=10).cat.codes), 1):\n",
    "\n",
    "        # ----- 打印时间信息 -----\n",
    "        start_now = datetime.now()\n",
    "        start_str = start_now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if elapsed_list:\n",
    "            avg_time = np.mean(elapsed_list)\n",
    "            est_end = start_now + timedelta(seconds=avg_time)\n",
    "\n",
    "            # 每 5 个一组输出耗时\n",
    "            parts = [f\"{t:6.1f}s\" for t in elapsed_list]\n",
    "            grouped = [\" \".join(parts[j:j+5]) for j in range(0, len(parts), 5)]\n",
    "            elapsed_str = \" /// \".join(grouped)\n",
    "\n",
    "            print(\n",
    "                f\"🔄{i:2d}/{K_FOLDS} ST {start_str}\"\n",
    "                f\" ET {est_end.strftime('%H:%M:%S')}\"\n",
    "                f\" avg {avg_time:.1f}s\"\n",
    "                f\" [{elapsed_str}]\",\n",
    "                end=\"\\r\", flush=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"🔄{i:2d}/{K_FOLDS} ST {start_str} ET (暂无历史数据)\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "        # ----- 开始训练 -----\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. 数据集划分\n",
    "        x_train, x_val = features_train.iloc[train_idx], features_train.iloc[val_idx]\n",
    "        y_train, y_val = target_train[train_idx], target_train[val_idx]\n",
    "\n",
    "        # 2. Yeo-Johnson 变换\n",
    "        y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "        y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "\n",
    "        # 3. 特征选择（轻量级 XGBoost）\n",
    "        # 使用\n",
    "        selector_model = xgb.XGBRegressor(**config[\"xgb_selector_model_params\"])\n",
    "        # selector_model = xgb.XGBRegressor(\n",
    "        #     n_estimators   = 500,\n",
    "        #     max_depth      = 6,\n",
    "        #     learning_rate  = 0.05,\n",
    "        #     random_state   = 2025,\n",
    "        #     device         = \"cpu\",\n",
    "        #     objective      = \"reg:absoluteerror\",\n",
    "        #     tree_method    = \"hist\",\n",
    "        #     verbosity      = 0\n",
    "        # )\n",
    "        \n",
    "        \n",
    "\n",
    "        selector_model.fit(x_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(selector_model, prefit=True, threshold=config[\"selector_threshold\"])\n",
    "        selected_features = x_train.columns[selector.get_support()].tolist()\n",
    "        if verbose > 0:\n",
    "            print(f\"✅ 选择的特征数量: {len(selected_features)}\")\n",
    "\n",
    "\n",
    "        # 4. 保留重要特征\n",
    "        x_train_new = x_train[selected_features]\n",
    "        x_val_new   = x_val[selected_features]\n",
    "        x_test_new  = features_test[selected_features]\n",
    "\n",
    "        # 5. 转换为 DMatrix\n",
    "        dtrain = xgb.DMatrix(x_train_new, y_train, feature_names=selected_features)\n",
    "        dval   = xgb.DMatrix(x_val_new,   y_val,   feature_names=selected_features)\n",
    "        dtest  = xgb.DMatrix(x_test_new,             feature_names=selected_features)\n",
    "\n",
    "\n",
    "        # 6. XGBoost 训练\n",
    "        xgb_model = xgb.train(\n",
    "            params                 = config[\"xgb_train_model_params\"],\n",
    "            dtrain                 = dtrain,\n",
    "            num_boost_round        = config[\"num_boost_round\"],\n",
    "            evals                  = [(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds  = 300,\n",
    "            verbose_eval           = (1000 if verbose > 0 else False)\n",
    "        )\n",
    "\n",
    "\n",
    "        # 保存模型\n",
    "        model_path = os.path.join(history_DIR, f\"xgb_model_fold{i}.json\")\n",
    "        xgb_model.save_model(model_path)\n",
    "\n",
    "        # 7. 获取特征重要性\n",
    "        imp_dict = xgb_model.get_score(importance_type=\"gain\")\n",
    "        imp_df = pd.DataFrame(imp_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "        imp_df[\"Fold\"] = i\n",
    "        all_importances.append(imp_df)\n",
    "\n",
    "\n",
    "        # 8. 预测\n",
    "        y_train_pred = xgb_model.predict(dtrain)\n",
    "        y_val_pred   = xgb_model.predict(dval)\n",
    "        y_test_pred  = xgb_model.predict(dtest)\n",
    "\n",
    "        # 9. 逆变换\n",
    "        y_train      = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "        y_val        = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "        y_train_pred = yeo.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
    "        y_val_pred   = yeo.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
    "        y_test_pred  = yeo.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        # 10. 计算 MAE\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        val_mae   = mean_absolute_error(y_val,   y_val_pred)\n",
    "        if verbose > 0:\n",
    "            print(f\"Fold {i}: Train MAE={train_mae:.4f}, Val MAE={val_mae:.4f}，用时 {elapsed:.2f} 秒\")\n",
    "\n",
    "\n",
    "        # ----- 保存结果 -----\n",
    "        train_score.append(train_mae)\n",
    "        val_score.append(val_mae)\n",
    "        oof_val[val_idx] = y_val_pred\n",
    "        test_pred.append(y_test_pred)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        elapsed_list.append(elapsed)\n",
    "\n",
    "        fold_records.append({\n",
    "            \"Fold\": i,\n",
    "            \"Train_MAE\": train_mae,\n",
    "            \"Val_MAE\": val_mae,\n",
    "            \"Num_Features\": len(selected_features),\n",
    "            \"Selected_Features\": selected_features,\n",
    "            \"elapsed\": elapsed\n",
    "        })\n",
    "\n",
    "    # 保存整体结果\n",
    "    # ==============================================================\n",
    "    if verbose > 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"📊 Train MAE 平均值 : {np.mean(train_score):.4f}\")\n",
    "        print(f\"📊 Val   MAE 平均值 : {np.mean(val_score):.4f}\")\n",
    "        print(f\"📊 Train MAE 标准差 : {np.std(train_score, ddof=0):.4f}\")\n",
    "        print(f\"📊 Val   MAE 标准差 : {np.std(val_score, ddof=0):.4f}\")\n",
    "\n",
    "    # 参数\n",
    "    with open(os.path.join(history_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # 每折信息\n",
    "    folds_df = pd.DataFrame(fold_records)\n",
    "    folds_df.to_csv(os.path.join(history_DIR, \"folds_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # 特征重要性\n",
    "    if all_importances:\n",
    "        valid_imps = [df for df in all_importances if not df.empty]\n",
    "        all_imp_df = pd.concat(valid_imps, axis=0) if valid_imps else pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    else:\n",
    "        all_imp_df = pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    all_imp_df.to_csv(os.path.join(history_DIR, \"feature_importance_all.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # 测试集预测\n",
    "    test_pred_array = np.vstack(test_pred).T\n",
    "    test_pred_df = pd.DataFrame(test_pred_array, columns=[f\"Fold_{j+1}\" for j in range(test_pred_array.shape[1])])\n",
    "    test_pred_df[\"Final_Pred\"] = test_pred_df.mean(axis=1)\n",
    "    test_pred_df.to_csv(os.path.join(history_DIR, \"test_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 总结\n",
    "    with open(os.path.join(history_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train MAE Mean : {np.mean(train_score):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Mean : {np.mean(val_score):.4f}\\n\")\n",
    "        f.write(f\"Train MAE Std  : {np.std(train_score, ddof=0):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Std  : {np.std(val_score, ddof=0):.4f}\\n\")\n",
    "\n",
    "\n",
    "    # 最终提交\n",
    "    final_score = np.mean(val_score)\n",
    "    submission = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"sample_submission.csv\"))\n",
    "    submission[\"Tm\"] = test_pred_df[\"Final_Pred\"]\n",
    "\n",
    "    submission_path = os.path.join(history_DIR, f\"sub_{time_str}_{final_score:.8f}.csv\")\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    submission.to_csv(os.path.join(DIRS['SUBMISSION'], f\"sub_{time_str}_{final_score:.8f}.csv\"), index=False)\n",
    "\n",
    "        \n",
    "    config[\"time_str\"] = time_str\n",
    "    config[\"score\"] = final_score\n",
    "\n",
    "\n",
    "    # ---------- 返回结果 ----------\n",
    "    return {\n",
    "        \"oof_val\": oof_val,\n",
    "        \"train_score\": train_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_pred\": test_pred_df,\n",
    "        \"folds_info\": folds_df,\n",
    "        \"feature_importance\": all_imp_df,\n",
    "        \"submission_path\": submission_path,\n",
    "        \"time\": time_str,\n",
    "        \"final_score\": final_score,\n",
    "        \"config\": config\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行一次\n",
    "\n",
    "X = features_train\n",
    "y = target_train\n",
    "X_test = features_test\n",
    "print(X.shape, X_test.shape)\n",
    "\n",
    "\n",
    "results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "config = results['config']\n",
    "\n",
    "print('\\n',results['final_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印当前config\n",
    "print(config_to_str(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3573b32",
   "metadata": {},
   "source": [
    "# 提交 kaggle 平台测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据 submission_time 定位文件路径 提交 kaggle 平台测试\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def find_submission_file(submission_time, submission_dir):\n",
    "    \"\"\"\n",
    "    在 submission_dir 下查找包含 submission_time 的文件\n",
    "    一旦找到立刻返回完整路径；如果没找到则返回 None\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(submission_dir):\n",
    "        if submission_time in fname:\n",
    "            file_path = os.path.join(submission_dir, fname)\n",
    "            print(f\"✅ 找到目标文件: {fname}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(f\"⚠️ 未找到包含 {submission_time} 的文件\")\n",
    "    return None\n",
    "\n",
    "def submit_and_get_score(file_path, competition_name, message=\"My submission\"):\n",
    "    \"\"\"\n",
    "    封装 Kaggle 提交并等待结果评分\n",
    "    --------------------------------------\n",
    "    file_path        : str  提交文件路径\n",
    "    competition_name : str  Kaggle 比赛名称 (URL 最后一段)\n",
    "    message          : str  提交备注\n",
    "    \"\"\"\n",
    "    # 1. 配置 Kaggle API\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = r\"C:\\Users\\Admin\\.kaggle\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"✅ Kaggle API 已经配置成功！\")\n",
    "\n",
    "    # 2. 提交文件\n",
    "    api.competition_submit(\n",
    "        file_name=file_path,\n",
    "        competition=competition_name,\n",
    "        message=message\n",
    "    )\n",
    "    print(\"✅ 提交完成！请等待评分...\")\n",
    "\n",
    "    # 3. 动态等待\n",
    "    spinner = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
    "    while True:\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        latest = submissions[0]\n",
    "        status_str = str(latest._status).lower()\n",
    "\n",
    "        if \"complete\" in status_str and latest._public_score is not None:\n",
    "            print(\"\\n🎯 最终结果:\")\n",
    "            print(f\"Public 分数 : {latest._public_score}\")\n",
    "            print(f\"Private 分数: {latest._private_score}\")\n",
    "            print(f\"提交 ID     : {latest._ref}\")\n",
    "            print(f\"文件名      : {latest._file_name}\")\n",
    "            print(f\"状态        : {latest._status}\")\n",
    "            print(f\"提交时间    : {latest._date}\")\n",
    "            print(f\"描述/备注   : {latest._description}\")\n",
    "            return latest\n",
    "\n",
    "        spin_char = next(spinner)\n",
    "        print(f\"当前状态: {status_str} , 等待中 {spin_char}\", end=\"\\r\", flush=True)\n",
    "        time.sleep(0.2)  # 每 0.5 秒检查一次\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb1c16",
   "metadata": {},
   "source": [
    "### 不轻易运行，再三考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_time 提交\n",
    "submission_time = \"2025-10-21 23-51-09\"\n",
    "competition_name = \"melting-point\"\n",
    "message =  f\"该提交文件的参数：\\n{config_to_str(config)} \"\n",
    "print(message)\n",
    "\n",
    "target_file = find_submission_file(submission_time, DIRS['SUBMISSION'] )\n",
    "\n",
    "# submit_and_get_score(target_file, competition_name, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9607e",
   "metadata": {},
   "source": [
    "# 参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验配置单\n",
    "base_config  = {\n",
    "    # 固定开关\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"remove_dup_smiles\" : True, \n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 100,\n",
    "\n",
    "    # 特征选择 XGBoost 参数\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:absoluteerror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"mean\",   \n",
    "\n",
    "    # 训练设置\n",
    "    \"xgb_train_model_params\": {\n",
    "        'max_depth'   : 6,\n",
    "        'eta'         : 0.1,\n",
    "        'tree_method' : 'hist',\n",
    "        'eval_metric' : 'mae',\n",
    "    },\n",
    "    \"num_boost_round\": 15000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化任务  加入标识符 host: hao-2   ip: 192.168.40.1\n",
    "\n",
    "import copy\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna 的目标函数 (Objective Function)\n",
    "    每次 trial 会生成一组超参数，用于训练 XGBoost 模型，\n",
    "    并返回交叉验证的平均 RMSE 作为优化目标。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. 定义 超参数 搜索空间\n",
    "    # 拷贝一份 config，避免全局污染\n",
    "    config = copy.deepcopy(base_config)\n",
    "\n",
    "    # 只修改需要优化的参数\n",
    "    config[\"remove_dup_smiles\"] = trial.suggest_categorical(\"remove_dup_smiles\", [True, False])\n",
    "    config[\"use_feature_gen\"]   = trial.suggest_categorical(\"use_feature_gen\", [True, False])\n",
    "    config[\"use_pca\"]           = trial.suggest_categorical(\"use_pca\", [True, False])\n",
    "\n",
    "    # config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_int(\"selector_random_state\", 1, 9999)\n",
    "    config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_categorical(\"selector_random_state\", [42, 2025])\n",
    "    config[\"xgb_selector_model_params\"][\"device\"]       = trial.suggest_categorical(\"selector_device\", [\"cpu\", \"cuda\"])\n",
    "    # config[\"xgb_selector_model_params\"][\"tree_method\"]  = trial.suggest_categorical(\"selector_tree_method\", [\"hist\", \"approx\"])\n",
    "\n",
    "    config[\"selector_threshold\"] = trial.suggest_categorical(\"selector_threshold\", [\"mean\", \"0.75*mean\", \"0.5*mean\", \"1.25*mean\"])\n",
    "\n",
    "    config[\"xgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\"train_max_depth\", 3, 12)\n",
    "    config[\"xgb_train_model_params\"][\"eta\"] = trial.suggest_float(\"train_eta\", 0.01 , 0.3 , log=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 主流程---------------------------------------------------------------------------------------------------\n",
    "    # 创建一个黑洞缓冲区\n",
    "    f = io.StringIO()\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        None\n",
    "\n",
    "    # 打印当前config\n",
    "    print(config_to_str(config))\n",
    "    \n",
    "\n",
    "    # 加载数据\n",
    "    merge_df, test_df =  loaddata(DIRS)\n",
    "\n",
    "    # 数据拆分\n",
    "    print(\"数据拆分---------------------------\")\n",
    "    features_train, target_train, features_test = prepare_features_and_target(merge_df, test_df, config)\n",
    "\n",
    "    # 特征生成\n",
    "    if config[\"use_feature_gen\"]:\n",
    "        print(\"特征生成---------------------------\")\n",
    "        features_train = add_chemical_features(features_train)\n",
    "        features_test  = add_chemical_features(features_test)\n",
    "        print(features_train.shape, features_test.shape)\n",
    "\n",
    "    # 数据降维\n",
    "    if config[\"use_pca\"]:\n",
    "        print(\"数据降维---------------------------\")\n",
    "        features_train_reduced = apply_truncated_svd(features_train, n_components = 100)\n",
    "        features_test_reduced = apply_truncated_svd(features_test, n_components = 100)\n",
    "\n",
    "        features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "        features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "        print(features_train.shape, features_test.shape)\n",
    "\n",
    "    X, y, X_test = features_train, target_train, features_test\n",
    "    print(\"开始训练---------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "    config = results['config']\n",
    "    score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "    HOSTNAME = socket.gethostname()\n",
    "    HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "    trial.set_user_attr(\"host\", HOSTNAME)        # 你自己定义主机 A/B\n",
    "    trial.set_user_attr(\"ip\", HOST_IP)        # 你自己定义角色 A/B\n",
    "\n",
    "    # 4. 返回平均 MAE\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8aec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"test\" if base_config[\"ISTEST\"] else \"optuna_task1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始优化\n",
    "\n",
    "# 1. 定义 SQLite 数据库存储路径\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name = STUDY_NAME,\n",
    "    # study_name=\"ghsdjsrtjrswtjhwrt\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# 自动获取当前主机名\\当前主机的 IP 地址\n",
    "HOSTNAME = socket.gethostname()\n",
    "HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "print(\"主机名:\", HOSTNAME,\" 主机 IP:\", HOST_IP)\n",
    "time.sleep(1)\n",
    "\n",
    "# 5. 启动超参数搜索\n",
    "print(\"🔎 开始超参数搜索...\")\n",
    "if base_config[\"ISTEST\"]:\n",
    "    study.optimize(objective, n_trials = 3)\n",
    "else:\n",
    "    study.optimize(objective, n_trials = 100)\n",
    "\n",
    "\n",
    "# 6. 打印最优结果\n",
    "print(\"\\n✅ 训练完成！\")\n",
    "print(f\"📊 已完成试验次数 : {len(study.trials)}\")\n",
    "print(f\"🏆 最优试验编号   : {study.best_trial.number}\")\n",
    "print(f\"📉 最优 MAE       : {study.best_value}\")\n",
    "print(f\"⚙️ 最优参数组合   : {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c00178",
   "metadata": {},
   "source": [
    "# 管理数据库信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebff3434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据库中的 study 列表:\n",
      "- optuna_task1\n",
      "         Trials:\n",
      "    Trial    0: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 6, 'train_eta': 0.14963770710824598}\n",
      "    Trial    1: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 7, 'train_eta': 0.018021464633564754}\n",
      "    Trial    2: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.021659776125338565}\n",
      "    Trial    3: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 4, 'train_eta': 0.23918065062107188}\n",
      "    Trial    4: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 6, 'train_eta': 0.10392736979117173}\n",
      "    Trial    5: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.24138275928502126}\n",
      "    Trial    6: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.06282544238599601}\n",
      "    Trial    7: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 10, 'train_eta': 0.017725574116549923}\n",
      "    Trial    8: host=hao-2           , ip=192.168.40.1   , value=17.8499   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.01368700824230773}\n",
      "    Trial    9: host=hao-2           , ip=192.168.40.1   , value=17.9953   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.05053010910637573}\n",
      "    Trial   10: host=hao-2           , ip=192.168.40.1   , value=18.9248   , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.026729074261343022}\n",
      "    Trial   11: host=hao-2           , ip=192.168.40.1   , value=18.4746   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 12, 'train_eta': 0.0955937800637472}\n",
      "    Trial   12: host=hao-2           , ip=192.168.40.1   , value=18.8439   , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 5, 'train_eta': 0.01844230194954457}\n",
      "    Trial   13: host=hao-2           , ip=192.168.40.1   , value=18.5825   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 11, 'train_eta': 0.013329388592334063}\n",
      "    Trial   14: host=hao-2           , ip=192.168.40.1   , value=17.5563   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.013678962412610542}\n",
      "    Trial   15: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 6, 'train_eta': 0.03616840082637632}\n",
      "    Trial   16: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 9, 'train_eta': 0.03020978585855579}\n",
      "    Trial   17: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 9, 'train_eta': 0.06269871406279931}\n",
      "    Trial   18: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.0384   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.09158730675112933}\n",
      "    Trial   19: host=hao-2           , ip=192.168.40.1   , value=18.0853   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 9, 'train_eta': 0.045732428074001644}\n",
      "    Trial   20: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.2934   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 5, 'train_eta': 0.01810149024614987}\n",
      "    Trial   21: host=hao-2           , ip=192.168.40.1   , value=17.9752   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.06743144217230011}\n",
      "    Trial   22: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.5059   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 8, 'train_eta': 0.2193719583768128}\n",
      "    Trial   23: host=hao-2           , ip=192.168.40.1   , value=18.0255   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01061463707842147}\n",
      "    Trial   24: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01024884269234489}\n",
      "    Trial   25: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.02880197004042992}\n",
      "    总 trial 数量: 26\n",
      "====================================================================================================\n",
      "- test\n",
      "         Trials:\n",
      "    Trial    0: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.04410160779890175}\n",
      "    Trial    1: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=49.1581   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.04105647603066798}\n",
      "    Trial    2: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 4, 'train_eta': 0.04449261200509654}\n",
      "    总 trial 数量: 3\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 查询数据库详细数据\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "if not studies:\n",
    "    print(\"❌ 当前数据库里无 study\")\n",
    "else:\n",
    "    print(\"✅ 数据库中的 study 列表:\")\n",
    "    for s in studies:\n",
    "\n",
    "        print(\"-\", s.study_name)\n",
    "\n",
    "        study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "\n",
    "        print(\"         Trials:\")\n",
    "        for trial in study.trials:\n",
    "            host = trial.user_attrs.get(\"host\") or \"unknown\"\n",
    "            ip = trial.user_attrs.get(\"ip\") or \"unknown\"\n",
    "            value = f\"{trial.value:.4f}\" if trial.value is not None else \"None\"\n",
    "\n",
    "            print(\n",
    "                f\"    Trial {trial.number:4d}: \"\n",
    "                f\"host={host:<16}, ip={ip:<15}, \"\n",
    "                f\"value={value:<10}, params={trial.params}\"\n",
    "            )\n",
    "\n",
    "        print(\"    总 trial 数量:\", len(study.trials))\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理前：先查看数据库里当前有哪些 study 存在，以及每个 study 里有多少个 trial\n",
    "\n",
    "storage = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"现有 study：\", [s.study_name for s in studies])\n",
    "\n",
    "for s in studies:\n",
    "    study = optuna.load_study(study_name=s.study_name, storage=storage)\n",
    "    print(f\"Study:   {s.study_name:30s}, Trials: {len(study.trials):4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理中：删除指定 study\n",
    "# 指定要删除的名称\n",
    "to_delete = [\"melting_point_study\"]   # 可以写一个或多个\n",
    "\n",
    "to_delete = [            ]\n",
    "\n",
    "for s in studies:\n",
    "    if s.study_name in to_delete:\n",
    "        optuna.delete_study(study_name=s.study_name, storage=storage)\n",
    "        print(\"已删除:\", s.study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce984fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理后：再次检查\n",
    "studies_after = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"清理后 study：\", [s.study_name for s in studies_after])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
