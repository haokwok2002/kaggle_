{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a782e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalon available: True\n",
      "âœ… è·¯å¾„å·²åˆ›å»ºï¼š\n",
      "\n",
      "dir          : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\n",
      "DATA_DIR000  : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\\DATA_DIR000\n",
      "HISTORY      : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\\HISTORY\n",
      "SUBMISSION   : D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point\\SUBMISSION\n"
     ]
    }
   ],
   "source": [
    "# ç³»ç»Ÿåº“\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ç¬¬ä¸‰æ–¹ç§‘å­¦è®¡ç®— & å¯è§†åŒ–\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…ä¹±ç \n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # é»‘ä½“\n",
    "plt.rcParams['axes.unicode_minus'] = False          # è§£å†³è´Ÿå·æ˜¾ç¤ºæˆæ–¹å—çš„é—®é¢˜\n",
    "\n",
    "# æœºå™¨å­¦ä¹  & ä¼˜åŒ–\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "\n",
    "# åŒ–å­¦ä¿¡æ¯å­¦ (RDKit)\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import (\n",
    "    Descriptors, Crippen, rdMolDescriptors,\n",
    "    MACCSkeys, RDKFingerprint, rdFingerprintGenerator\n",
    ")\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "# å…³é—­ RDKit çš„è­¦å‘Š\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Avalon æŒ‡çº¹ï¼ˆå¯é€‰ï¼‰\n",
    "try:\n",
    "    from rdkit.Avalon import pyAvalonTools\n",
    "    avalon_available = True\n",
    "except ImportError:\n",
    "    avalon_available = False\n",
    "print(f\"Avalon available: {avalon_available}\")\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe_connected\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = r'D:\\æ•°æ®\\Kaggle\\Thermophysical Property Melting Point'\n",
    "else:\n",
    "    dir = os.getcwd()\n",
    "\n",
    "\n",
    "DIRS = {\n",
    "    \"dir\":              dir,                                       \n",
    "    \"DATA_DIR000\":      os.path.join(dir, \"DATA_DIR000\"),\n",
    "    \"HISTORY\":          os.path.join(dir, \"HISTORY\"),\n",
    "    \"SUBMISSION\":       os.path.join(dir, \"SUBMISSION\"),\n",
    "}\n",
    "\n",
    "# è‡ªåŠ¨åˆ›å»ºç›®å½•\n",
    "for key, path in DIRS.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# æ‰“å°æ—¶ä¸€è¡Œä¸€ä¸ªåœ°å€\n",
    "print(\"âœ… è·¯å¾„å·²åˆ›å»ºï¼š\\n\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f677d",
   "metadata": {},
   "source": [
    "# æ•°æ®æå–å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½Kaggle è®­ç»ƒé›†å’Œ Bradley ç†”ç‚¹å…¬å¼€æ•°æ®é›†\n",
    "\n",
    "# Kaggle æä¾›çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_df = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"test.csv\"))\n",
    "\n",
    "# å¤–éƒ¨ Bradley ç†”ç‚¹å…¬å¼€æ•°æ®é›†\n",
    "bradley_df = pd.read_excel(os.path.join(DIRS['DATA_DIR000'], \"BradleyMeltingPointDataset.xlsx\"))\n",
    "bradleyplus_df = pd.read_excel(os.path.join(DIRS['DATA_DIR000'], \"BradleyDoublePlusGoodMeltingPointDataset.xlsx\"))\n",
    "\n",
    "# åªä¿ç•™éœ€è¦çš„åˆ—\n",
    "train_df = train_df[['SMILES', 'Tm']]\n",
    "test_df  = test_df[['id', 'SMILES']]\n",
    "\n",
    "# è¾“å‡ºæ•°æ®é›†è§„æ¨¡ï¼Œç¡®è®¤åŠ è½½æˆåŠŸ\n",
    "print(\"Train                        shape:\", train_df.shape)\n",
    "print(\"Test                         shape:\", test_df.shape)\n",
    "print(\"Bradley dataset              shape:\", bradley_df.shape)\n",
    "print(\"Bradley Plus Good dataset    shape:\", bradleyplus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤–éƒ¨ Bradley ç†”ç‚¹æ•°æ®é›†å¤„ç† & åˆå¹¶Kaggle è®­ç»ƒé›†\n",
    "\n",
    "# 1. æ‘„æ°åº¦ â†’ å¼€å°”æ–‡: T(K) = T(Â°C) + 273.15\n",
    "bradley_df['Tm']     = bradley_df['mpC'] + 273.15\n",
    "bradleyplus_df['Tm'] = bradleyplus_df['mpC'] + 273.15\n",
    "\n",
    "# 2. ä¿ç•™ [SMILES, Tm] å¹¶ç»Ÿä¸€åˆ—å\n",
    "bradley_df     = bradley_df[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
    "bradleyplus_df = bradleyplus_df[['smiles', 'Tm']].rename(columns={'smiles': 'SMILES'})\n",
    "\n",
    "# æ‰“å°åŸå§‹ä¿¡æ¯\n",
    "print(f\"ğŸ“Š Kaggle è®­ç»ƒé›†    shape    : {train_df.shape}\")\n",
    "print(f\"ğŸ“Š Bradley          shape    : {bradley_df.shape}\")\n",
    "print(f\"ğŸ“Š Bradley Plus     shape    : {bradleyplus_df.shape}\")\n",
    "\n",
    "# 3. åˆå¹¶ Bradley & Bradley Plus\n",
    "bradley_merge = pd.concat([bradley_df, bradleyplus_df], axis=0).reset_index(drop=True)\n",
    "print(f\"ğŸ“Š Bradley åˆå¹¶å   shape    : {bradley_merge.shape}\")\n",
    "\n",
    "# 4. æ‹¼æ¥åˆ° Kaggle è®­ç»ƒé›†\n",
    "merge_df = pd.concat([train_df, bradley_merge], axis=0).reset_index(drop=True)\n",
    "print(f\"ğŸ“Š æ‹¼æ¥å merge_df  shape    : {merge_df.shape}\")\n",
    "\n",
    "# 5. å»é‡å¤„ç†\n",
    "dup_count = merge_df.duplicated(subset=['SMILES', 'Tm']).sum()\n",
    "print(f\"âš ï¸ å‘ç°é‡å¤æ•°æ®æ¡æ•°          : {dup_count}\")\n",
    "\n",
    "merge_df = merge_df.drop_duplicates(subset=['SMILES', 'Tm']).reset_index(drop=True)\n",
    "print(f\"âœ… å»é‡å merge_df  shape    : {merge_df.shape}\")\n",
    "\n",
    "# 6. æœ€ç»ˆç¡®è®¤\n",
    "print(\"ğŸ¯ æ•°æ®åˆå¹¶ & å»é‡å®Œæˆï¼\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6200fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–æ‰€æœ‰åˆ†å­æè¿°ç¬¦ (Descriptors)\n",
    "def extract_all_descriptors(df, SMILES_col):\n",
    "    \"\"\"\n",
    "    è¾“å…¥:\n",
    "        df         : DataFrameï¼ŒåŒ…å« SMILES åˆ—\n",
    "        SMILES_col : å­—ç¬¦ä¸²ï¼ŒSMILES åˆ—çš„åç§°\n",
    "    è¾“å‡º:\n",
    "        DataFrameï¼ŒåŸå§‹æ•°æ® + 208 ä¸ªåˆ†å­æè¿°ç¬¦\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. è·å– RDKit å†…ç½®çš„åˆ†å­æè¿°ç¬¦\n",
    "    descriptor_list = Descriptors._descList   # [(name, func), ...]\n",
    "    descriptors = [desc[0] for desc in descriptor_list]\n",
    "    print(f\"ğŸ“Š ä¸€å…±å­˜åœ¨ {len(descriptors)} ä¸ªåˆ†å­æè¿°ç¬¦ç‰¹å¾\")\n",
    "\n",
    "    # 2. éå†æ¯ä¸ªåˆ†å­ï¼Œè®¡ç®—æè¿°ç¬¦\n",
    "    results = []\n",
    "    total = len(df)\n",
    "    for idx, smi in enumerate(df[SMILES_col]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        if mol is None:\n",
    "            row = {name: None for name, func in descriptor_list}   # æ— æ•ˆ SMILES\n",
    "        else:\n",
    "            row = {name: func(mol) for name, func in descriptor_list}  # æœ‰æ•ˆ SMILES\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # æ‰“å°è¿›åº¦æ¡ï¼ˆè¦†ç›–å¼æ‰“å°ï¼‰\n",
    "        print(f\"ğŸ”„ å¤„ç†è¿›åº¦: {idx+1:5d}/{total:5d}\", end=\"\\r\", flush=True)\n",
    "    print(\"\\nâœ… æè¿°ç¬¦è®¡ç®—å®Œæˆ\")\n",
    "\n",
    "    # 3. åˆå¹¶åŸå§‹æ•°æ®ä¸æ–°ç‰¹å¾\n",
    "    df_desc = pd.DataFrame(results)\n",
    "    return pd.concat([df, df_desc], axis=1)\n",
    "\n",
    "\n",
    "# ============ åº”ç”¨å‡½æ•° ============\n",
    "merge_df = extract_all_descriptors(merge_df, \"SMILES\")\n",
    "test_df  = extract_all_descriptors(test_df, \"SMILES\")\n",
    "\n",
    "# åˆ é™¤æ— æ•ˆæ•°æ® (æœ‰ NaN çš„è¡Œ)\n",
    "merge_df = merge_df.dropna().reset_index(drop=True)\n",
    "test_df  = test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… merge_df shape = {merge_df.shape}\")\n",
    "print(f\"âœ… test_df shape  = {test_df.shape}\")\n",
    "\n",
    "\n",
    "# # ä¿å­˜åˆ° CSV\n",
    "# merge_path = os.path.join(DIRS['DATA_DIR000'], \"merge_descriptors.csv\")\n",
    "# test_path  = os.path.join(DIRS['DATA_DIR000'], \"test_descriptors.csv\")\n",
    "# merge_df.to_csv(merge_path, index=False)\n",
    "# test_df.to_csv(test_path, index=False)\n",
    "\n",
    "# print(f\"âœ… merge_df shape = {merge_df.shape}ï¼Œå·²ä¿å­˜åˆ° {merge_path}\")\n",
    "# print(f\"âœ… test_df shape  = {test_df.shape}ï¼Œå·²ä¿å­˜åˆ° {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–æ‰€æœ‰åˆ†å­æŒ‡çº¹ (Fingerprints)\n",
    "def extract_all_fingerprint(df, SMILES_col, morgan_radius=2, morgan_nbits=1024):\n",
    "    \"\"\"\n",
    "    è¾“å…¥å‚æ•°:\n",
    "        df            : DataFrameï¼ŒåŒ…å« SMILES çš„è¡¨æ ¼\n",
    "        SMILES_col    : strï¼ŒSMILES æ‰€åœ¨åˆ—çš„åˆ—å\n",
    "        morgan_radius : intï¼ŒMorgan æŒ‡çº¹åŠå¾„ (é»˜è®¤=2)\n",
    "        morgan_nbits  : intï¼ŒMorgan/FCFP/AtomPair æŒ‡çº¹é•¿åº¦ (é»˜è®¤=1024)\n",
    "\n",
    "    è¿”å›:\n",
    "        DataFrameï¼ŒåŸå§‹æ•°æ® + å¤šç§åˆ†å­æŒ‡çº¹ç‰¹å¾\n",
    "    \"\"\"\n",
    "\n",
    "    fps_data = []   # å­˜å‚¨æ‰€æœ‰åˆ†å­çš„æŒ‡çº¹ç‰¹å¾å­—å…¸\n",
    "\n",
    "    # 1. å®šä¹‰æŒ‡çº¹ç”Ÿæˆå™¨\n",
    "    morgan_gen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=morgan_radius, fpSize=morgan_nbits,\n",
    "        countSimulation=True, includeChirality=False\n",
    "    )\n",
    "    fcfp = rdFingerprintGenerator.GetMorganFeatureAtomInvGen()\n",
    "    fcfp_gen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=morgan_radius, fpSize=morgan_nbits,\n",
    "        atomInvariantsGenerator=fcfp, countSimulation=True, includeChirality=False\n",
    "    )\n",
    "    atom_gen = rdFingerprintGenerator.GetAtomPairGenerator(\n",
    "        fpSize=morgan_nbits, countSimulation=True, includeChirality=False\n",
    "    )\n",
    "\n",
    "    # 2. éå†åˆ†å­ï¼Œæå–æŒ‡çº¹\n",
    "    total = len(df)\n",
    "    for idx, smi in enumerate(df[SMILES_col]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            fps_data.append({})\n",
    "            print(f\"âš  æ— æ•ˆ SMILES: {smi}\")\n",
    "            continue\n",
    "\n",
    "        feature_row = {}\n",
    "\n",
    "        # 2.1 Morgan æŒ‡çº¹ (ECFP)\n",
    "        morgan_fp = morgan_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"Morgan_{i}\"] = morgan_fp[i]\n",
    "\n",
    "        # 2.2 åŠŸèƒ½ç±» Morgan (FCFP)\n",
    "        fcfp_fp = fcfp_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"FCFP_{i}\"] = fcfp_fp[i]\n",
    "\n",
    "        # 2.3 MACCS Keys (å›ºå®š 167 ä½)\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        for i in range(len(maccs_fp)):\n",
    "            feature_row[f\"MACCS_{i}\"] = int(maccs_fp[i])\n",
    "\n",
    "        # 2.4 AtomPair æŒ‡çº¹\n",
    "        atompair_fp = atom_gen.GetCountFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_row[f\"AtomPair_{i}\"] = atompair_fp[i]\n",
    "\n",
    "        # 2.5 RDKit å†…ç½®æŒ‡çº¹\n",
    "        rdkit_fp = RDKFingerprint(mol)\n",
    "        for i in range(len(rdkit_fp)):\n",
    "            feature_row[f\"RDKIT_{i}\"] = int(rdkit_fp[i])\n",
    "\n",
    "        # 2.6 Avalon æŒ‡çº¹ (è‹¥å¯ç”¨)\n",
    "        if avalon_available:\n",
    "            avalon_fp = pyAvalonTools.GetAvalonFP(mol, morgan_nbits)\n",
    "            for i in range(len(avalon_fp)):\n",
    "                feature_row[f\"Avalon_{i}\"] = int(avalon_fp[i])\n",
    "\n",
    "        fps_data.append(feature_row)\n",
    "        print(f\"ğŸ”„ æŒ‡çº¹æå–è¿›åº¦: {idx+1:5d}/{total:5d}\", end=\"\\r\", flush=True)\n",
    "    print(\"\\nâœ… åˆ†å­æŒ‡çº¹è®¡ç®—å®Œæˆ\")\n",
    "\n",
    "    # 3. åˆå¹¶ç»“æœå¹¶è¿”å›\n",
    "    fps_df = pd.DataFrame(fps_data)\n",
    "    return pd.concat([df, fps_df], axis=1)\n",
    "\n",
    "\n",
    "# ============ åº”ç”¨å‡½æ•° ============\n",
    "merge_df = extract_all_fingerprint(merge_df, \"SMILES\")\n",
    "test_df  = extract_all_fingerprint(test_df, \"SMILES\")\n",
    "\n",
    "print(f\"âœ… merge_df shape = {merge_df.shape}\")\n",
    "print(f\"âœ… test_df shape  = {test_df.shape}\")\n",
    "\n",
    "# # ä¿å­˜ç»“æœ\n",
    "# merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "# test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "# merge_df.to_csv(merge_fp_path, index=False)\n",
    "# test_df.to_csv(test_fp_path, index=False)\n",
    "\n",
    "# print(f\"âœ… merge_df shape = {merge_df.shape}ï¼Œå·²ä¿å­˜åˆ° {merge_fp_path}\")\n",
    "# print(f\"âœ… test_df shape  = {test_df.shape}ï¼Œå·²ä¿å­˜åˆ° {test_fp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88ece8",
   "metadata": {},
   "source": [
    "# æ•°æ®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "def loaddata(DIRS):\n",
    "    # å®šä¹‰è·¯å¾„\n",
    "    merge_fp_path = os.path.join(DIRS['DATA_DIR000'], \"merge_fingerprints.csv\")\n",
    "    test_fp_path  = os.path.join(DIRS['DATA_DIR000'], \"test_fingerprints.csv\")\n",
    "    # è¯»å–æ•°æ®\n",
    "    merge_df = pd.read_csv(merge_fp_path)\n",
    "    test_df  = pd.read_csv(test_fp_path)\n",
    "\n",
    "    # æ‰“å°ä¿¡æ¯\n",
    "    print(f\"âœ… merge_df åŠ è½½å®Œæˆï¼Œshape = {merge_df.shape}\")\n",
    "    print(f\"âœ… test_df  åŠ è½½å®Œæˆï¼Œshape = {test_df.shape}\")\n",
    "\n",
    "    print(\"ç‰¹å¾å­—æ®µ: SMILES, Tm | æè¿°ç¬¦: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\")\n",
    "    print(\"åˆè®¡ç‰¹å¾æ€»æ•° = 6528\")\n",
    "\n",
    "    return  merge_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "merge_df, test_df =  loaddata(DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c871162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å°æ¸…å•\n",
    "def config_to_str(config: dict, indent: int = 0) -> str:\n",
    "    \"\"\"é€’å½’ç”Ÿæˆé…ç½®å­—ç¬¦ä¸²\"\"\"\n",
    "    prefix = \"     \" * indent\n",
    "    lines = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            lines.append(f\"{prefix}ğŸ”¹ {key}:\")\n",
    "            lines.append(config_to_str(value, indent + 1))  # é€’å½’æ‹¼æ¥å­å­—å…¸\n",
    "        else:\n",
    "            lines.append(f\"{prefix}- {key:<20}: {value}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f737573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒé…ç½®å•\n",
    "config = {\n",
    "    # å›ºå®šå¼€å…³\n",
    "    \"ISTEST\"            : True,\n",
    "\n",
    "    \"remove_dup_smiles\" : True, \n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 100,\n",
    "\n",
    "    # ç‰¹å¾é€‰æ‹© XGBoost å‚æ•°\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:absoluteerror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"mean\",   \n",
    "\n",
    "    # è®­ç»ƒè®¾ç½®\n",
    "    \"xgb_train_model_params\": {\n",
    "        'max_depth'   : 6,\n",
    "        'eta'         : 0.1,\n",
    "        'tree_method' : 'hist',\n",
    "        'eval_metric' : 'mae',\n",
    "    },\n",
    "    \"num_boost_round\": 15000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ‹†åˆ† (ç‰¹å¾çŸ©é˜µ ä¸ ç›®æ ‡å‘é‡)\n",
    "# ============================================\n",
    "# ç‰¹å¾å­—æ®µ: SMILES, Tm | æè¿°ç¬¦: 217 | Morgan: 1024 | FCFP: 1024 | MACCS: 167 | AtomPair: 1024 | RDKit: 2048 | Avalon: 1024\n",
    "# åˆè®¡ç‰¹å¾æ€»æ•° = 6528\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_features_and_target(merge_df: pd.DataFrame, test_df: pd.DataFrame, config: dict):\n",
    "    \"\"\"\n",
    "    æ•°æ®æ‹†åˆ†å‡½æ•°ï¼šæ„é€ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾çŸ©é˜µä¸ç›®æ ‡å‘é‡\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1. æ£€æŸ¥å¹¶å¤„ç†é‡å¤ SMILES\n",
    "    if config[\"remove_dup_smiles\"]:\n",
    "\n",
    "        dup_smiles = set(merge_df['SMILES']) & set(test_df['SMILES'])\n",
    "        print(f\"âš ï¸ æ£€æµ‹åˆ° {len(dup_smiles)} ä¸ªé‡å¤ SMILES\")\n",
    "\n",
    "        before_shape = merge_df.shape\n",
    "        # åˆ é™¤è®­ç»ƒé›†ä¸­å‡ºç°åœ¨æµ‹è¯•é›†çš„ SMILESï¼Œé¿å…æ•°æ®æ³„æ¼\n",
    "        merge_df = merge_df[~merge_df['SMILES'].isin(test_df['SMILES'])].reset_index(drop=True)\n",
    "        after_shape = merge_df.shape\n",
    "\n",
    "        print(f\"âœ… åˆ é™¤å®Œæˆ: ä» {before_shape} â†’ {after_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 2. æ„é€ ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å‘é‡\n",
    "    features_train = merge_df.drop(columns=['SMILES', 'Tm'])   # è®­ç»ƒé›†ç‰¹å¾ (X)\n",
    "    target_train   = merge_df['Tm']                            # è®­ç»ƒé›†ç›®æ ‡ (y, ç†”ç‚¹)\n",
    "    features_test  = test_df.drop(columns=['SMILES', 'id'])    # æµ‹è¯•é›†ç‰¹å¾ (æ—  Tm)\n",
    "\n",
    "\n",
    "    # éšæœºé€‰å–éƒ¨åˆ†ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼š50 ä¸ªï¼‰\n",
    "    if config[\"ISTEST\"]:\n",
    "        np.random.seed(42)\n",
    "        selected_features = np.random.choice(\n",
    "            merge_df.drop(columns=['SMILES', 'Tm']).columns,\n",
    "            size=110,\n",
    "            replace=False\n",
    "        )\n",
    "        sample_len = 100\n",
    "        features_train = merge_df.iloc[:sample_len][selected_features]   # è®­ç»ƒç‰¹å¾ (å‰ 1000 æ¡)\n",
    "        target_train = merge_df.iloc[:sample_len]['Tm']               # è®­ç»ƒç›®æ ‡\n",
    "        features_test = test_df[selected_features]          # æµ‹è¯•ç‰¹å¾ (åŒæ ·çš„ç‰¹å¾åˆ—)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3. æ‰“å°ç»´åº¦ä¿¡æ¯\n",
    "    print(\"ğŸ“Š æ•°æ®æ‹†åˆ†å®Œæˆ\")\n",
    "    print(f\"è®­ç»ƒé›†ç‰¹å¾ features_train  shape   : {features_train.shape}\")\n",
    "    print(f\"è®­ç»ƒé›†ç›®æ ‡   target_train  shape   : {target_train.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†ç‰¹å¾  features_test  shape   : {features_test.shape}\")\n",
    "    print(f\"           features_train  ç±»å‹    : {type(features_train)}\")\n",
    "\n",
    "    return features_train, target_train, features_test\n",
    "\n",
    "features_train, target_train, features_test = prepare_features_and_target(merge_df, test_df, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2724c74",
   "metadata": {},
   "source": [
    "### éé›¶å æ¯”åˆ†å¸ƒçš„ç›´æ–¹å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_nonzero_ratio_hist(features_train: pd.DataFrame, features_test: pd.DataFrame, bins_size: int = 10):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å¹¶æ¯”è¾ƒè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¯åˆ—éé›¶å æ¯”åˆ†å¸ƒçš„ç›´æ–¹å›¾\n",
    "\n",
    "    å‚æ•°:\n",
    "        features_train : pd.DataFrame\n",
    "            è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ\n",
    "        features_test  : pd.DataFrame\n",
    "            æµ‹è¯•é›†ç‰¹å¾çŸ©é˜µ\n",
    "        bins_size : int, é»˜è®¤=10\n",
    "            åˆ†ç®±æ•°é‡ (0%~100% åŒºé—´åˆ’åˆ†)\n",
    "\n",
    "    è¿”å›:\n",
    "        train_counts, test_counts : np.ndarray\n",
    "            ä¸¤ä¸ªæ•°æ®é›†åœ¨å„åŒºé—´å†…çš„åˆ—æ•°\n",
    "    \"\"\"\n",
    "    # é€åˆ—éé›¶å æ¯”ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰\n",
    "    train_ratio = features_train.apply(lambda col: (col != 0).mean() * 100)\n",
    "    test_ratio  = features_test.apply(lambda col: (col != 0).mean() * 100)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # ç»˜åˆ¶ç›´æ–¹å›¾ (density=True è¡¨ç¤ºé¢‘ç‡å½¢å¼)\n",
    "    counts1, bins1, _ = plt.hist(train_ratio, bins=bins_size, alpha=0.6, \n",
    "                                 label=\"features_train\", density=True)\n",
    "    counts2, bins2, _ = plt.hist(test_ratio, bins=bins_size, alpha=0.6, \n",
    "                                 label=\"features_test\", density=True)\n",
    "\n",
    "    # åˆ†åˆ«è®¡ç®—æ•°é‡ï¼ˆè€Œä¸æ˜¯é¢‘ç‡ï¼‰\n",
    "    train_counts, _ = np.histogram(train_ratio, bins=bins1)\n",
    "    test_counts, _ = np.histogram(test_ratio, bins=bins2)\n",
    "\n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"features_train å„åŒºé—´æ•°é‡ï¼š\")\n",
    "    for i in range(len(bins1)-1):\n",
    "        print(f\"{bins1[i]:.0f}% - {bins1[i+1]:.0f}% : {train_counts[i]} åˆ—\")\n",
    "\n",
    "    print(\"\\nfeatures_test å„åŒºé—´æ•°é‡ï¼š\")\n",
    "    for i in range(len(bins2)-1):\n",
    "        print(f\"{bins2[i]:.0f}% - {bins2[i+1]:.0f}% : {test_counts[i]} åˆ—\")\n",
    "\n",
    "    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°é‡\n",
    "    for c, b in zip(train_counts, bins1[:-1]):\n",
    "        if c > 0:\n",
    "            plt.text(b + (bins1[1]-bins1[0])/2, 0.01, str(c), \n",
    "                     ha=\"center\", va=\"bottom\", fontsize=8, color=\"black\", rotation=90)\n",
    "\n",
    "    for c, b in zip(test_counts, bins2[:-1]):\n",
    "        if c > 0:\n",
    "            plt.text(b + (bins2[1]-bins2[0])/2, 0.03, str(c), \n",
    "                     ha=\"center\", va=\"bottom\", fontsize=8, color=\"blue\", rotation=90)\n",
    "\n",
    "    plt.xlabel(\"éé›¶å æ¯” (%)\")\n",
    "    plt.ylabel(\"é¢‘ç‡ (Frequency, 0~1)\")\n",
    "    plt.title(\"å„åˆ—éé›¶å æ¯”åˆ†å¸ƒç›´æ–¹å›¾\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return train_counts, test_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_counts, test_counts = plot_nonzero_ratio_hist(features_train, features_test, bins_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f45be",
   "metadata": {},
   "source": [
    "### ç‰¹å¾ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_chemical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åŸºäºåˆ†å­æè¿°ç¬¦æ„é€ æ–°çš„è¡ç”Ÿç‰¹å¾\n",
    "    è¾“å…¥:\n",
    "        df : pd.DataFrameï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š\n",
    "            ['NumHDonors', 'NumHAcceptors', 'MolLogP', 'TPSA',\n",
    "            'NumRotatableBonds', 'MolWt', 'NumAromaticRings', 'BertzCT']\n",
    "    è¾“å‡º:\n",
    "        df_new : pd.DataFrameï¼ŒåŒ…å«æ–°å¢ç‰¹å¾\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['HBond_Product']        = df['NumHDonors'] * df['NumHAcceptors']\n",
    "    df['HBond_Sum']            = df['NumHDonors'] + df['NumHAcceptors']\n",
    "    df['LogP_div_TPSA']        = df['MolLogP'] / (df['TPSA'] + 1)\n",
    "    df['LogP_x_TPSA']          = df['MolLogP'] * df['TPSA']\n",
    "    df['Flexibility_Score']    = df['NumRotatableBonds'] / (df['MolWt'] + 1)\n",
    "    df['MolWt_x_AromaticRings']= df['MolWt'] * df['NumAromaticRings']\n",
    "    df['Complexity_per_MW']    = df['BertzCT'] / (df['MolWt'] + 1)\n",
    "    df['Rigidity_Score']       = df['NumAromaticRings'] / (df['NumRotatableBonds'] + 1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280348cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"use_feature_gen\"]:\n",
    "    features_train = add_chemical_features(features_train)\n",
    "    features_test  = add_chemical_features(features_test)\n",
    "\n",
    "features_train.shape, features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4582b1",
   "metadata": {},
   "source": [
    "### PCAé™ç»´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "def apply_truncated_svd(df: pd.DataFrame, n_components: int = 100, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ TruncatedSVD å¯¹ DataFrame è¿›è¡Œé™ç»´\n",
    "    è¾“å…¥:\n",
    "        df           : pd.DataFrameï¼Œç‰¹å¾çŸ©é˜µï¼ˆéœ€å»æ‰ ID / label ç­‰éç‰¹å¾åˆ—ï¼‰\n",
    "        n_components : intï¼Œé™ç»´åçš„ç›®æ ‡ç»´åº¦\n",
    "        random_state : intï¼Œéšæœºç§å­\n",
    "    è¾“å‡º:\n",
    "        reduced_df   : pd.DataFrameï¼Œé™ç»´åçš„ç»“æœï¼Œä¿æŒåŸè¡Œç´¢å¼•\n",
    "    \"\"\"\n",
    "    # è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µ\n",
    "    X_sparse = sparse.csr_matrix(df.values)\n",
    "\n",
    "    # åˆå§‹åŒ– SVD\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    # è®­ç»ƒå¹¶é™ç»´\n",
    "    X_reduced_array = svd.fit_transform(X_sparse)\n",
    "\n",
    "    # åŒ…è£…ä¸º DataFrame\n",
    "    reduced_df = pd.DataFrame(\n",
    "        X_reduced_array,\n",
    "        index=df.index,\n",
    "        columns=[f\"SVD_{i+1}\" for i in range(X_reduced_array.shape[1])]\n",
    "    )\n",
    "    # æ–¹å·®è§£é‡Šç‡\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "\n",
    "    # æ‰“å°ä¿¡æ¯\n",
    "    print( \"åŸå§‹ç»´åº¦         : \", df.shape)\n",
    "    print( \"é™ç»´å           : \", reduced_df.shape) \n",
    "    print(f\"ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”   :  {explained_var:.2%}\")\n",
    "\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717978bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹æ•°æ®é™ç»´\n",
    "if config[\"use_pca\"]:\n",
    "    features_train_reduced = apply_truncated_svd(features_train, n_components = 100)\n",
    "    features_test_reduced = apply_truncated_svd(features_test, n_components = 100)\n",
    "\n",
    "    features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "    features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "\n",
    "features_train.shape, features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f752",
   "metadata": {},
   "source": [
    "# å•æ¬¡è®­ç»ƒæ¨å¯¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold + XGBoost è¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œå¹¶ä¿å­˜å®éªŒç»“æœ\n",
    "# ==============================================================\n",
    "def run_kfold_xgb(features_train, target_train, features_test, config, DIRS, K_FOLDS=10, verbose=0):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Stratified K-Fold + XGBoost è¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œå¹¶ä¿å­˜å®éªŒç»“æœ\n",
    "\n",
    "    å‚æ•°:\n",
    "        features_train, target_train        : è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "        features_test      : æµ‹è¯•é›†ç‰¹å¾\n",
    "        params      : XGBoost æœ€ä¼˜å‚æ•° (dict)\n",
    "        DIRS        : ä¿å­˜ç»“æœçš„ç›®å½•å­—å…¸\n",
    "        K_FOLDS     : æŠ˜æ•° (é»˜è®¤=5)\n",
    "        verbose     : æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "    config[\"X shape\"] = features_train.shape\n",
    "    config[\"y shape\"] = target_train.shape\n",
    "    config[\"X_test shape\"] = features_test.shape\n",
    "\n",
    "\n",
    "    # ---------- åˆ›å»ºç›®å½• ----------\n",
    "    for _, path in DIRS.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    history_DIR = os.path.join(DIRS['HISTORY'], time_str)\n",
    "    os.makedirs(history_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"â€”â€”\" * 20)\n",
    "    print(f\"âœ… å½“å‰ç»“æœå°†ä¿å­˜åˆ°: {time_str}\")\n",
    "\n",
    "\n",
    "    # ---------- å®šä¹‰äº¤å‰éªŒè¯ ----------\n",
    "    skfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    yeo = PowerTransformer(method=\"yeo-johnson\")                                # å®šä¹‰ Yeo-Johnson å˜æ¢\n",
    "\n",
    "    # ---------- åˆå§‹åŒ–å­˜å‚¨ ----------\n",
    "    oof_val = np.zeros(len(features_train))       # OOF é¢„æµ‹\n",
    "    train_score, val_score = [], []  # æ¯æŠ˜ MAE\n",
    "    test_pred = []                   # æ¯æŠ˜ test é¢„æµ‹\n",
    "    fold_records = []                # ä¿å­˜æ¯æŠ˜ä¿¡æ¯\n",
    "    all_importances = []             # ç‰¹å¾é‡è¦æ€§\n",
    "    elapsed_list = []                # è€—æ—¶è®°å½•\n",
    "\n",
    "\n",
    "\n",
    "    # å¾ªç¯æ¯ä¸€æŠ˜\n",
    "    # ==============================================================\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skfold.split(features_train, pd.qcut(target_train, q=10).cat.codes), 1):\n",
    "\n",
    "        # ----- æ‰“å°æ—¶é—´ä¿¡æ¯ -----\n",
    "        start_now = datetime.now()\n",
    "        start_str = start_now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if elapsed_list:\n",
    "            avg_time = np.mean(elapsed_list)\n",
    "            est_end = start_now + timedelta(seconds=avg_time)\n",
    "\n",
    "            # æ¯ 5 ä¸ªä¸€ç»„è¾“å‡ºè€—æ—¶\n",
    "            parts = [f\"{t:6.1f}s\" for t in elapsed_list]\n",
    "            grouped = [\" \".join(parts[j:j+5]) for j in range(0, len(parts), 5)]\n",
    "            elapsed_str = \" /// \".join(grouped)\n",
    "\n",
    "            print(\n",
    "                f\"ğŸ”„{i:2d}/{K_FOLDS} ST {start_str}\"\n",
    "                f\" ET {est_end.strftime('%H:%M:%S')}\"\n",
    "                f\" avg {avg_time:.1f}s\"\n",
    "                f\" [{elapsed_str}]\",\n",
    "                end=\"\\r\", flush=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"ğŸ”„{i:2d}/{K_FOLDS} ST {start_str} ET (æš‚æ— å†å²æ•°æ®)\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "        # ----- å¼€å§‹è®­ç»ƒ -----\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. æ•°æ®é›†åˆ’åˆ†\n",
    "        x_train, x_val = features_train.iloc[train_idx], features_train.iloc[val_idx]\n",
    "        y_train, y_val = target_train[train_idx], target_train[val_idx]\n",
    "\n",
    "        # 2. Yeo-Johnson å˜æ¢\n",
    "        y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "        y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "\n",
    "        # 3. ç‰¹å¾é€‰æ‹©ï¼ˆè½»é‡çº§ XGBoostï¼‰\n",
    "        # ä½¿ç”¨\n",
    "        selector_model = xgb.XGBRegressor(**config[\"xgb_selector_model_params\"])\n",
    "        # selector_model = xgb.XGBRegressor(\n",
    "        #     n_estimators   = 500,\n",
    "        #     max_depth      = 6,\n",
    "        #     learning_rate  = 0.05,\n",
    "        #     random_state   = 2025,\n",
    "        #     device         = \"cpu\",\n",
    "        #     objective      = \"reg:absoluteerror\",\n",
    "        #     tree_method    = \"hist\",\n",
    "        #     verbosity      = 0\n",
    "        # )\n",
    "        \n",
    "        \n",
    "\n",
    "        selector_model.fit(x_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(selector_model, prefit=True, threshold=config[\"selector_threshold\"])\n",
    "        selected_features = x_train.columns[selector.get_support()].tolist()\n",
    "        if verbose > 0:\n",
    "            print(f\"âœ… é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}\")\n",
    "\n",
    "\n",
    "        # 4. ä¿ç•™é‡è¦ç‰¹å¾\n",
    "        x_train_new = x_train[selected_features]\n",
    "        x_val_new   = x_val[selected_features]\n",
    "        x_test_new  = features_test[selected_features]\n",
    "\n",
    "        # 5. è½¬æ¢ä¸º DMatrix\n",
    "        dtrain = xgb.DMatrix(x_train_new, y_train, feature_names=selected_features)\n",
    "        dval   = xgb.DMatrix(x_val_new,   y_val,   feature_names=selected_features)\n",
    "        dtest  = xgb.DMatrix(x_test_new,             feature_names=selected_features)\n",
    "\n",
    "\n",
    "        # 6. XGBoost è®­ç»ƒ\n",
    "        xgb_model = xgb.train(\n",
    "            params                 = config[\"xgb_train_model_params\"],\n",
    "            dtrain                 = dtrain,\n",
    "            num_boost_round        = config[\"num_boost_round\"],\n",
    "            evals                  = [(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds  = 300,\n",
    "            verbose_eval           = (1000 if verbose > 0 else False)\n",
    "        )\n",
    "\n",
    "\n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        model_path = os.path.join(history_DIR, f\"xgb_model_fold{i}.json\")\n",
    "        xgb_model.save_model(model_path)\n",
    "\n",
    "        # 7. è·å–ç‰¹å¾é‡è¦æ€§\n",
    "        imp_dict = xgb_model.get_score(importance_type=\"gain\")\n",
    "        imp_df = pd.DataFrame(imp_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "        imp_df[\"Fold\"] = i\n",
    "        all_importances.append(imp_df)\n",
    "\n",
    "\n",
    "        # 8. é¢„æµ‹\n",
    "        y_train_pred = xgb_model.predict(dtrain)\n",
    "        y_val_pred   = xgb_model.predict(dval)\n",
    "        y_test_pred  = xgb_model.predict(dtest)\n",
    "\n",
    "        # 9. é€†å˜æ¢\n",
    "        y_train      = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "        y_val        = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "        y_train_pred = yeo.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
    "        y_val_pred   = yeo.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
    "        y_test_pred  = yeo.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        # 10. è®¡ç®— MAE\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        val_mae   = mean_absolute_error(y_val,   y_val_pred)\n",
    "        if verbose > 0:\n",
    "            print(f\"Fold {i}: Train MAE={train_mae:.4f}, Val MAE={val_mae:.4f}ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’\")\n",
    "\n",
    "\n",
    "        # ----- ä¿å­˜ç»“æœ -----\n",
    "        train_score.append(train_mae)\n",
    "        val_score.append(val_mae)\n",
    "        oof_val[val_idx] = y_val_pred\n",
    "        test_pred.append(y_test_pred)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        elapsed_list.append(elapsed)\n",
    "\n",
    "        fold_records.append({\n",
    "            \"Fold\": i,\n",
    "            \"Train_MAE\": train_mae,\n",
    "            \"Val_MAE\": val_mae,\n",
    "            \"Num_Features\": len(selected_features),\n",
    "            \"Selected_Features\": selected_features,\n",
    "            \"elapsed\": elapsed\n",
    "        })\n",
    "\n",
    "    # ä¿å­˜æ•´ä½“ç»“æœ\n",
    "    # ==============================================================\n",
    "    if verbose > 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"ğŸ“Š Train MAE å¹³å‡å€¼ : {np.mean(train_score):.4f}\")\n",
    "        print(f\"ğŸ“Š Val   MAE å¹³å‡å€¼ : {np.mean(val_score):.4f}\")\n",
    "        print(f\"ğŸ“Š Train MAE æ ‡å‡†å·® : {np.std(train_score, ddof=0):.4f}\")\n",
    "        print(f\"ğŸ“Š Val   MAE æ ‡å‡†å·® : {np.std(val_score, ddof=0):.4f}\")\n",
    "\n",
    "    # å‚æ•°\n",
    "    with open(os.path.join(history_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # æ¯æŠ˜ä¿¡æ¯\n",
    "    folds_df = pd.DataFrame(fold_records)\n",
    "    folds_df.to_csv(os.path.join(history_DIR, \"folds_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # ç‰¹å¾é‡è¦æ€§\n",
    "    if all_importances:\n",
    "        valid_imps = [df for df in all_importances if not df.empty]\n",
    "        all_imp_df = pd.concat(valid_imps, axis=0) if valid_imps else pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    else:\n",
    "        all_imp_df = pd.DataFrame(columns=[\"Feature\", \"Importance\", \"Fold\"])\n",
    "    all_imp_df.to_csv(os.path.join(history_DIR, \"feature_importance_all.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    test_pred_array = np.vstack(test_pred).T\n",
    "    test_pred_df = pd.DataFrame(test_pred_array, columns=[f\"Fold_{j+1}\" for j in range(test_pred_array.shape[1])])\n",
    "    test_pred_df[\"Final_Pred\"] = test_pred_df.mean(axis=1)\n",
    "    test_pred_df.to_csv(os.path.join(history_DIR, \"test_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # æ€»ç»“\n",
    "    with open(os.path.join(history_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train MAE Mean : {np.mean(train_score):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Mean : {np.mean(val_score):.4f}\\n\")\n",
    "        f.write(f\"Train MAE Std  : {np.std(train_score, ddof=0):.4f}\\n\")\n",
    "        f.write(f\"Val   MAE Std  : {np.std(val_score, ddof=0):.4f}\\n\")\n",
    "\n",
    "\n",
    "    # æœ€ç»ˆæäº¤\n",
    "    final_score = np.mean(val_score)\n",
    "    submission = pd.read_csv(os.path.join(DIRS['DATA_DIR000'], \"sample_submission.csv\"))\n",
    "    submission[\"Tm\"] = test_pred_df[\"Final_Pred\"]\n",
    "\n",
    "    submission_path = os.path.join(history_DIR, f\"sub_{time_str}_{final_score:.8f}.csv\")\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    submission.to_csv(os.path.join(DIRS['SUBMISSION'], f\"sub_{time_str}_{final_score:.8f}.csv\"), index=False)\n",
    "\n",
    "        \n",
    "    config[\"time_str\"] = time_str\n",
    "    config[\"score\"] = final_score\n",
    "\n",
    "\n",
    "    # ---------- è¿”å›ç»“æœ ----------\n",
    "    return {\n",
    "        \"oof_val\": oof_val,\n",
    "        \"train_score\": train_score,\n",
    "        \"val_score\": val_score,\n",
    "        \"test_pred\": test_pred_df,\n",
    "        \"folds_info\": folds_df,\n",
    "        \"feature_importance\": all_imp_df,\n",
    "        \"submission_path\": submission_path,\n",
    "        \"time\": time_str,\n",
    "        \"final_score\": final_score,\n",
    "        \"config\": config\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œä¸€æ¬¡\n",
    "\n",
    "X = features_train\n",
    "y = target_train\n",
    "X_test = features_test\n",
    "print(X.shape, X_test.shape)\n",
    "\n",
    "\n",
    "results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "config = results['config']\n",
    "\n",
    "print('\\n',results['final_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å°å½“å‰config\n",
    "print(config_to_str(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3573b32",
   "metadata": {},
   "source": [
    "# æäº¤ kaggle å¹³å°æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ® submission_time å®šä½æ–‡ä»¶è·¯å¾„ æäº¤ kaggle å¹³å°æµ‹è¯•\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def find_submission_file(submission_time, submission_dir):\n",
    "    \"\"\"\n",
    "    åœ¨ submission_dir ä¸‹æŸ¥æ‰¾åŒ…å« submission_time çš„æ–‡ä»¶\n",
    "    ä¸€æ—¦æ‰¾åˆ°ç«‹åˆ»è¿”å›å®Œæ•´è·¯å¾„ï¼›å¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å› None\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(submission_dir):\n",
    "        if submission_time in fname:\n",
    "            file_path = os.path.join(submission_dir, fname)\n",
    "            print(f\"âœ… æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {fname}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(f\"âš ï¸ æœªæ‰¾åˆ°åŒ…å« {submission_time} çš„æ–‡ä»¶\")\n",
    "    return None\n",
    "\n",
    "def submit_and_get_score(file_path, competition_name, message=\"My submission\"):\n",
    "    \"\"\"\n",
    "    å°è£… Kaggle æäº¤å¹¶ç­‰å¾…ç»“æœè¯„åˆ†\n",
    "    --------------------------------------\n",
    "    file_path        : str  æäº¤æ–‡ä»¶è·¯å¾„\n",
    "    competition_name : str  Kaggle æ¯”èµ›åç§° (URL æœ€åä¸€æ®µ)\n",
    "    message          : str  æäº¤å¤‡æ³¨\n",
    "    \"\"\"\n",
    "    # 1. é…ç½® Kaggle API\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = r\"C:\\Users\\Admin\\.kaggle\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"âœ… Kaggle API å·²ç»é…ç½®æˆåŠŸï¼\")\n",
    "\n",
    "    # 2. æäº¤æ–‡ä»¶\n",
    "    api.competition_submit(\n",
    "        file_name=file_path,\n",
    "        competition=competition_name,\n",
    "        message=message\n",
    "    )\n",
    "    print(\"âœ… æäº¤å®Œæˆï¼è¯·ç­‰å¾…è¯„åˆ†...\")\n",
    "\n",
    "    # 3. åŠ¨æ€ç­‰å¾…\n",
    "    spinner = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
    "    while True:\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        latest = submissions[0]\n",
    "        status_str = str(latest._status).lower()\n",
    "\n",
    "        if \"complete\" in status_str and latest._public_score is not None:\n",
    "            print(\"\\nğŸ¯ æœ€ç»ˆç»“æœ:\")\n",
    "            print(f\"Public åˆ†æ•° : {latest._public_score}\")\n",
    "            print(f\"Private åˆ†æ•°: {latest._private_score}\")\n",
    "            print(f\"æäº¤ ID     : {latest._ref}\")\n",
    "            print(f\"æ–‡ä»¶å      : {latest._file_name}\")\n",
    "            print(f\"çŠ¶æ€        : {latest._status}\")\n",
    "            print(f\"æäº¤æ—¶é—´    : {latest._date}\")\n",
    "            print(f\"æè¿°/å¤‡æ³¨   : {latest._description}\")\n",
    "            return latest\n",
    "\n",
    "        spin_char = next(spinner)\n",
    "        print(f\"å½“å‰çŠ¶æ€: {status_str} , ç­‰å¾…ä¸­ {spin_char}\", end=\"\\r\", flush=True)\n",
    "        time.sleep(0.2)  # æ¯ 0.5 ç§’æ£€æŸ¥ä¸€æ¬¡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb1c16",
   "metadata": {},
   "source": [
    "### ä¸è½»æ˜“è¿è¡Œï¼Œå†ä¸‰è€ƒè™‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_time æäº¤\n",
    "submission_time = \"2025-10-21 23-51-09\"\n",
    "competition_name = \"melting-point\"\n",
    "message =  f\"è¯¥æäº¤æ–‡ä»¶çš„å‚æ•°ï¼š\\n{config_to_str(config)} \"\n",
    "print(message)\n",
    "\n",
    "target_file = find_submission_file(submission_time, DIRS['SUBMISSION'] )\n",
    "\n",
    "# submit_and_get_score(target_file, competition_name, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9607e",
   "metadata": {},
   "source": [
    "# å‚æ•°ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒé…ç½®å•\n",
    "base_config  = {\n",
    "    # å›ºå®šå¼€å…³\n",
    "    \"ISTEST\"            : False,\n",
    "\n",
    "    \"remove_dup_smiles\" : True, \n",
    "    \"use_feature_gen\"   : False,\n",
    "    \"use_pca\"           : True,\n",
    "    \"pca_components\"    : 100,\n",
    "\n",
    "    # ç‰¹å¾é€‰æ‹© XGBoost å‚æ•°\n",
    "    \"xgb_selector_model_params\": {\n",
    "        \"n_estimators\"  : 500,\n",
    "        \"max_depth\"     : 6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"random_state\"  : 2025,\n",
    "        \"device\"        : \"cpu\",\n",
    "        \"objective\"     : \"reg:absoluteerror\",\n",
    "        \"tree_method\"   : \"hist\",\n",
    "        \"verbosity\"     : 0\n",
    "    },\n",
    "\n",
    "    \"selector_threshold\"  : \"mean\",   \n",
    "\n",
    "    # è®­ç»ƒè®¾ç½®\n",
    "    \"xgb_train_model_params\": {\n",
    "        'max_depth'   : 6,\n",
    "        'eta'         : 0.1,\n",
    "        'tree_method' : 'hist',\n",
    "        'eval_metric' : 'mae',\n",
    "    },\n",
    "    \"num_boost_round\": 15000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¼˜åŒ–ä»»åŠ¡  åŠ å…¥æ ‡è¯†ç¬¦ host: hao-2   ip: 192.168.40.1\n",
    "\n",
    "import copy\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna çš„ç›®æ ‡å‡½æ•° (Objective Function)\n",
    "    æ¯æ¬¡ trial ä¼šç”Ÿæˆä¸€ç»„è¶…å‚æ•°ï¼Œç”¨äºè®­ç»ƒ XGBoost æ¨¡å‹ï¼Œ\n",
    "    å¹¶è¿”å›äº¤å‰éªŒè¯çš„å¹³å‡ RMSE ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 1. å®šä¹‰ è¶…å‚æ•° æœç´¢ç©ºé—´\n",
    "    # æ‹·è´ä¸€ä»½ configï¼Œé¿å…å…¨å±€æ±¡æŸ“\n",
    "    config = copy.deepcopy(base_config)\n",
    "\n",
    "    # åªä¿®æ”¹éœ€è¦ä¼˜åŒ–çš„å‚æ•°\n",
    "    config[\"remove_dup_smiles\"] = trial.suggest_categorical(\"remove_dup_smiles\", [True, False])\n",
    "    config[\"use_feature_gen\"]   = trial.suggest_categorical(\"use_feature_gen\", [True, False])\n",
    "    config[\"use_pca\"]           = trial.suggest_categorical(\"use_pca\", [True, False])\n",
    "\n",
    "    # config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_int(\"selector_random_state\", 1, 9999)\n",
    "    config[\"xgb_selector_model_params\"][\"random_state\"] = trial.suggest_categorical(\"selector_random_state\", [42, 2025])\n",
    "    config[\"xgb_selector_model_params\"][\"device\"]       = trial.suggest_categorical(\"selector_device\", [\"cpu\", \"cuda\"])\n",
    "    # config[\"xgb_selector_model_params\"][\"tree_method\"]  = trial.suggest_categorical(\"selector_tree_method\", [\"hist\", \"approx\"])\n",
    "\n",
    "    config[\"selector_threshold\"] = trial.suggest_categorical(\"selector_threshold\", [\"mean\", \"0.75*mean\", \"0.5*mean\", \"1.25*mean\"])\n",
    "\n",
    "    config[\"xgb_train_model_params\"][\"max_depth\"] = trial.suggest_int(\"train_max_depth\", 3, 12)\n",
    "    config[\"xgb_train_model_params\"][\"eta\"] = trial.suggest_float(\"train_eta\", 0.01 , 0.3 , log=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ä¸»æµç¨‹---------------------------------------------------------------------------------------------------\n",
    "    # åˆ›å»ºä¸€ä¸ªé»‘æ´ç¼“å†²åŒº\n",
    "    f = io.StringIO()\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        None\n",
    "\n",
    "    # æ‰“å°å½“å‰config\n",
    "    print(config_to_str(config))\n",
    "    \n",
    "\n",
    "    # åŠ è½½æ•°æ®\n",
    "    merge_df, test_df =  loaddata(DIRS)\n",
    "\n",
    "    # æ•°æ®æ‹†åˆ†\n",
    "    print(\"æ•°æ®æ‹†åˆ†---------------------------\")\n",
    "    features_train, target_train, features_test = prepare_features_and_target(merge_df, test_df, config)\n",
    "\n",
    "    # ç‰¹å¾ç”Ÿæˆ\n",
    "    if config[\"use_feature_gen\"]:\n",
    "        print(\"ç‰¹å¾ç”Ÿæˆ---------------------------\")\n",
    "        features_train = add_chemical_features(features_train)\n",
    "        features_test  = add_chemical_features(features_test)\n",
    "        print(features_train.shape, features_test.shape)\n",
    "\n",
    "    # æ•°æ®é™ç»´\n",
    "    if config[\"use_pca\"]:\n",
    "        print(\"æ•°æ®é™ç»´---------------------------\")\n",
    "        features_train_reduced = apply_truncated_svd(features_train, n_components = 100)\n",
    "        features_test_reduced = apply_truncated_svd(features_test, n_components = 100)\n",
    "\n",
    "        features_train = pd.concat([features_train, features_train_reduced], axis=1)\n",
    "        features_test = pd.concat([features_test, features_test_reduced], axis=1)\n",
    "        print(features_train.shape, features_test.shape)\n",
    "\n",
    "    X, y, X_test = features_train, target_train, features_test\n",
    "    print(\"å¼€å§‹è®­ç»ƒ---------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = run_kfold_xgb(X, y, X_test, config, DIRS, K_FOLDS = 10, verbose = 0)\n",
    "    config = results['config']\n",
    "    score = results['final_score']\n",
    "\n",
    "\n",
    "\n",
    "    HOSTNAME = socket.gethostname()\n",
    "    HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "    trial.set_user_attr(\"host\", HOSTNAME)        # ä½ è‡ªå·±å®šä¹‰ä¸»æœº A/B\n",
    "    trial.set_user_attr(\"ip\", HOST_IP)        # ä½ è‡ªå·±å®šä¹‰è§’è‰² A/B\n",
    "\n",
    "    # 4. è¿”å›å¹³å‡ MAE\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8aec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"test\" if base_config[\"ISTEST\"] else \"optuna_task1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹ä¼˜åŒ–\n",
    "\n",
    "# 1. å®šä¹‰ SQLite æ•°æ®åº“å­˜å‚¨è·¯å¾„\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name = STUDY_NAME,\n",
    "    # study_name=\"ghsdjsrtjrswtjhwrt\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# è‡ªåŠ¨è·å–å½“å‰ä¸»æœºå\\å½“å‰ä¸»æœºçš„ IP åœ°å€\n",
    "HOSTNAME = socket.gethostname()\n",
    "HOST_IP = socket.gethostbyname(HOSTNAME)\n",
    "print(\"ä¸»æœºå:\", HOSTNAME,\" ä¸»æœº IP:\", HOST_IP)\n",
    "time.sleep(1)\n",
    "\n",
    "# 5. å¯åŠ¨è¶…å‚æ•°æœç´¢\n",
    "print(\"ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢...\")\n",
    "if base_config[\"ISTEST\"]:\n",
    "    study.optimize(objective, n_trials = 3)\n",
    "else:\n",
    "    study.optimize(objective, n_trials = 100)\n",
    "\n",
    "\n",
    "# 6. æ‰“å°æœ€ä¼˜ç»“æœ\n",
    "print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š å·²å®Œæˆè¯•éªŒæ¬¡æ•° : {len(study.trials)}\")\n",
    "print(f\"ğŸ† æœ€ä¼˜è¯•éªŒç¼–å·   : {study.best_trial.number}\")\n",
    "print(f\"ğŸ“‰ æœ€ä¼˜ MAE       : {study.best_value}\")\n",
    "print(f\"âš™ï¸ æœ€ä¼˜å‚æ•°ç»„åˆ   : {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c00178",
   "metadata": {},
   "source": [
    "# ç®¡ç†æ•°æ®åº“ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebff3434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®åº“ä¸­çš„ study åˆ—è¡¨:\n",
      "- optuna_task1\n",
      "         Trials:\n",
      "    Trial    0: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 6, 'train_eta': 0.14963770710824598}\n",
      "    Trial    1: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 7, 'train_eta': 0.018021464633564754}\n",
      "    Trial    2: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.021659776125338565}\n",
      "    Trial    3: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 4, 'train_eta': 0.23918065062107188}\n",
      "    Trial    4: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 6, 'train_eta': 0.10392736979117173}\n",
      "    Trial    5: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.24138275928502126}\n",
      "    Trial    6: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.06282544238599601}\n",
      "    Trial    7: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 10, 'train_eta': 0.017725574116549923}\n",
      "    Trial    8: host=hao-2           , ip=192.168.40.1   , value=17.8499   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 6, 'train_eta': 0.01368700824230773}\n",
      "    Trial    9: host=hao-2           , ip=192.168.40.1   , value=17.9953   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.05053010910637573}\n",
      "    Trial   10: host=hao-2           , ip=192.168.40.1   , value=18.9248   , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.026729074261343022}\n",
      "    Trial   11: host=hao-2           , ip=192.168.40.1   , value=18.4746   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 12, 'train_eta': 0.0955937800637472}\n",
      "    Trial   12: host=hao-2           , ip=192.168.40.1   , value=18.8439   , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 5, 'train_eta': 0.01844230194954457}\n",
      "    Trial   13: host=hao-2           , ip=192.168.40.1   , value=18.5825   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 11, 'train_eta': 0.013329388592334063}\n",
      "    Trial   14: host=hao-2           , ip=192.168.40.1   , value=17.5563   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 8, 'train_eta': 0.013678962412610542}\n",
      "    Trial   15: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '0.5*mean', 'train_max_depth': 6, 'train_eta': 0.03616840082637632}\n",
      "    Trial   16: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.75*mean', 'train_max_depth': 9, 'train_eta': 0.03020978585855579}\n",
      "    Trial   17: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 9, 'train_eta': 0.06269871406279931}\n",
      "    Trial   18: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.0384   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.09158730675112933}\n",
      "    Trial   19: host=hao-2           , ip=192.168.40.1   , value=18.0853   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': '1.25*mean', 'train_max_depth': 9, 'train_eta': 0.045732428074001644}\n",
      "    Trial   20: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.2934   , params={'remove_dup_smiles': False, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 5, 'train_eta': 0.01810149024614987}\n",
      "    Trial   21: host=hao-2           , ip=192.168.40.1   , value=17.9752   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': '0.75*mean', 'train_max_depth': 4, 'train_eta': 0.06743144217230011}\n",
      "    Trial   22: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=18.5059   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '1.25*mean', 'train_max_depth': 8, 'train_eta': 0.2193719583768128}\n",
      "    Trial   23: host=hao-2           , ip=192.168.40.1   , value=18.0255   , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': False, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01061463707842147}\n",
      "    Trial   24: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.01024884269234489}\n",
      "    Trial   25: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 7, 'train_eta': 0.02880197004042992}\n",
      "    æ€» trial æ•°é‡: 26\n",
      "====================================================================================================\n",
      "- test\n",
      "         Trials:\n",
      "    Trial    0: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': True, 'use_pca': True, 'selector_random_state': 2025, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 10, 'train_eta': 0.04410160779890175}\n",
      "    Trial    1: host=DESKTOP-M056LUV , ip=198.18.0.1     , value=49.1581   , params={'remove_dup_smiles': False, 'use_feature_gen': False, 'use_pca': True, 'selector_random_state': 42, 'selector_device': 'cuda', 'selector_threshold': '0.5*mean', 'train_max_depth': 4, 'train_eta': 0.04105647603066798}\n",
      "    Trial    2: host=unknown         , ip=unknown        , value=None      , params={'remove_dup_smiles': True, 'use_feature_gen': False, 'use_pca': False, 'selector_random_state': 2025, 'selector_device': 'cpu', 'selector_threshold': 'mean', 'train_max_depth': 4, 'train_eta': 0.04449261200509654}\n",
      "    æ€» trial æ•°é‡: 3\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢æ•°æ®åº“è¯¦ç»†æ•°æ®\n",
    "\n",
    "storage_url = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage_url)\n",
    "\n",
    "if not studies:\n",
    "    print(\"âŒ å½“å‰æ•°æ®åº“é‡Œæ—  study\")\n",
    "else:\n",
    "    print(\"âœ… æ•°æ®åº“ä¸­çš„ study åˆ—è¡¨:\")\n",
    "    for s in studies:\n",
    "\n",
    "        print(\"-\", s.study_name)\n",
    "\n",
    "        study = optuna.load_study(study_name=s.study_name, storage=storage_url)\n",
    "\n",
    "        print(\"         Trials:\")\n",
    "        for trial in study.trials:\n",
    "            host = trial.user_attrs.get(\"host\") or \"unknown\"\n",
    "            ip = trial.user_attrs.get(\"ip\") or \"unknown\"\n",
    "            value = f\"{trial.value:.4f}\" if trial.value is not None else \"None\"\n",
    "\n",
    "            print(\n",
    "                f\"    Trial {trial.number:4d}: \"\n",
    "                f\"host={host:<16}, ip={ip:<15}, \"\n",
    "                f\"value={value:<10}, params={trial.params}\"\n",
    "            )\n",
    "\n",
    "        print(\"    æ€» trial æ•°é‡:\", len(study.trials))\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†å‰ï¼šå…ˆæŸ¥çœ‹æ•°æ®åº“é‡Œå½“å‰æœ‰å“ªäº› study å­˜åœ¨ï¼Œä»¥åŠæ¯ä¸ª study é‡Œæœ‰å¤šå°‘ä¸ª trial\n",
    "\n",
    "storage = \"mysql+pymysql://user1:123456@10.162.147.95:3306/kaggle_melting_point_optuna\"\n",
    "\n",
    "studies = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"ç°æœ‰ studyï¼š\", [s.study_name for s in studies])\n",
    "\n",
    "for s in studies:\n",
    "    study = optuna.load_study(study_name=s.study_name, storage=storage)\n",
    "    print(f\"Study:   {s.study_name:30s}, Trials: {len(study.trials):4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†ä¸­ï¼šåˆ é™¤æŒ‡å®š study\n",
    "# æŒ‡å®šè¦åˆ é™¤çš„åç§°\n",
    "to_delete = [\"melting_point_study\"]   # å¯ä»¥å†™ä¸€ä¸ªæˆ–å¤šä¸ª\n",
    "\n",
    "to_delete = [            ]\n",
    "\n",
    "for s in studies:\n",
    "    if s.study_name in to_delete:\n",
    "        optuna.delete_study(study_name=s.study_name, storage=storage)\n",
    "        print(\"å·²åˆ é™¤:\", s.study_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce984fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†åï¼šå†æ¬¡æ£€æŸ¥\n",
    "studies_after = optuna.study.get_all_study_summaries(storage=storage)\n",
    "print(\"æ¸…ç†å studyï¼š\", [s.study_name for s in studies_after])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
